{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7iWPXagdQhK"
      },
      "source": [
        "# Cyclotrons for Nuclear Medicine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asugYFtKJUsQ"
      },
      "source": [
        "###Setup\n",
        "First, make the necessary imports.\n",
        "Note that further imports may have to be made in addition to the ones below, if your application uses additional fetures such as loaders and tools. You can find the code for these imports in the respective sections of the tutorial notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6iiGJtSmt1H5",
        "outputId": "37706b19-472c-4a59-8491-78944a9fa718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-community langchain-core langchain-openai langchain-huggingface\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "import pprint\n",
        "import getpass\n",
        "from IPython.display import Markdown\n",
        "import IPython.display as ipd\n",
        "from PIL import Image\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qviEHKsjTZ1a"
      },
      "source": [
        "Then, assign the API keys to be able to use OpenAI, Google Serper, Huggingface, etc.\n",
        "\n",
        "When working with sensitive information like API keys or passwords in Google Colab, it's crucial to handle data securely. As you learnt in the tutorial session, two common approaches for this are using **Colab's Secrets Manager**, which stores and retrieves secrets without exposing them in the notebook, and `getpass`, a Python function that securely prompts users to input secrets during runtime without showing them. Both methods help ensure your sensitive data remains protected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9h1QKVPyTYTL"
      },
      "outputs": [],
      "source": [
        "#You can remove the keys you will not use\n",
        "\n",
        "#When using Colab Secret Manager\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "#When using getpass\n",
        "#os.environ['OPENAI_API_KEY'] = getpass.getpass()\n",
        "\n",
        "#When using Colab Secret Manager\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n",
        "#When using getpass\n",
        "#os.environ['SERPER_API_KEY'] = getpass.getpass()\n",
        "\n",
        "#When using Colab Secret Manager\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "#When using getpass\n",
        "#os.environ['HF_TOKEN'] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do I have to install this:"
      ],
      "metadata": {
        "id": "gPChC-Eh4bLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y \\\n",
        "  libatk1.0-0 \\\n",
        "  libatk-bridge2.0-0 \\\n",
        "  libcups2 \\\n",
        "  libdrm2 \\\n",
        "  libxkbcommon0 \\\n",
        "  libxcomposite1 \\\n",
        "  libxdamage1 \\\n",
        "  libxfixes3 \\\n",
        "  libxrandr2 \\\n",
        "  libgbm1 \\\n",
        "  libpango-1.0-0 \\\n",
        "  libcairo2 \\\n",
        "  libasound2"
      ],
      "metadata": {
        "id": "oz0G5ijXdJwh",
        "outputId": "89269b82-5779-450d-e5d7-d90885d75d4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [1 InRelease 0 B/129 kB \r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Waiting for headers] [1 InRelease 43.1 kB/129 kB 33%] [2 InRelease 3,632 B/\r0% [Waiting for headers] [1 InRelease 76.4 kB/129 kB 59%] [Connected to r2u.sta\r                                                                               \rGet:3 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Waiting for headers] [1 InRelease 79.3 kB/129 kB 62%] [Waiting for headers]\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://cli.github.com/packages stable/main amd64 Packages [354 B]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,637 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,871 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,624 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,600 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,968 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Fetched 36.3 MB in 9s (4,227 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libasound2 set to manually installed.\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo2 set to manually installed.\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxdamage1 set to manually installed.\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxfixes3 set to manually installed.\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxkbcommon0 set to manually installed.\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "libxrandr2 set to manually installed.\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.16).\n",
            "libcups2 set to manually installed.\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libdrm2 set to manually installed.\n",
            "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libgbm1 set to manually installed.\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libpango-1.0-0 set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core gsettings-desktop-schemas libatk1.0-data libatspi2.0-0 libxtst6\n",
            "  session-migration\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core gsettings-desktop-schemas libatk-bridge2.0-0 libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libxcomposite1 libxtst6 session-migration\n",
            "0 upgraded, 9 newly installed, 0 to remove and 54 not upgraded.\n",
            "Need to get 318 kB of archives.\n",
            "After this operation, 1,497 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Fetched 318 kB in 1s (368 kB/s)\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 117528 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../1-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../2-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../3-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../4-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../5-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../6-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../7-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../8-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service → /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do I have to install this:"
      ],
      "metadata": {
        "id": "ECg-VASV4fWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright pandas\n",
        "!playwright install chromium"
      ],
      "metadata": {
        "id": "4s5mABJXbxwQ",
        "outputId": "92a31c39-5129-4bf6-9abb-4d4b9b2ad6ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pyee<14,>=13 (from playwright)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl (46.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "Successfully installed playwright-1.57.0 pyee-13.0.0\n",
            "Downloading Chromium 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-linux.zip\u001b[22m\n",
            "(node:1737) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G164.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 0% 43.3s\u001b[0K\u001b[1G164.7 MiB [] 0% 18.8s\u001b[0K\u001b[1G164.7 MiB [] 0% 14.2s\u001b[0K\u001b[1G164.7 MiB [] 0% 11.1s\u001b[0K\u001b[1G164.7 MiB [] 1% 6.2s\u001b[0K\u001b[1G164.7 MiB [] 2% 4.6s\u001b[0K\u001b[1G164.7 MiB [] 2% 3.9s\u001b[0K\u001b[1G164.7 MiB [] 3% 3.5s\u001b[0K\u001b[1G164.7 MiB [] 4% 3.4s\u001b[0K\u001b[1G164.7 MiB [] 4% 3.6s\u001b[0K\u001b[1G164.7 MiB [] 5% 3.2s\u001b[0K\u001b[1G164.7 MiB [] 6% 3.0s\u001b[0K\u001b[1G164.7 MiB [] 7% 2.7s\u001b[0K\u001b[1G164.7 MiB [] 8% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 8% 2.5s\u001b[0K\u001b[1G164.7 MiB [] 9% 2.5s\u001b[0K\u001b[1G164.7 MiB [] 10% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 11% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 12% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 13% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 14% 2.1s\u001b[0K\u001b[1G164.7 MiB [] 15% 2.1s\u001b[0K\u001b[1G164.7 MiB [] 15% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 15% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 16% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 17% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 17% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 18% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 19% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 20% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 21% 2.1s\u001b[0K\u001b[1G164.7 MiB [] 21% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 22% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 22% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 23% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 24% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 25% 2.1s\u001b[0K\u001b[1G164.7 MiB [] 26% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 27% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 28% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 29% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 30% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 31% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 32% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 33% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 34% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 35% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 36% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 37% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 38% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 38% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 39% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 40% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 41% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 41% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 42% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 43% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 44% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 45% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 46% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 47% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 48% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 49% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 50% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 51% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 52% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 53% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 54% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 55% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 56% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 57% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 58% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 59% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 60% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 61% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 62% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 62% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 63% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 64% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 65% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 66% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 67% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 68% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 69% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 70% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 71% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 72% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 73% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 74% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 75% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 76% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 77% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 78% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 79% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 80% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 81% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 82% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 83% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 84% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 85% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 86% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 87% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 88% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 89% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 90% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 91% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 92% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 93% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 97% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium-1200\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "(node:1780) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 5% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 19% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 46% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 90% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Downloading Chromium Headless Shell 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-headless-shell-linux.zip\u001b[22m\n",
            "(node:1795) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G109.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 0% 30.5s\u001b[0K\u001b[1G109.7 MiB [] 0% 12.1s\u001b[0K\u001b[1G109.7 MiB [] 0% 9.8s\u001b[0K\u001b[1G109.7 MiB [] 1% 6.4s\u001b[0K\u001b[1G109.7 MiB [] 2% 3.6s\u001b[0K\u001b[1G109.7 MiB [] 3% 2.7s\u001b[0K\u001b[1G109.7 MiB [] 4% 2.2s\u001b[0K\u001b[1G109.7 MiB [] 6% 2.1s\u001b[0K\u001b[1G109.7 MiB [] 6% 2.2s\u001b[0K\u001b[1G109.7 MiB [] 7% 2.0s\u001b[0K\u001b[1G109.7 MiB [] 9% 1.8s\u001b[0K\u001b[1G109.7 MiB [] 10% 1.7s\u001b[0K\u001b[1G109.7 MiB [] 12% 1.6s\u001b[0K\u001b[1G109.7 MiB [] 13% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 14% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 16% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 17% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 19% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 20% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 21% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 22% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 23% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 24% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 25% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 27% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 29% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 30% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 32% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 33% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 35% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 37% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 38% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 40% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 41% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 43% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 45% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 47% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 49% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 51% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 53% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 55% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 57% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 59% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 61% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 62% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 64% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 66% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 67% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 69% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 70% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 71% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 72% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 74% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 76% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 78% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 80% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 81% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 84% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 85% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 87% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 89% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 96% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What am I doing here:"
      ],
      "metadata": {
        "id": "Bd2vY8px4iCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def test_playwright():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        await page.goto(\"https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx\")\n",
        "        print(await page.title())\n",
        "        await browser.close()\n",
        "\n",
        "await test_playwright()\n"
      ],
      "metadata": {
        "id": "jy6Mjd2ebs3H",
        "outputId": "8cf90010-a394-4b5d-c27c-221f92e97482",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pages - Cyclotrons used for Radionuclide Production\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web scraping of the IAEA cyclotron database\n",
        "\n",
        "This script automatically extracts structured data from the IAEA public web page\n",
        "listing cyclotron facilities worldwide:\n",
        "https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx\n",
        "\n",
        "The website is implemented using SharePoint and loads data dynamically across\n",
        "multiple pages. Because of this, a simple HTTP request is not sufficient and a\n",
        "browser automation tool (Playwright) is used to simulate real user navigation.\n",
        "\n",
        "Main features of the script:\n",
        "- Uses Playwright (async) to control a headless Chromium browser.\n",
        "- Navigates through all pages of the table by clicking the \"Next\" button.\n",
        "- Extracts tabular data from each page.\n",
        "- Cleans and normalizes rows to handle SharePoint quirks (e.g. header rows\n",
        "  injected into the table body, extra cells in the first row of each page).\n",
        "- Deduplicates rows using a hash-based fingerprint.\n",
        "- Stores the final structured dataset into a CSV file.\n",
        "\n",
        "Extracted fields:\n",
        "- Country\n",
        "- City\n",
        "- Facility\n",
        "- Manufacturer\n",
        "- Model\n",
        "- Proton energy (MeV)\n",
        "\n",
        "The final output is saved as:\n",
        "iaea_cyclotrons_normalized.csv\n",
        "\n",
        "This approach demonstrates:\n",
        "- Practical web scraping of JavaScript-heavy websites\n",
        "- Asynchronous programming in Python\n",
        "- Robust data cleaning\n",
        "- Reproducible data extraction for research purposes"
      ],
      "metadata": {
        "id": "fOJasaYn4uNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# IAEA Cyclotron List Scraper (SharePoint) — Colab-ready\n",
        "# ------------------------------------------------------------\n",
        "# What this does:\n",
        "# - Opens the IAEA SharePoint list view of cyclotrons\n",
        "# - Iterates through pages by clicking the \"Next\" button\n",
        "# - Extracts the table rows efficiently (one JS call per page)\n",
        "# - Cleans SharePoint quirks (header labels inside tbody, multi-line / tabbed cells)\n",
        "# - Deduplicates rows (hash fingerprint)\n",
        "# - Saves a CSV sorted by Country and City\n",
        "#\n",
        "# Output:\n",
        "#   /content/iaea_cyclotrons.csv\n",
        "# ============================================================\n",
        "\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "from playwright.async_api import async_playwright\n",
        "import os\n",
        "import hashlib\n",
        "import re\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "BASE_URL = \"https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx\"\n",
        "HASH_PREFIX = \"#InplviewHashd5afe566-18ad-4ac0-8aeb-ccf833dbc282=\"\n",
        "OUTPUT_CSV = os.path.join(os.getcwd(), \"iaea_cyclotrons.csv\")\n",
        "\n",
        "# Expected columns in the IAEA list view\n",
        "EXPECTED_COLS = 6  # Country, City, Facility, Manufacturer, Model, Proton energy (MeV)\n",
        "\n",
        "# SharePoint sometimes injects these header labels inside table rows\n",
        "HEADER_LABELS = [\n",
        "    \"country\", \"city\", \"facility\", \"manufacturer\", \"model\",\n",
        "    \"proton energy (mev)\", \"proton energy\"\n",
        "]\n",
        "HEADER_SET = set(HEADER_LABELS)\n",
        "\n",
        "# IMPORTANT: target the actual SharePoint \"list view\" table (reduces missing rows)\n",
        "TABLE_ROW_SELECTOR = \"table.ms-listviewtable tbody tr\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers for cleaning/parsing\n",
        "# -----------------------------\n",
        "def norm_text(s: str) -> str:\n",
        "    \"\"\"Normalize text for comparisons.\"\"\"\n",
        "    return \" \".join((s or \"\").split()).strip().lower()\n",
        "\n",
        "\n",
        "def row_fingerprint(cells):\n",
        "    \"\"\"\n",
        "    Make a stable hash of the cleaned row values.\n",
        "    Used to deduplicate rows across pages (SharePoint sometimes repeats).\n",
        "    \"\"\"\n",
        "    norm = [\" \".join((c or \"\").split()) for c in cells]\n",
        "    return hashlib.md5(\" | \".join(norm).encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "\n",
        "def strip_header_prefix(cell: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert 'City: Vienna' -> 'Vienna' (for known header labels).\n",
        "    Sometimes SharePoint prepends labels inside a cell.\n",
        "    \"\"\"\n",
        "    c = (cell or \"\").strip()\n",
        "    c_norm = norm_text(c)\n",
        "    for h in HEADER_LABELS:\n",
        "        if re.match(rf\"^{re.escape(h)}(\\s*[:\\-]?\\s+)\", c_norm):\n",
        "            return re.sub(rf\"(?i)^{re.escape(h)}\\s*[:\\-]?\\s+\", \"\", c).strip()\n",
        "    return c\n",
        "\n",
        "\n",
        "def flatten_multiline_cells(raw_cells):\n",
        "    \"\"\"\n",
        "    SharePoint quirks:\n",
        "    - multiple values in one cell separated by tabs\n",
        "    - multiple values separated by newlines\n",
        "    This function splits on tabs/newlines and flattens into a token list.\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    for c in raw_cells:\n",
        "        if not c:\n",
        "            continue\n",
        "        parts = re.split(r\"[\\t\\r\\n]+\", str(c))\n",
        "        for part in parts:\n",
        "            part = part.strip()\n",
        "            if part:\n",
        "                tokens.append(part)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def clean_and_align_tokens(raw_cells):\n",
        "    \"\"\"\n",
        "    Convert raw table cells -> exactly 6 cleaned fields.\n",
        "    Strategy:\n",
        "    - flatten tabs/newlines\n",
        "    - remove header-label tokens\n",
        "    - strip 'Label: value' prefixes\n",
        "    - if too many tokens remain, pick the best contiguous window of length 6\n",
        "    \"\"\"\n",
        "    tokens = flatten_multiline_cells(raw_cells)\n",
        "\n",
        "    processed = []\n",
        "    for t in tokens:\n",
        "        t_norm = norm_text(t)\n",
        "        if t_norm in HEADER_SET:\n",
        "            continue\n",
        "\n",
        "        t2 = strip_header_prefix(t)\n",
        "        t2_norm = norm_text(t2)\n",
        "        if not t2 or t2_norm in HEADER_SET:\n",
        "            continue\n",
        "\n",
        "        processed.append(t2.strip())\n",
        "\n",
        "    # Not enough data to form a row\n",
        "    if len(processed) < EXPECTED_COLS:\n",
        "        return None\n",
        "\n",
        "    # Exactly the correct number of fields\n",
        "    if len(processed) == EXPECTED_COLS:\n",
        "        # Drop if it still looks like a header row\n",
        "        if any(norm_text(x) in HEADER_SET for x in processed):\n",
        "            return None\n",
        "        return processed\n",
        "\n",
        "    # More than 6 tokens => choose the best \"slice\" of 6 tokens\n",
        "    def badness(x: str) -> int:\n",
        "        xn = norm_text(x)\n",
        "        if xn in HEADER_SET:\n",
        "            return 100\n",
        "        if xn.isdigit():  # sometimes SharePoint injects numeric IDs\n",
        "            return 10\n",
        "        if any(xn.startswith(h + \" \") for h in HEADER_LABELS):  # e.g. \"city zurich\"\n",
        "            return 10\n",
        "        return 0\n",
        "\n",
        "    best_window = None\n",
        "    best_score = None\n",
        "\n",
        "    for start in range(0, len(processed) - EXPECTED_COLS + 1):\n",
        "        window = processed[start:start + EXPECTED_COLS]\n",
        "        if any(norm_text(w) in HEADER_SET for w in window):\n",
        "            continue\n",
        "\n",
        "        score = sum(badness(w) for w in window)\n",
        "        if best_score is None or score < best_score:\n",
        "            best_score = score\n",
        "            best_window = window\n",
        "\n",
        "    return best_window\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Page navigation helpers\n",
        "# -----------------------------\n",
        "async def wait_for_table_refresh(page, prev_first_row_text, timeout_ms=20000):\n",
        "    \"\"\"\n",
        "    After clicking Next, wait until the first row content changes.\n",
        "    This is more reliable than a fixed sleep on SharePoint pages.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        await page.wait_for_function(\n",
        "            \"\"\"(prev) => {\n",
        "                const r = document.querySelector('table.ms-listviewtable tbody tr');\n",
        "                return r && r.innerText && r.innerText !== prev;\n",
        "            }\"\"\",\n",
        "            arg=prev_first_row_text,\n",
        "            timeout=timeout_ms\n",
        "        )\n",
        "    except:\n",
        "        # Fallback: small sleep if SharePoint is slow\n",
        "        await page.wait_for_timeout(1200)\n",
        "\n",
        "\n",
        "async def robust_click(el):\n",
        "    \"\"\"\n",
        "    SharePoint \"Next\" can be visible but outside viewport.\n",
        "    Try:\n",
        "    1) scroll into view\n",
        "    2) force-click\n",
        "    3) JS click as fallback\n",
        "    \"\"\"\n",
        "    try:\n",
        "        await el.scroll_into_view_if_needed()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        await el.click(force=True, timeout=20000)\n",
        "        return True\n",
        "    except:\n",
        "        try:\n",
        "            await el.evaluate(\"node => node.click()\")\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main scraper\n",
        "# -----------------------------\n",
        "async def scrape_all_pages():\n",
        "    all_rows = []\n",
        "    seen_rows = set()\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        # Launch headless browser\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "\n",
        "        # Create a browser context and block heavy resources for speed\n",
        "        context = await browser.new_context()\n",
        "        await context.route(\n",
        "            \"**/*\",\n",
        "            lambda route: route.abort()\n",
        "            if route.request.resource_type in (\"image\", \"font\", \"media\", \"stylesheet\")\n",
        "            else route.continue_()\n",
        "        )\n",
        "\n",
        "        page = await context.new_page()\n",
        "        page.set_default_timeout(20000)\n",
        "\n",
        "        # Open the first page (hash-based view)\n",
        "        url = BASE_URL + HASH_PREFIX\n",
        "        print(f\"Loading first page: {url}\")\n",
        "        await page.goto(url, timeout=60000, wait_until=\"domcontentloaded\")\n",
        "\n",
        "        page_index = 0\n",
        "\n",
        "        while True:\n",
        "            page_index += 1\n",
        "            print(f\"\\n--- Page {page_index} ---\")\n",
        "\n",
        "            # Ensure list view rows exist\n",
        "            try:\n",
        "                await page.wait_for_selector(TABLE_ROW_SELECTOR, timeout=20000)\n",
        "            except:\n",
        "                print(\"No list view table rows found → stopping\")\n",
        "                break\n",
        "\n",
        "            # Extract all rows from the list view in ONE browser call (fast)\n",
        "            raw_rows = await page.eval_on_selector_all(\n",
        "                TABLE_ROW_SELECTOR,\n",
        "                \"\"\"trs => trs.map(tr =>\n",
        "                    Array.from(tr.querySelectorAll('td')).map(td => td.innerText)\n",
        "                )\"\"\"\n",
        "            )\n",
        "\n",
        "            print(f\"Rows extracted from DOM this page: {len(raw_rows)}\")\n",
        "\n",
        "            new_rows_this_page = 0\n",
        "            kept_after_parsing = 0\n",
        "\n",
        "            # Parse each extracted row\n",
        "            for raw_cells in raw_rows:\n",
        "                cells = clean_and_align_tokens(raw_cells)\n",
        "                if cells is None or len(cells) != EXPECTED_COLS:\n",
        "                    continue\n",
        "\n",
        "                # Safety: never allow header labels as data\n",
        "                if any(norm_text(x) in HEADER_SET for x in cells):\n",
        "                    continue\n",
        "\n",
        "                kept_after_parsing += 1\n",
        "\n",
        "                # Deduplicate across pages\n",
        "                fp = row_fingerprint(cells)\n",
        "                if fp in seen_rows:\n",
        "                    continue\n",
        "\n",
        "                seen_rows.add(fp)\n",
        "                new_rows_this_page += 1\n",
        "\n",
        "                all_rows.append({\n",
        "                    \"Country\": cells[0],\n",
        "                    \"City\": cells[1],\n",
        "                    \"Facility\": cells[2],\n",
        "                    \"Manufacturer\": cells[3],\n",
        "                    \"Model\": cells[4],\n",
        "                    \"Proton energy (MeV)\": cells[5],\n",
        "                })\n",
        "\n",
        "            print(f\"Rows kept after parsing this page: {kept_after_parsing}\")\n",
        "            print(f\"New unique rows added this page: {new_rows_this_page}\")\n",
        "            print(f\"Total unique rows so far: {len(all_rows)}\")\n",
        "\n",
        "            # Capture first row text to detect refresh after clicking Next\n",
        "            try:\n",
        "                prev_first = await page.locator(TABLE_ROW_SELECTOR).first.inner_text()\n",
        "            except:\n",
        "                prev_first = \"\"\n",
        "\n",
        "            # Find \"Next\" and click it\n",
        "            next_el = page.locator(\n",
        "                'a[title=\"Next\"], a[aria-label=\"Next\"], a[title=\"Next page\"], a[aria-label=\"Next page\"], a:has-text(\"Next\")'\n",
        "            ).first\n",
        "\n",
        "            # Stop if no next page\n",
        "            if await next_el.count() == 0 or not await next_el.is_visible() or not await next_el.is_enabled():\n",
        "                print(\"No Next button → stopping\")\n",
        "                break\n",
        "\n",
        "            ok = await robust_click(next_el)\n",
        "            if not ok:\n",
        "                print(\"Could not click Next → stopping\")\n",
        "                break\n",
        "\n",
        "            # Wait for the table to refresh\n",
        "            await wait_for_table_refresh(page, prev_first)\n",
        "\n",
        "        await context.close()\n",
        "        await browser.close()\n",
        "\n",
        "    # Build final DataFrame, sort, and save\n",
        "    df = pd.DataFrame(all_rows)\n",
        "    df = df.sort_values([\"Country\", \"City\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "    df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "    print(\"\\nDONE\")\n",
        "    print(f\"Saved {len(df)} unique cyclotron rows to:\")\n",
        "    print(OUTPUT_CSV)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Run in Colab (top-level await is supported in Colab notebooks)\n",
        "# -----------------------------\n",
        "await scrape_all_pages()"
      ],
      "metadata": {
        "id": "Ypt70A4qdvOL",
        "outputId": "8022b111-85ec-4acc-ec57-3fcbcd50d07b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading first page: https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx#InplviewHashd5afe566-18ad-4ac0-8aeb-ccf833dbc282=\n",
            "\n",
            "--- Page 1 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 30\n",
            "\n",
            "--- Page 2 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 29\n",
            "Total unique rows so far: 59\n",
            "\n",
            "--- Page 3 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 89\n",
            "\n",
            "--- Page 4 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 24\n",
            "Total unique rows so far: 113\n",
            "\n",
            "--- Page 5 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 140\n",
            "\n",
            "--- Page 6 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 168\n",
            "\n",
            "--- Page 7 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 193\n",
            "\n",
            "--- Page 8 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 221\n",
            "\n",
            "--- Page 9 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 249\n",
            "\n",
            "--- Page 10 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 29\n",
            "Total unique rows so far: 278\n",
            "\n",
            "--- Page 11 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 306\n",
            "\n",
            "--- Page 12 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 26\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 332\n",
            "\n",
            "--- Page 13 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 360\n",
            "\n",
            "--- Page 14 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 390\n",
            "\n",
            "--- Page 15 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 420\n",
            "\n",
            "--- Page 16 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 24\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 442\n",
            "\n",
            "--- Page 17 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 26\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 464\n",
            "\n",
            "--- Page 18 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 491\n",
            "\n",
            "--- Page 19 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 518\n",
            "\n",
            "--- Page 20 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 540\n",
            "\n",
            "--- Page 21 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 562\n",
            "\n",
            "--- Page 22 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 587\n",
            "\n",
            "--- Page 23 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 615\n",
            "\n",
            "--- Page 24 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 643\n",
            "\n",
            "--- Page 25 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 19\n",
            "New unique rows added this page: 16\n",
            "Total unique rows so far: 659\n",
            "\n",
            "--- Page 26 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 18\n",
            "New unique rows added this page: 15\n",
            "Total unique rows so far: 674\n",
            "\n",
            "--- Page 27 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 27\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 701\n",
            "\n",
            "--- Page 28 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 23\n",
            "New unique rows added this page: 21\n",
            "Total unique rows so far: 722\n",
            "\n",
            "--- Page 29 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 748\n",
            "\n",
            "--- Page 30 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 774\n",
            "\n",
            "--- Page 31 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 27\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 800\n",
            "\n",
            "--- Page 32 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 826\n",
            "\n",
            "--- Page 33 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 19\n",
            "Total unique rows so far: 845\n",
            "\n",
            "--- Page 34 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 870\n",
            "\n",
            "--- Page 35 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 892\n",
            "\n",
            "--- Page 36 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 23\n",
            "Total unique rows so far: 915\n",
            "\n",
            "--- Page 37 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 940\n",
            "\n",
            "--- Page 38 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 965\n",
            "\n",
            "--- Page 39 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 990\n",
            "\n",
            "--- Page 40 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 27\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 1017\n",
            "\n",
            "--- Page 41 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 1045\n",
            "\n",
            "--- Page 42 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 1073\n",
            "\n",
            "--- Page 43 ---\n",
            "Rows extracted from DOM this page: 20\n",
            "Rows kept after parsing this page: 20\n",
            "New unique rows added this page: 20\n",
            "Total unique rows so far: 1093\n",
            "No Next button → stopping\n",
            "\n",
            "DONE\n",
            "Saved 1093 unique cyclotron rows to:\n",
            "/content/iaea_cyclotrons.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the first part of the multimodal\n",
        "\n",
        "Creation of the function: print_country_report"
      ],
      "metadata": {
        "id": "16Js_U3pdyC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# 1) Load your CSV\n",
        "# =========================\n",
        "CSV_PATH = \"/content/iaea_cyclotrons.csv\"   # <- adjust if different\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Normalize whitespace (helpful for grouping)\n",
        "for col in [\"Country\",\"City\",\"Facility\",\"Manufacturer\",\"Model\",\"Proton energy (MeV)\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "# =========================\n",
        "# 2) Helper: parse energy to numeric (best-effort)\n",
        "#    Handles: \"11\", \"16.5\", \"16-18\", \"16 / 18\", etc.\n",
        "# =========================\n",
        "def parse_energy_to_float(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in (\"nan\", \"none\"):\n",
        "        return None\n",
        "    # find numbers in the string\n",
        "    nums = re.findall(r\"\\d+(?:\\.\\d+)?\", s)\n",
        "    if not nums:\n",
        "        return None\n",
        "    # if range-like, take the max (or change to avg if you prefer)\n",
        "    vals = [float(n) for n in nums]\n",
        "    return max(vals)\n",
        "\n",
        "df[\"Energy_num\"] = df[\"Proton energy (MeV)\"].apply(parse_energy_to_float)\n",
        "\n",
        "# =========================\n",
        "# 3) Country summary function\n",
        "# =========================\n",
        "def country_summary(country, top_n=15):\n",
        "    \"\"\"\n",
        "    Return a structured summary for a country:\n",
        "    - total cyclotrons\n",
        "    - cities list + counts\n",
        "    - facilities list + counts\n",
        "    - manufacturer / model breakdown\n",
        "    - energy stats\n",
        "    \"\"\"\n",
        "    # case-insensitive match\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    if sub.empty:\n",
        "        # fuzzy suggestion: show close matches by containment\n",
        "        candidates = df[df[\"Country\"].str.lower().str.contains(str(country).strip().lower(), na=False)][\"Country\"].unique()\n",
        "        return {\n",
        "            \"country\": country,\n",
        "            \"found\": False,\n",
        "            \"message\": f\"No exact match for '{country}'.\",\n",
        "            \"did_you_mean\": sorted(candidates)[:20]\n",
        "        }\n",
        "\n",
        "    total = len(sub)\n",
        "\n",
        "    cities = (sub.groupby(\"City\")\n",
        "                .size()\n",
        "                .sort_values(ascending=False))\n",
        "\n",
        "    facilities = (sub.groupby(\"Facility\")\n",
        "                    .size()\n",
        "                    .sort_values(ascending=False))\n",
        "\n",
        "    manufacturers = (sub.groupby(\"Manufacturer\")\n",
        "                       .size()\n",
        "                       .sort_values(ascending=False))\n",
        "\n",
        "    models = (sub.groupby(\"Model\")\n",
        "                .size()\n",
        "                .sort_values(ascending=False))\n",
        "\n",
        "    # manufacturer-model combo\n",
        "    manu_model = (sub.groupby([\"Manufacturer\",\"Model\"])\n",
        "                    .size()\n",
        "                    .sort_values(ascending=False))\n",
        "\n",
        "    energy_stats = {\n",
        "        \"count_numeric\": int(sub[\"Energy_num\"].notna().sum()),\n",
        "        \"min\": float(sub[\"Energy_num\"].min()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "        \"median\": float(sub[\"Energy_num\"].median()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "        \"max\": float(sub[\"Energy_num\"].max()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"country\": country,\n",
        "        \"found\": True,\n",
        "        \"total_cyclotrons\": total,\n",
        "        \"cities_top\": cities.head(top_n),\n",
        "        \"facilities_top\": facilities.head(top_n),\n",
        "        \"manufacturers\": manufacturers,\n",
        "        \"models_top\": models.head(top_n),\n",
        "        \"manufacturer_model_top\": manu_model.head(top_n),\n",
        "        \"energy_stats\": energy_stats,\n",
        "        \"all_cities_count\": int(cities.shape[0]),\n",
        "        \"all_facilities_count\": int(facilities.shape[0]),\n",
        "    }\n",
        "\n",
        "# =========================\n",
        "# 4) Pretty-print function (human readable)\n",
        "# =========================\n",
        "def print_country_report(country, top_n=10):\n",
        "    out = country_summary(country, top_n=top_n)\n",
        "    if not out[\"found\"]:\n",
        "        print(out[\"message\"])\n",
        "        if out.get(\"did_you_mean\"):\n",
        "            print(\"Did you mean one of these?\")\n",
        "            for c in out[\"did_you_mean\"]:\n",
        "                print(\" -\", c)\n",
        "        return\n",
        "\n",
        "    print(f\"=== {out['country']} ===\")\n",
        "    print(f\"Total cyclotrons: {out['total_cyclotrons']}\")\n",
        "    print(f\"Cities covered: {out['all_cities_count']}\")\n",
        "    print(f\"Facilities covered: {out['all_facilities_count']}\")\n",
        "    print()\n",
        "\n",
        "    es = out[\"energy_stats\"]\n",
        "    print(\"Energy (MeV) stats (numeric rows only):\")\n",
        "    print(f\"  numeric entries: {es['count_numeric']}\")\n",
        "    print(f\"  min / median / max: {es['min']} / {es['median']} / {es['max']}\")\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} cities by count:\")\n",
        "    print(out[\"cities_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} facilities by count:\")\n",
        "    print(out[\"facilities_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(\"Manufacturer counts:\")\n",
        "    print(out[\"manufacturers\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} models:\")\n",
        "    print(out[\"models_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} (Manufacturer, Model) pairs:\")\n",
        "    print(out[\"manufacturer_model_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "# =========================\n",
        "# 5) Example usage\n",
        "# =========================\n",
        "print_country_report(\"Switzerland\", top_n=10)"
      ],
      "metadata": {
        "id": "CYd5RFlyGaoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4a8728-1bed-47b6-a53b-8a694bcdce4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Switzerland ===\n",
            "Total cyclotrons: 4\n",
            "Cities covered: 4\n",
            "Facilities covered: 4\n",
            "\n",
            "Energy (MeV) stats (numeric rows only):\n",
            "  numeric entries: 4\n",
            "  min / median / max: 16.0 / 17.0 / 18.0\n",
            "\n",
            "Top 10 cities by count:\n",
            "City\n",
            "Bern         1\n",
            "Genève       1\n",
            "Schlieren    1\n",
            "Zürich       1\n",
            "\n",
            "Top 10 facilities by count:\n",
            "Facility\n",
            "SWAN / Uni. Bern                                                             1\n",
            "Universitätsspital Zueurich, Labor Schlieren, Klinik für Onkologie (Wagi)    1\n",
            "Universitätsspital Zürich (USZ)                                              1\n",
            "unspecified                                                                  1\n",
            "\n",
            "Manufacturer counts:\n",
            "Manufacturer\n",
            "GE     2\n",
            "IBA    2\n",
            "\n",
            "Top 10 models:\n",
            "Model\n",
            "PETtrace            2\n",
            "CYCLONE 18          1\n",
            "CYCLONE 18/18 HC    1\n",
            "\n",
            "Top 10 (Manufacturer, Model) pairs:\n",
            "Manufacturer  Model           \n",
            "GE            PETtrace            2\n",
            "IBA           CYCLONE 18          1\n",
            "              CYCLONE 18/18 HC    1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora altro"
      ],
      "metadata": {
        "id": "q28_7bbDrUKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up OpenAI LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(temperature=0.9, model=\"gpt-4.1-mini\")\n"
      ],
      "metadata": {
        "id": "d50cJdyarBTD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the structured report into a prompt for the LLM"
      ],
      "metadata": {
        "id": "iX0CDcARsldc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def country_report_for_llm(country, top_n=10):\n",
        "    out = country_summary(country, top_n=top_n)\n",
        "\n",
        "    if not out[\"found\"]:\n",
        "        return f\"No data found for country: {country}\"\n",
        "\n",
        "    # Convert tables into readable text blocks\n",
        "    cities_text = out[\"cities_top\"].to_string()\n",
        "    facilities_text = out[\"facilities_top\"].to_string()\n",
        "    manufacturers_text = out[\"manufacturers\"].to_string()\n",
        "    models_text = out[\"models_top\"].to_string()\n",
        "\n",
        "    es = out[\"energy_stats\"]\n",
        "    energy_text = (\n",
        "        f\"Numeric entries: {es['count_numeric']}\\n\"\n",
        "        f\"Min energy: {es['min']}\\n\"\n",
        "        f\"Median energy: {es['median']}\\n\"\n",
        "        f\"Max energy: {es['max']}\"\n",
        "    )\n",
        "\n",
        "    # Final report string fed to the LLM\n",
        "    report = f\"\"\"\n",
        "IAEA Cyclotron Dataset Report for {out['country']}\n",
        "\n",
        "Total cyclotrons: {out['total_cyclotrons']}\n",
        "Cities covered: {out['all_cities_count']}\n",
        "Facilities covered: {out['all_facilities_count']}\n",
        "\n",
        "Top cities:\n",
        "{cities_text}\n",
        "\n",
        "Top facilities:\n",
        "{facilities_text}\n",
        "\n",
        "Manufacturers:\n",
        "{manufacturers_text}\n",
        "\n",
        "Top models:\n",
        "{models_text}\n",
        "\n",
        "Energy statistics:\n",
        "{energy_text}\n",
        "\"\"\"\n",
        "\n",
        "    return report.strip()"
      ],
      "metadata": {
        "id": "Sua-G7FSrkZz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell sends a structured country-level report to an LLM and asks it to generate a concise executive summary.\n",
        "The model is prompted to focus on market-relevant aspects such as:\n",
        "- Infrastructure scale\n",
        "- Geographic concentration\n",
        "- Major suppliers\n",
        "- Notable patterns in the data\n",
        "The output is rendered nicely using Markdown.\n"
      ],
      "metadata": {
        "id": "HgrQPVXKsq3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "country = \"Italy\"\n",
        "\n",
        "report_text = country_report_for_llm(country)\n",
        "\n",
        "response = llm.invoke(\n",
        "    f\"\"\"\n",
        "You are a market analyst in nuclear medicine.\n",
        "Write a concise executive summary (max 150 words) based on this dataset report.\n",
        "\n",
        "Focus on:\n",
        "- Overall infrastructure scale\n",
        "- Geographic concentration\n",
        "- Major suppliers\n",
        "- Any notable patterns\n",
        "\n",
        "Report:\n",
        "{report_text}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "WVJruj2Sr6x3",
        "outputId": "fa449b15-29e6-434a-ab00-3f6ca6b3d740"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Italy's nuclear medicine infrastructure comprises 40 cyclotrons distributed across 30 cities and 39 facilities, highlighting a well-established network. Major urban centers like Rome and Milan host the highest concentration, each with 4 cyclotrons, followed by Naples with 3, indicating geographic concentration in key metropolitan areas. The supplier landscape is dominated by GE, providing half of all cyclotrons (20 units), followed by IBA with 13, and Siemens with 4, reflecting a market led by a few major manufacturers. The most common models include GE’s PETtrace (11 units) and IBA’s CYCLONE 18 and MiniTrace (9 units each), emphasizing preference for established, versatile cyclotrons. Energy levels range from 10 to 19 MeV, with a median of 16 MeV, suitable for a wide range of radiopharmaceutical production. This distribution and supplier dominance suggest a mature, centralized infrastructure optimized for diverse nuclear medicine applications."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we should make an analysis of all countries in the IAEA database and create a document with all executive summaries."
      ],
      "metadata": {
        "id": "YDJ5UilKt9se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "Oa_N9nWg4xoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install reportlab geopy geopandas shapely pyproj fiona\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB_LeP0YswpW",
        "outputId": "c64e8ebe-cdfc-42ee-a244-14d59d96a832"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/2.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/2.0 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the country name for consistency"
      ],
      "metadata": {
        "id": "pXVjaXVyoqnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install country_converter\n",
        "\n",
        "import os\n",
        "import json\n",
        "import country_converter as coco\n",
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "IN_CSV  = \"/content/iaea_cyclotrons.csv\"\n",
        "OUT_CSV = \"/content/iaea_cyclotrons_clean.csv\"\n",
        "CACHE   = \"/content/country_fix_cache.json\"\n",
        "\n",
        "df = pd.read_csv(IN_CSV)\n",
        "\n",
        "# Load or create cache for LLM fixes\n",
        "if os.path.exists(CACHE):\n",
        "    with open(CACHE, \"r\", encoding=\"utf-8\") as f:\n",
        "        fix_cache = json.load(f)\n",
        "else:\n",
        "    fix_cache = {}\n",
        "\n",
        "# ✅ Allowed SHORT names (map-friendly)\n",
        "allowed = set(coco.CountryConverter().data[\"name_short\"].dropna().astype(str))\n",
        "\n",
        "def llm_fix_country(raw):\n",
        "    raw = str(raw).strip()\n",
        "    if raw in fix_cache:\n",
        "        return fix_cache[raw]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Map this value to the correct country name (common short form).\n",
        "Return only the country name, nothing else.\n",
        "\n",
        "Examples of desired style: Italy, Belarus, North Macedonia, Czechia, Russia.\n",
        "\n",
        "Input: {raw}\n",
        "\"\"\".strip()\n",
        "\n",
        "    resp = llm.invoke(prompt)\n",
        "    candidate = (resp.content or \"\").strip().strip('\"').strip(\"'\")\n",
        "\n",
        "    # Validate against allowed short names\n",
        "    if candidate in allowed:\n",
        "        fixed = candidate\n",
        "    else:\n",
        "        # fallback: let country_converter try to interpret candidate\n",
        "        fixed = coco.convert(names=candidate, to=\"name_short\", not_found=raw)\n",
        "\n",
        "    fix_cache[raw] = fixed\n",
        "\n",
        "    with open(CACHE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fix_cache, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return fixed\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Step A: deterministic conversion first (SHORT names)\n",
        "# -----------------------\n",
        "unique = df[\"Country\"].dropna().unique()\n",
        "\n",
        "short_names = coco.convert(names=list(unique), to=\"name_short\", not_found=None)\n",
        "mapping = dict(zip(unique, short_names))\n",
        "\n",
        "df[\"Country_clean\"] = df[\"Country\"].map(mapping)\n",
        "\n",
        "# -----------------------\n",
        "# Step B: LLM fallback only for missing\n",
        "# -----------------------\n",
        "missing = df.loc[df[\"Country_clean\"].isna(), \"Country\"].unique()\n",
        "print(\"LLM resolving:\", missing)\n",
        "\n",
        "for val in missing:\n",
        "    fixed = llm_fix_country(val)\n",
        "    df.loc[df[\"Country\"] == val, \"Country_clean\"] = fixed\n",
        "\n",
        "# -----------------------\n",
        "# Step C: overwrite original column\n",
        "# -----------------------\n",
        "df[\"Country\"] = df[\"Country_clean\"]\n",
        "df = df.drop(columns=[\"Country_clean\"])\n",
        "\n",
        "# -----------------------\n",
        "# Step D: save cleaned file\n",
        "# -----------------------\n",
        "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"✅ Cleaned file saved to: {OUT_CSV}\")\n",
        "\n",
        "# Optional sanity check\n",
        "print(\"\\nUnique countries after cleaning:\")\n",
        "print(sorted(df[\"Country\"].dropna().unique()))\n"
      ],
      "metadata": {
        "id": "1il6h6IVouev",
        "outputId": "b2081e70-c003-4d80-f5ee-298238bbce69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:country_converter.country_converter:Dubai not found in regex\n",
            "WARNING:country_converter.country_converter:Northern Ireland not found in regex\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM resolving: []\n",
            "✅ Cleaned file saved to: /content/iaea_cyclotrons_clean.csv\n",
            "\n",
            "Unique countries after cleaning:\n",
            "['Algeria', 'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahrain', 'Bangladesh', 'Belarus', 'Belgium', 'Bolivia', 'Brazil', 'Brunei Darussalam', 'Bulgaria', 'Canada', 'Chile', 'China', 'Colombia', 'Costa Rica', 'Croatia', 'Cuba', 'Cyprus', 'Czechia', 'Denmark', 'Dominican Republic', 'Dubai', 'Ecuador', 'Egypt', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Kuwait', 'Latvia', 'Lebanon', 'Libya', 'Lithuania', 'Malaysia', 'Mexico', 'Morocco', 'Myanmar', 'Netherlands', 'New Zealand', 'Nigeria', 'North Macedonia', 'Northern Ireland', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Romania', 'Russia', 'Saudi Arabia', 'Singapore', 'Slovakia', 'South Africa', 'South Korea', 'Spain', 'Sweden', 'Switzerland', 'Syria', 'Taiwan', 'Thailand', 'Tunisia', 'Türkiye', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Venezuela', 'Vietnam']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted(df[\"Country\"].unique()))"
      ],
      "metadata": {
        "id": "x1MnIoPBsCer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data + basic cleaning"
      ],
      "metadata": {
        "id": "YnqKziIt41bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, hashlib, math\n",
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"/content/iaea_cyclotrons_clean.csv\"  # <-- adjust if needed\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Clean whitespace\n",
        "for col in [\"Country\",\"City\",\"Facility\",\"Manufacturer\",\"Model\",\"Proton energy (MeV)\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "# Standardize \"unspecified\" to NaN for analysis\n",
        "def to_nan_if_unspecified(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in (\"nan\",\"none\",\"unspecified\",\"n/a\",\"na\"):\n",
        "        return None\n",
        "    return s\n",
        "\n",
        "for col in [\"Facility\",\"Manufacturer\",\"Model\",\"City\",\"Country\",\"Proton energy (MeV)\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].apply(to_nan_if_unspecified)\n",
        "\n",
        "# Parse energy numeric (best-effort)\n",
        "def parse_energy_to_float(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    s = str(x).strip()\n",
        "    if s == \"\":\n",
        "        return None\n",
        "    nums = re.findall(r\"\\d+(?:\\.\\d+)?\", s)\n",
        "    if not nums:\n",
        "        return None\n",
        "    vals = [float(n) for n in nums]\n",
        "    return max(vals)  # choose max if ranges exist\n",
        "\n",
        "df[\"Energy_num\"] = df[\"Proton energy (MeV)\"].apply(parse_energy_to_float)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SYVR40I54zY6",
        "outputId": "5b5d0903-3d1a-4184-c987-985e596e2cab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Country          City                                           Facility  \\\n",
              "0    Algeria         Alger                          Clinique Fatema Al Azhare   \n",
              "1    Algeria   Constantine                                               None   \n",
              "2    Algeria    Tizi Ouzou                           Hôpital Chahids Mahmoudi   \n",
              "3  Argentina     Bariloche  Centro Nacional de Medicina Nuclear y Radioter...   \n",
              "4  Argentina  Buenos Aires                                              Fleni   \n",
              "\n",
              "  Manufacturer          Model Proton energy (MeV)  Energy_num  \n",
              "0          IBA  CYCLONE KIUBE                  18        18.0  \n",
              "1          IBA  CYCLONE KIUBE                  18        18.0  \n",
              "2           GE       PETtrace                16.5        16.5  \n",
              "3          IBA     CYCLONE 18                  18        18.0  \n",
              "4           GE       PETtrace                  16        16.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-885facf4-c68b-485c-990f-6e0c3e4d7830\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>Facility</th>\n",
              "      <th>Manufacturer</th>\n",
              "      <th>Model</th>\n",
              "      <th>Proton energy (MeV)</th>\n",
              "      <th>Energy_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Algeria</td>\n",
              "      <td>Alger</td>\n",
              "      <td>Clinique Fatema Al Azhare</td>\n",
              "      <td>IBA</td>\n",
              "      <td>CYCLONE KIUBE</td>\n",
              "      <td>18</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Algeria</td>\n",
              "      <td>Constantine</td>\n",
              "      <td>None</td>\n",
              "      <td>IBA</td>\n",
              "      <td>CYCLONE KIUBE</td>\n",
              "      <td>18</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Algeria</td>\n",
              "      <td>Tizi Ouzou</td>\n",
              "      <td>Hôpital Chahids Mahmoudi</td>\n",
              "      <td>GE</td>\n",
              "      <td>PETtrace</td>\n",
              "      <td>16.5</td>\n",
              "      <td>16.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Argentina</td>\n",
              "      <td>Bariloche</td>\n",
              "      <td>Centro Nacional de Medicina Nuclear y Radioter...</td>\n",
              "      <td>IBA</td>\n",
              "      <td>CYCLONE 18</td>\n",
              "      <td>18</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Argentina</td>\n",
              "      <td>Buenos Aires</td>\n",
              "      <td>Fleni</td>\n",
              "      <td>GE</td>\n",
              "      <td>PETtrace</td>\n",
              "      <td>16</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-885facf4-c68b-485c-990f-6e0c3e4d7830')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-885facf4-c68b-485c-990f-6e0c3e4d7830 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-885facf4-c68b-485c-990f-6e0c3e4d7830');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1093,\n  \"fields\": [\n    {\n      \"column\": \"Country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 93,\n        \"samples\": [\n          \"Italy\",\n          \"Czechia\",\n          \"Myanmar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"City\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 699,\n        \"samples\": [\n          \"Zhengzhou\",\n          \"Je-ju\",\n          \"Tokyo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Facility\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 623,\n        \"samples\": [\n          \"Landspitali\",\n          \"University of Southern California Molecular bldg\",\n          \"CICLOTRON COLOMBIA S.A.S.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Manufacturer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"Ionetix\",\n          \"TCC The Cyclotron Corporation\",\n          \"Sichuan Longevous Beamtech(LBT).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 122,\n        \"samples\": [\n          \"Cyclone 18/18 Twin\",\n          \"LB-11MTS\",\n          \"LBT-11\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Proton energy (MeV)\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"3-20\",\n          \"19\",\n          \"30\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Energy_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.082378358480085,\n        \"min\": 3.0,\n        \"max\": 700.0,\n        \"num_unique_values\": 33,\n        \"samples\": [\n          12.5,\n          20.0,\n          80.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Country report text generator for LLM"
      ],
      "metadata": {
        "id": "gSwW9HGt4-TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def country_summary(country, top_n=15):\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    if sub.empty:\n",
        "        candidates = df[df[\"Country\"].str.lower().str.contains(str(country).strip().lower(), na=False)][\"Country\"].dropna().unique()\n",
        "        return {\"country\": country, \"found\": False, \"did_you_mean\": sorted(candidates)[:20]}\n",
        "\n",
        "    total = len(sub)\n",
        "    cities = sub.groupby(\"City\").size().sort_values(ascending=False)\n",
        "    facilities = sub.groupby(\"Facility\").size().sort_values(ascending=False)\n",
        "    manufacturers = sub.groupby(\"Manufacturer\").size().sort_values(ascending=False)\n",
        "    models = sub.groupby(\"Model\").size().sort_values(ascending=False)\n",
        "    manu_model = sub.groupby([\"Manufacturer\",\"Model\"]).size().sort_values(ascending=False)\n",
        "\n",
        "    energy_stats = {\n",
        "        \"count_numeric\": int(sub[\"Energy_num\"].notna().sum()),\n",
        "        \"min\": float(sub[\"Energy_num\"].min()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "        \"median\": float(sub[\"Energy_num\"].median()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "        \"max\": float(sub[\"Energy_num\"].max()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"country\": country,\n",
        "        \"found\": True,\n",
        "        \"total_cyclotrons\": total,\n",
        "        \"cities_top\": cities.head(top_n),\n",
        "        \"facilities_top\": facilities.head(top_n),\n",
        "        \"manufacturers\": manufacturers,\n",
        "        \"models_top\": models.head(top_n),\n",
        "        \"manufacturer_model_top\": manu_model.head(top_n),\n",
        "        \"energy_stats\": energy_stats,\n",
        "        \"all_cities_count\": int(cities.shape[0]),\n",
        "        \"all_facilities_count\": int(facilities.shape[0]),\n",
        "    }\n",
        "\n",
        "def country_report_for_llm(country, top_n=10):\n",
        "    out = country_summary(country, top_n=top_n)\n",
        "    if not out[\"found\"]:\n",
        "        return f\"No exact match for '{country}'. Suggestions: {out.get('did_you_mean', [])}\"\n",
        "\n",
        "    cities_text = out[\"cities_top\"].to_string()\n",
        "    facilities_text = out[\"facilities_top\"].to_string()\n",
        "    manufacturers_text = out[\"manufacturers\"].to_string()\n",
        "    models_text = out[\"models_top\"].to_string()\n",
        "\n",
        "    es = out[\"energy_stats\"]\n",
        "    energy_text = (\n",
        "        f\"Numeric entries: {es['count_numeric']}\\n\"\n",
        "        f\"Min energy: {es['min']}\\n\"\n",
        "        f\"Median energy: {es['median']}\\n\"\n",
        "        f\"Max energy: {es['max']}\"\n",
        "    )\n",
        "\n",
        "    report = f\"\"\"\n",
        "IAEA Cyclotron Dataset Report for {out['country']}\n",
        "\n",
        "Total cyclotrons: {out['total_cyclotrons']}\n",
        "Cities covered: {out['all_cities_count']}\n",
        "Facilities covered: {out['all_facilities_count']}\n",
        "\n",
        "Top cities:\n",
        "{cities_text}\n",
        "\n",
        "Top facilities:\n",
        "{facilities_text}\n",
        "\n",
        "Manufacturers:\n",
        "{manufacturers_text}\n",
        "\n",
        "Top models:\n",
        "{models_text}\n",
        "\n",
        "Energy statistics:\n",
        "{energy_text}\n",
        "\"\"\"\n",
        "    return report.strip()\n",
        "\n",
        "def llm_exec_summary(country, top_n=10):\n",
        "    report_text = country_report_for_llm(country, top_n=top_n)\n",
        "    prompt = f\"\"\"\n",
        "You are a market analyst in nuclear medicine.\n",
        "Write a concise executive summary (max 150 words) based on this dataset report.\n",
        "\n",
        "Focus on:\n",
        "- Overall infrastructure scale\n",
        "- Geographic concentration\n",
        "- Major suppliers\n",
        "- Any notable patterns\n",
        "\n",
        "Report:\n",
        "{report_text}\n",
        "\"\"\".strip()\n",
        "    return llm.invoke(prompt).content.strip()"
      ],
      "metadata": {
        "id": "XwA0zk8744-B"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geocoding + caching (for map dots)"
      ],
      "metadata": {
        "id": "0TJqwDJq5FRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "from geopy.extra.rate_limiter import RateLimiter\n",
        "\n",
        "GEO_CACHE = \"/content/geocode_cache.csv\"\n",
        "\n",
        "# Load existing cache if available\n",
        "if os.path.exists(GEO_CACHE):\n",
        "    geo_cache = pd.read_csv(GEO_CACHE)\n",
        "else:\n",
        "    geo_cache = pd.DataFrame(columns=[\"Country\",\"City\",\"lat\",\"lon\",\"display_name\"])\n",
        "\n",
        "geo_cache[\"Country\"] = geo_cache[\"Country\"].astype(str)\n",
        "geo_cache[\"City\"] = geo_cache[\"City\"].astype(str)\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"iaea_cyclotron_colab_geocoder\")\n",
        "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1.0, swallow_exceptions=True)\n",
        "\n",
        "def get_latlon(country, city):\n",
        "    global geo_cache   # <-- must come first\n",
        "\n",
        "    # cache lookup\n",
        "    hit = geo_cache[\n",
        "        (geo_cache[\"Country\"].str.lower() == str(country).lower()) &\n",
        "        (geo_cache[\"City\"].str.lower() == str(city).lower())\n",
        "    ]\n",
        "\n",
        "    if not hit.empty and pd.notna(hit.iloc[0][\"lat\"]) and pd.notna(hit.iloc[0][\"lon\"]):\n",
        "        return float(hit.iloc[0][\"lat\"]), float(hit.iloc[0][\"lon\"])\n",
        "\n",
        "    # best-effort geocode\n",
        "    q = f\"{city}, {country}\"\n",
        "    loc = geocode(q)\n",
        "\n",
        "    if loc is None:\n",
        "        # store miss to avoid retry storms\n",
        "        new_row = {\n",
        "            \"Country\": country,\n",
        "            \"City\": city,\n",
        "            \"lat\": None,\n",
        "            \"lon\": None,\n",
        "            \"display_name\": None\n",
        "        }\n",
        "        geo_cache = pd.concat([geo_cache, pd.DataFrame([new_row])], ignore_index=True)\n",
        "        geo_cache.to_csv(GEO_CACHE, index=False)\n",
        "        return None, None\n",
        "\n",
        "    lat, lon = loc.latitude, loc.longitude\n",
        "    display_name = getattr(loc, \"address\", None)\n",
        "\n",
        "    new_row = {\n",
        "        \"Country\": country,\n",
        "        \"City\": city,\n",
        "        \"lat\": lat,\n",
        "        \"lon\": lon,\n",
        "        \"display_name\": display_name\n",
        "    }\n",
        "\n",
        "    if any(v is not None for v in new_row.values()):\n",
        "        geo_cache = pd.concat([geo_cache, pd.DataFrame([new_row])], ignore_index=True)\n",
        "        geo_cache.to_csv(GEO_CACHE, index=False)\n",
        "\n",
        "    return lat, lon\n",
        "\n",
        "def geocode_country_cities(country):\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    cities = sorted([c for c in sub[\"City\"].dropna().unique() if str(c).strip()])\n",
        "\n",
        "    pts = []\n",
        "    for city in cities:\n",
        "        lat, lon = get_latlon(country, city)\n",
        "        if lat is not None and lon is not None:\n",
        "            pts.append((city, lat, lon))\n",
        "    return pts"
      ],
      "metadata": {
        "id": "RnItl1Uc5CoZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates charts + maps"
      ],
      "metadata": {
        "id": "waf1MbKa5P8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from reportlab.platypus import Table\n",
        "\n",
        "IMG_DIR = \"/content/iaea_report_imgs\"\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "def save_city_bar_chart(country, top_n=10):\n",
        "    out = country_summary(country, top_n=top_n)\n",
        "    if not out[\"found\"] or out[\"cities_top\"].empty:\n",
        "        return None\n",
        "\n",
        "    s = out[\"cities_top\"]\n",
        "    path = os.path.join(IMG_DIR, f\"{country}_cities.png\")\n",
        "\n",
        "    plt.figure()\n",
        "    s[::-1].plot(kind=\"barh\")  # reverse so largest at top visually\n",
        "    plt.title(f\"{country}: Top {min(top_n, len(s))} cities by cyclotron count\")\n",
        "    plt.xlabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=180)\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "def save_manufacturer_bar_chart(country, top_n=10):\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    s = sub[\"Manufacturer\"].dropna().value_counts().head(top_n)\n",
        "    if s.empty:\n",
        "        return None\n",
        "\n",
        "    path = os.path.join(IMG_DIR, f\"{country}_manufacturers.png\")\n",
        "    plt.figure()\n",
        "    s[::-1].plot(kind=\"barh\")\n",
        "    plt.title(f\"{country}: Top manufacturers\")\n",
        "    plt.xlabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=180)\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "def save_energy_hist(country):\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    vals = sub[\"Energy_num\"].dropna()\n",
        "    if vals.empty:\n",
        "        return None\n",
        "\n",
        "    path = os.path.join(IMG_DIR, f\"{country}_energy.png\")\n",
        "    plt.figure()\n",
        "    plt.hist(vals, bins=12)\n",
        "    plt.title(f\"{country}: Proton energy distribution (MeV)\")\n",
        "    plt.xlabel(\"MeV\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=180)\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "def save_country_map(country):\n",
        "    import geopandas as gpd\n",
        "    from shapely.geometry import Point\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    pts = geocode_country_cities(country)\n",
        "    if not pts:\n",
        "        return None\n",
        "\n",
        "    world = gpd.read_file(\n",
        "        \"https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\"\n",
        "    )\n",
        "\n",
        "    gdf_pts = gpd.GeoDataFrame(\n",
        "        [{\"City\": c, \"geometry\": Point(lon, lat)} for c, lat, lon in pts],\n",
        "        crs=\"EPSG:4326\"\n",
        "    )\n",
        "\n",
        "    country_poly = world[world[\"NAME\"].str.lower() == str(country).strip().lower()]\n",
        "\n",
        "    path = os.path.join(IMG_DIR, f\"{country}_map.png\")\n",
        "    plt.figure(figsize=(7, 4.5))\n",
        "    ax = plt.gca()\n",
        "\n",
        "    if not country_poly.empty:\n",
        "        country_poly.plot(ax=ax, linewidth=0.6, edgecolor=\"black\")\n",
        "        minx, miny, maxx, maxy = country_poly.total_bounds\n",
        "    else:\n",
        "        minx, miny, maxx, maxy = gdf_pts.total_bounds\n",
        "\n",
        "    pad_x = (maxx - minx) * 0.08 if maxx > minx else 2\n",
        "    pad_y = (maxy - miny) * 0.08 if maxy > miny else 2\n",
        "    ax.set_xlim(minx - pad_x, maxx + pad_x)\n",
        "    ax.set_ylim(miny - pad_y, maxy + pad_y)\n",
        "\n",
        "    # Red dots + black edge, drawn on top\n",
        "    gdf_pts.plot(\n",
        "        ax=ax,\n",
        "        color=\"red\",\n",
        "        markersize=60,\n",
        "        edgecolor=\"black\",\n",
        "        linewidth=0.4,\n",
        "        zorder=5\n",
        "    )\n",
        "\n",
        "    plt.title(f\"{country}: Cyclotron locations (by city)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=220)\n",
        "    plt.close()\n",
        "\n",
        "    return path\n",
        "\n",
        "def plots_2x2_block(imgs, cell_w=255, cell_h=185, pad=6):\n",
        "    \"\"\"\n",
        "    imgs: dict with keys: 'city', 'manufacturer', 'energy', 'map'\n",
        "    Returns a ReportLab Table laid out 2x2, each cell contains (title + image).\n",
        "    \"\"\"\n",
        "    def cell(title, path):\n",
        "        if path and os.path.exists(path):\n",
        "            return [\n",
        "                Paragraph(title, styles[\"Heading4\"]),\n",
        "                rl_image_fit(path, max_w=cell_w, max_h=cell_h)\n",
        "            ]\n",
        "        else:\n",
        "            return [Paragraph(title + \" (not available)\", styles[\"Normal\"])]\n",
        "\n",
        "    data = [\n",
        "        [cell(\"Top cities\", imgs.get(\"city\")),         cell(\"Manufacturers\", imgs.get(\"manufacturer\"))],\n",
        "        [cell(\"Energy distribution\", imgs.get(\"energy\")), cell(\"Map (red dots)\", imgs.get(\"map\"))],\n",
        "    ]\n",
        "\n",
        "    t = Table(\n",
        "        data,\n",
        "        colWidths=[(cell_w + pad) * 1.0, (cell_w + pad) * 1.0],\n",
        "        rowHeights=[cell_h + 30, cell_h + 30],  # extra room for title\n",
        "    )\n",
        "    t.setStyle(TableStyle([\n",
        "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
        "        (\"LEFTPADDING\", (0,0), (-1,-1), pad),\n",
        "        (\"RIGHTPADDING\", (0,0), (-1,-1), pad),\n",
        "        (\"TOPPADDING\", (0,0), (-1,-1), pad),\n",
        "        (\"BOTTOMPADDING\", (0,0), (-1,-1), pad),\n",
        "    ]))\n",
        "    return t"
      ],
      "metadata": {
        "id": "1T6NA7lm5JhG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison + data quality sections (tables)"
      ],
      "metadata": {
        "id": "MF1RwQm05Uyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def global_comparison_tables():\n",
        "    # Top countries by cyclotron count\n",
        "    top_countries = df[\"Country\"].dropna().value_counts().head(25)\n",
        "\n",
        "    # Manufacturer global counts\n",
        "    top_manu = df[\"Manufacturer\"].dropna().value_counts().head(15)\n",
        "\n",
        "    # Energy by country (numeric only)\n",
        "    energy_country = (\n",
        "        df.dropna(subset=[\"Country\"])\n",
        "          .groupby(\"Country\")[\"Energy_num\"]\n",
        "          .agg([\"count\",\"min\",\"median\",\"max\"])\n",
        "          .sort_values(\"count\", ascending=False)\n",
        "          .head(25)\n",
        "    )\n",
        "\n",
        "    return top_countries, top_manu, energy_country\n",
        "\n",
        "def data_quality_summary():\n",
        "    total = len(df)\n",
        "    missing = {}\n",
        "    for col in [\"City\",\"Facility\",\"Manufacturer\",\"Model\",\"Proton energy (MeV)\",\"Energy_num\"]:\n",
        "        missing[col] = int(df[col].isna().sum())\n",
        "\n",
        "    return {\n",
        "        \"total_rows\": total,\n",
        "        \"missing_counts\": missing,\n",
        "        \"missing_pct\": {k: (v/total*100 if total else 0) for k,v in missing.items()}\n",
        "    }"
      ],
      "metadata": {
        "id": "ztifd5fN5SPG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multimodal nested call to comment on each map"
      ],
      "metadata": {
        "id": "H7NwaFbC5eGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL: only if you have a multimodal model client that can accept images\n",
        "# You can keep it disabled by default.\n",
        "\n",
        "USE_MAP_LLM = False  # set True only if you have an image-capable model client\n",
        "\n",
        "def llm_map_insight(country, map_path):\n",
        "    if not USE_MAP_LLM:\n",
        "        return None\n",
        "\n",
        "    # Example placeholder: adapt to your multimodal client API\n",
        "    # Many multimodal APIs require: [{\"type\":\"text\",...}, {\"type\":\"image_url\",...}]\n",
        "    # Here we leave it as a stub so you can plug your provider.\n",
        "    raise NotImplementedError(\"Plug in your multimodal LLM client here.\")"
      ],
      "metadata": {
        "id": "S7vO8MvK5XzW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the PDF (ReportLab)"
      ],
      "metadata": {
        "id": "U9oVZ8pe5h0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Precompute everything (LLM summaries + images) first,\n",
        "# then build the PDF in a separate step.\n",
        "# This makes debugging/layout much faster and avoids re-calling\n",
        "# the LLM / regenerating plots every time you rerun build_pdf().\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak, Table, TableStyle\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib import colors\n",
        "from xml.sax.saxutils import escape as xml_escape\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "PDF_PATH = \"/content/IAEA_Cyclotron_Country_Executive_Summaries.pdf\"\n",
        "styles = getSampleStyleSheet()\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def escape_paragraph_text(s: str) -> str:\n",
        "    \"\"\"\n",
        "    ReportLab Paragraph uses a mini-HTML parser.\n",
        "    Escape &, <, > to prevent malformed markup from breaking rendering.\n",
        "    Keep line breaks by converting '\\n' to '<br/>'.\n",
        "    \"\"\"\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = str(s)\n",
        "    s = xml_escape(s)\n",
        "    return s.replace(\"\\n\", \"<br/>\")\n",
        "\n",
        "def df_to_table(dataframe, max_rows=30):\n",
        "    df2 = dataframe.copy()\n",
        "    if len(df2) > max_rows:\n",
        "        df2 = df2.head(max_rows)\n",
        "\n",
        "    data = [list(df2.reset_index().columns)] + df2.reset_index().values.tolist()\n",
        "    t = Table(data, repeatRows=1)\n",
        "    t.setStyle(TableStyle([\n",
        "        (\"BACKGROUND\", (0,0), (-1,0), colors.lightgrey),\n",
        "        (\"GRID\", (0,0), (-1,-1), 0.25, colors.grey),\n",
        "        (\"FONTNAME\", (0,0), (-1,0), \"Helvetica-Bold\"),\n",
        "        (\"FONTSIZE\", (0,0), (-1,-1), 8),\n",
        "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
        "    ]))\n",
        "    return t\n",
        "\n",
        "def rl_image_fit(img_path, max_w=480, max_h=300):\n",
        "    \"\"\"\n",
        "    Create a ReportLab Image that fits inside (max_w x max_h) while preserving aspect ratio.\n",
        "    max_w/max_h are in points (1 point = 1/72 inch).\n",
        "    \"\"\"\n",
        "    with PILImage.open(img_path) as im:\n",
        "        w, h = im.size\n",
        "\n",
        "    scale = min(max_w / w, max_h / h)\n",
        "    new_w = w * scale\n",
        "    new_h = h * scale\n",
        "    return Image(img_path, width=new_w, height=new_h)\n",
        "\n",
        "# ============================================================\n",
        "# Step A) Precompute LLM summaries into a dict\n",
        "# ============================================================\n",
        "countries = sorted(df[\"Country\"].dropna().unique())\n",
        "\n",
        "summaries_by_country = {}\n",
        "for country in countries:\n",
        "    try:\n",
        "        summaries_by_country[country] = llm_exec_summary(country, top_n=10)\n",
        "    except Exception as e:\n",
        "        summaries_by_country[country] = f\"(LLM summary failed: {e})\"\n",
        "\n",
        "print(f\"Precomputed LLM summaries: {len(summaries_by_country)} countries\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Step B) Precompute images into a dict\n",
        "# Each country -> dict of paths {city, manufacturer, energy, map}\n",
        "# ============================================================\n",
        "images_by_country = {}\n",
        "\n",
        "for country in countries:\n",
        "    imgs = {}\n",
        "\n",
        "    try:\n",
        "        imgs[\"city\"] = save_city_bar_chart(country, top_n=10)\n",
        "    except Exception as e:\n",
        "        imgs[\"city\"] = None\n",
        "\n",
        "    try:\n",
        "        imgs[\"manufacturer\"] = save_manufacturer_bar_chart(country, top_n=10)\n",
        "    except Exception as e:\n",
        "        imgs[\"manufacturer\"] = None\n",
        "\n",
        "    try:\n",
        "        imgs[\"energy\"] = save_energy_hist(country)\n",
        "    except Exception as e:\n",
        "        imgs[\"energy\"] = None\n",
        "\n",
        "    try:\n",
        "        imgs[\"map\"] = save_country_map(country)\n",
        "    except Exception as e:\n",
        "        imgs[\"map\"] = None\n",
        "\n",
        "    images_by_country[country] = imgs\n",
        "\n",
        "print(f\"Precomputed images for: {len(images_by_country)} countries\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Step C) Build PDF using only the precomputed dictionaries\n",
        "# ============================================================\n",
        "def build_pdf():\n",
        "    doc = SimpleDocTemplate(\n",
        "        PDF_PATH,\n",
        "        pagesize=A4,\n",
        "        rightMargin=36,\n",
        "        leftMargin=36,\n",
        "        topMargin=36,\n",
        "        bottomMargin=36\n",
        "    )\n",
        "    story = []\n",
        "\n",
        "    # --- Title\n",
        "    story.append(Paragraph(\"IAEA Cyclotron Database — Executive Summaries by Country\", styles[\"Title\"]))\n",
        "    story.append(Spacer(1, 10))\n",
        "    story.append(Paragraph(\"Automatically generated from the scraped IAEA SharePoint cyclotron list.\", styles[\"Normal\"]))\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    # --- Comparison section\n",
        "    story.append(Paragraph(\"Global Comparison\", styles[\"Heading1\"]))\n",
        "    top_countries, top_manu, energy_country = global_comparison_tables()\n",
        "\n",
        "    story.append(Paragraph(\"Top countries by cyclotron count\", styles[\"Heading2\"]))\n",
        "    story.append(df_to_table(top_countries.to_frame(\"count\")))\n",
        "    story.append(Spacer(1, 10))\n",
        "\n",
        "    story.append(Paragraph(\"Top manufacturers (global)\", styles[\"Heading2\"]))\n",
        "    story.append(df_to_table(top_manu.to_frame(\"count\")))\n",
        "    story.append(Spacer(1, 10))\n",
        "\n",
        "    story.append(Paragraph(\"Energy statistics by country (numeric rows only)\", styles[\"Heading2\"]))\n",
        "    story.append(df_to_table(energy_country))\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    # --- Data quality section\n",
        "    dq = data_quality_summary()\n",
        "    story.append(Paragraph(\"Data Quality Summary\", styles[\"Heading1\"]))\n",
        "    story.append(Paragraph(f\"Total rows: <b>{dq['total_rows']}</b>\", styles[\"Normal\"]))\n",
        "    story.append(Spacer(1, 8))\n",
        "\n",
        "    dq_table_df = pd.DataFrame({\n",
        "        \"missing_count\": dq[\"missing_counts\"],\n",
        "        \"missing_pct\": {k: f\"{v:.1f}%\" for k, v in dq[\"missing_pct\"].items()}\n",
        "    })\n",
        "    story.append(df_to_table(dq_table_df))\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    # --- Per-country summaries\n",
        "    story.append(Paragraph(\"Country Executive Summaries\", styles[\"Heading1\"]))\n",
        "    story.append(Paragraph(\"Each country section includes an LLM-generated summary plus charts and a map.\", styles[\"Normal\"]))\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    for idx, country in enumerate(countries, start=1):\n",
        "        story.append(Paragraph(escape_paragraph_text(country), styles[\"Heading1\"]))\n",
        "        story.append(Spacer(1, 6))\n",
        "\n",
        "        # LLM summary (precomputed)\n",
        "        summary = summaries_by_country.get(country, \"\")\n",
        "        story.append(Paragraph(escape_paragraph_text(summary), styles[\"Normal\"]))\n",
        "        story.append(Spacer(1, 10))\n",
        "\n",
        "        # Images (precomputed)\n",
        "        imgs = images_by_country.get(country, {})\n",
        "\n",
        "        # Put the 4 plots on ONE page in a 2x2 grid\n",
        "        story.append(plots_2x2_block(imgs, cell_w=255, cell_h=185, pad=6))\n",
        "        story.append(Spacer(1, 10))\n",
        "\n",
        "        if idx != len(countries):\n",
        "            story.append(PageBreak())\n",
        "\n",
        "    doc.build(story)\n",
        "\n",
        "build_pdf()\n",
        "PDF_PATH"
      ],
      "metadata": {
        "id": "M2MdRIDj5hAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DflYFSX95mlx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}