{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommasocarzaniga/CNM_CycNucMed/blob/main/EMBA_Exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7iWPXagdQhK"
      },
      "source": [
        "# Cyclotrons for Nuclear Medicine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asugYFtKJUsQ"
      },
      "source": [
        "###Setup\n",
        "First, make the necessary imports.\n",
        "Note that further imports may have to be made in addition to the ones below, if your application uses additional fetures such as loaders and tools. You can find the code for these imports in the respective sections of the tutorial notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6iiGJtSmt1H5"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-community langchain-core langchain-openai langchain-huggingface\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "import pprint\n",
        "import getpass\n",
        "from IPython.display import Markdown\n",
        "import IPython.display as ipd\n",
        "from PIL import Image\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qviEHKsjTZ1a"
      },
      "source": [
        "Then, assign the API keys to be able to use OpenAI, Google Serper, Huggingface, etc.\n",
        "\n",
        "When working with sensitive information like API keys or passwords in Google Colab, it's crucial to handle data securely. As you learnt in the tutorial session, two common approaches for this are using **Colab's Secrets Manager**, which stores and retrieves secrets without exposing them in the notebook, and `getpass`, a Python function that securely prompts users to input secrets during runtime without showing them. Both methods help ensure your sensitive data remains protected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9h1QKVPyTYTL"
      },
      "outputs": [],
      "source": [
        "#You can remove the keys you will not use\n",
        "\n",
        "#When using Colab Secret Manager\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "#When using getpass\n",
        "#os.environ['OPENAI_API_KEY'] = getpass.getpass()\n",
        "\n",
        "#When using Colab Secret Manager\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n",
        "#When using getpass\n",
        "#os.environ['SERPER_API_KEY'] = getpass.getpass()\n",
        "\n",
        "#When using Colab Secret Manager\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "#When using getpass\n",
        "#os.environ['HF_TOKEN'] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do I have to install this:"
      ],
      "metadata": {
        "id": "gPChC-Eh4bLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y \\\n",
        "  libatk1.0-0 \\\n",
        "  libatk-bridge2.0-0 \\\n",
        "  libcups2 \\\n",
        "  libdrm2 \\\n",
        "  libxkbcommon0 \\\n",
        "  libxcomposite1 \\\n",
        "  libxdamage1 \\\n",
        "  libxfixes3 \\\n",
        "  libxrandr2 \\\n",
        "  libgbm1 \\\n",
        "  libpango-1.0-0 \\\n",
        "  libcairo2 \\\n",
        "  libasound2"
      ],
      "metadata": {
        "id": "oz0G5ijXdJwh",
        "outputId": "54fe216c-0e60-4b40-f35c-007a440240e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.92.22)] [Connecting to security.ub\r0% [Connecting to archive.ubuntu.com (91.189.92.22)] [Connecting to security.ub\r                                                                               \rGet:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cli.github.com/packages stable/main amd64 Packages [354 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,302 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,869 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,619 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,600 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [45.0 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,637 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,968 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Fetched 38.6 MB in 3s (12.2 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libasound2 set to manually installed.\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo2 set to manually installed.\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxdamage1 set to manually installed.\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxfixes3 set to manually installed.\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxkbcommon0 set to manually installed.\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "libxrandr2 set to manually installed.\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.16).\n",
            "libcups2 set to manually installed.\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libdrm2 set to manually installed.\n",
            "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libgbm1 set to manually installed.\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libpango-1.0-0 set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core gsettings-desktop-schemas libatk1.0-data libatspi2.0-0 libxtst6\n",
            "  session-migration\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core gsettings-desktop-schemas libatk-bridge2.0-0 libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libxcomposite1 libxtst6 session-migration\n",
            "0 upgraded, 9 newly installed, 0 to remove and 95 not upgraded.\n",
            "Need to get 318 kB of archives.\n",
            "After this operation, 1,497 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Fetched 318 kB in 1s (265 kB/s)\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../1-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../2-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../3-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../4-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../5-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../6-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../7-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../8-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service → /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do I have to install this:"
      ],
      "metadata": {
        "id": "ECg-VASV4fWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright pandas\n",
        "!playwright install chromium"
      ],
      "metadata": {
        "id": "4s5mABJXbxwQ",
        "outputId": "7b466a52-a9b4-494a-a289-83ddba3d65e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pyee<14,>=13 (from playwright)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl (46.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "Successfully installed playwright-1.57.0 pyee-13.0.0\n",
            "Downloading Chromium 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-linux.zip\u001b[22m\n",
            "(node:8872) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G164.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 0% 40.7s\u001b[0K\u001b[1G164.7 MiB [] 0% 9.9s\u001b[0K\u001b[1G164.7 MiB [] 0% 8.4s\u001b[0K\u001b[1G164.7 MiB [] 1% 3.7s\u001b[0K\u001b[1G164.7 MiB [] 2% 3.0s\u001b[0K\u001b[1G164.7 MiB [] 3% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 4% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 5% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 6% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 8% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 9% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 10% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 12% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 13% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 14% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 15% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 16% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 18% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 19% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 20% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 21% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 23% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 24% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 26% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 27% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 29% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 31% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 33% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 35% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 36% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 38% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 40% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 41% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 43% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 45% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 46% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 47% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 49% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 50% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 52% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 54% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 56% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 57% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 59% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 61% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 63% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 64% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 66% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 67% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 68% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 69% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 70% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 71% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 72% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 73% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 74% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 76% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 77% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 79% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 80% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 82% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 83% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 85% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 86% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 88% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 90% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 96% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium-1200\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "(node:8905) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 11% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 44% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 80% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Downloading Chromium Headless Shell 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-headless-shell-linux.zip\u001b[22m\n",
            "(node:8920) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G109.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 0% 32.2s\u001b[0K\u001b[1G109.7 MiB [] 0% 12.3s\u001b[0K\u001b[1G109.7 MiB [] 0% 10.0s\u001b[0K\u001b[1G109.7 MiB [] 1% 6.1s\u001b[0K\u001b[1G109.7 MiB [] 2% 3.6s\u001b[0K\u001b[1G109.7 MiB [] 3% 2.7s\u001b[0K\u001b[1G109.7 MiB [] 5% 2.2s\u001b[0K\u001b[1G109.7 MiB [] 5% 2.1s\u001b[0K\u001b[1G109.7 MiB [] 6% 2.2s\u001b[0K\u001b[1G109.7 MiB [] 7% 2.0s\u001b[0K\u001b[1G109.7 MiB [] 9% 1.8s\u001b[0K\u001b[1G109.7 MiB [] 10% 1.7s\u001b[0K\u001b[1G109.7 MiB [] 11% 1.6s\u001b[0K\u001b[1G109.7 MiB [] 13% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 14% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 15% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 17% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 18% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 20% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 21% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 22% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 23% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 24% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 25% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 26% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 28% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 29% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 31% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 32% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 34% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 35% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 36% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 37% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 38% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 39% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 40% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 41% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 43% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 44% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 46% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 48% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 50% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 52% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 53% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 54% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 56% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 57% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 59% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 60% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 61% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 62% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 64% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 65% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 66% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 68% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 69% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 70% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 71% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 73% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 74% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 76% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 78% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 79% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 81% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 82% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 83% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 85% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 86% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 87% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 89% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 90% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 92% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What am I doing here:"
      ],
      "metadata": {
        "id": "Bd2vY8px4iCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def test_playwright():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        await page.goto(\"https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx\")\n",
        "        print(await page.title())\n",
        "        await browser.close()\n",
        "\n",
        "await test_playwright()\n"
      ],
      "metadata": {
        "id": "jy6Mjd2ebs3H",
        "outputId": "434d07a7-4052-49b7-af00-56840aca61fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pages - Cyclotrons used for Radionuclide Production\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web scraping of the IAEA cyclotron database\n",
        "\n",
        "This script automatically extracts structured data from the IAEA public web page\n",
        "listing cyclotron facilities worldwide:\n",
        "https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx\n",
        "\n",
        "The website is implemented using SharePoint and loads data dynamically across\n",
        "multiple pages. Because of this, a simple HTTP request is not sufficient and a\n",
        "browser automation tool (Playwright) is used to simulate real user navigation.\n",
        "\n",
        "Main features of the script:\n",
        "- Uses Playwright (async) to control a headless Chromium browser.\n",
        "- Navigates through all pages of the table by clicking the \"Next\" button.\n",
        "- Extracts tabular data from each page.\n",
        "- Cleans and normalizes rows to handle SharePoint quirks (e.g. header rows\n",
        "  injected into the table body, extra cells in the first row of each page).\n",
        "- Deduplicates rows using a hash-based fingerprint.\n",
        "- Stores the final structured dataset into a CSV file.\n",
        "\n",
        "Extracted fields:\n",
        "- Country\n",
        "- City\n",
        "- Facility\n",
        "- Manufacturer\n",
        "- Model\n",
        "- Proton energy (MeV)\n",
        "\n",
        "The final output is saved as:\n",
        "iaea_cyclotrons_normalized.csv\n",
        "\n",
        "This approach demonstrates:\n",
        "- Practical web scraping of JavaScript-heavy websites\n",
        "- Asynchronous programming in Python\n",
        "- Robust data cleaning\n",
        "- Reproducible data extraction for research purposes"
      ],
      "metadata": {
        "id": "fOJasaYn4uNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# IAEA Cyclotron List Scraper (SharePoint) — Colab-ready\n",
        "# ------------------------------------------------------------\n",
        "# What this does:\n",
        "# - Opens the IAEA SharePoint list view of cyclotrons\n",
        "# - Iterates through pages by clicking the \"Next\" button\n",
        "# - Extracts the table rows efficiently (one JS call per page)\n",
        "# - Cleans SharePoint quirks (header labels inside tbody, multi-line / tabbed cells)\n",
        "# - Deduplicates rows (hash fingerprint)\n",
        "# - Saves a CSV sorted by Country and City\n",
        "#\n",
        "# Output:\n",
        "#   /content/iaea_cyclotrons.csv\n",
        "# ============================================================\n",
        "\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "from playwright.async_api import async_playwright\n",
        "import os\n",
        "import hashlib\n",
        "import re\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "BASE_URL = \"https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx\"\n",
        "HASH_PREFIX = \"#InplviewHashd5afe566-18ad-4ac0-8aeb-ccf833dbc282=\"\n",
        "OUTPUT_CSV = os.path.join(os.getcwd(), \"iaea_cyclotrons.csv\")\n",
        "\n",
        "# Expected columns in the IAEA list view\n",
        "EXPECTED_COLS = 6  # Country, City, Facility, Manufacturer, Model, Proton energy (MeV)\n",
        "\n",
        "# SharePoint sometimes injects these header labels inside table rows\n",
        "HEADER_LABELS = [\n",
        "    \"country\", \"city\", \"facility\", \"manufacturer\", \"model\",\n",
        "    \"proton energy (mev)\", \"proton energy\"\n",
        "]\n",
        "HEADER_SET = set(HEADER_LABELS)\n",
        "\n",
        "# IMPORTANT: target the actual SharePoint \"list view\" table (reduces missing rows)\n",
        "TABLE_ROW_SELECTOR = \"table.ms-listviewtable tbody tr\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers for cleaning/parsing\n",
        "# -----------------------------\n",
        "def norm_text(s: str) -> str:\n",
        "    \"\"\"Normalize text for comparisons.\"\"\"\n",
        "    return \" \".join((s or \"\").split()).strip().lower()\n",
        "\n",
        "\n",
        "def row_fingerprint(cells):\n",
        "    \"\"\"\n",
        "    Make a stable hash of the cleaned row values.\n",
        "    Used to deduplicate rows across pages (SharePoint sometimes repeats).\n",
        "    \"\"\"\n",
        "    norm = [\" \".join((c or \"\").split()) for c in cells]\n",
        "    return hashlib.md5(\" | \".join(norm).encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "\n",
        "def strip_header_prefix(cell: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert 'City: Vienna' -> 'Vienna' (for known header labels).\n",
        "    Sometimes SharePoint prepends labels inside a cell.\n",
        "    \"\"\"\n",
        "    c = (cell or \"\").strip()\n",
        "    c_norm = norm_text(c)\n",
        "    for h in HEADER_LABELS:\n",
        "        if re.match(rf\"^{re.escape(h)}(\\s*[:\\-]?\\s+)\", c_norm):\n",
        "            return re.sub(rf\"(?i)^{re.escape(h)}\\s*[:\\-]?\\s+\", \"\", c).strip()\n",
        "    return c\n",
        "\n",
        "\n",
        "def flatten_multiline_cells(raw_cells):\n",
        "    \"\"\"\n",
        "    SharePoint quirks:\n",
        "    - multiple values in one cell separated by tabs\n",
        "    - multiple values separated by newlines\n",
        "    This function splits on tabs/newlines and flattens into a token list.\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    for c in raw_cells:\n",
        "        if not c:\n",
        "            continue\n",
        "        parts = re.split(r\"[\\t\\r\\n]+\", str(c))\n",
        "        for part in parts:\n",
        "            part = part.strip()\n",
        "            if part:\n",
        "                tokens.append(part)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def clean_and_align_tokens(raw_cells):\n",
        "    \"\"\"\n",
        "    Convert raw table cells -> exactly 6 cleaned fields.\n",
        "    Strategy:\n",
        "    - flatten tabs/newlines\n",
        "    - remove header-label tokens\n",
        "    - strip 'Label: value' prefixes\n",
        "    - if too many tokens remain, pick the best contiguous window of length 6\n",
        "    \"\"\"\n",
        "    tokens = flatten_multiline_cells(raw_cells)\n",
        "\n",
        "    processed = []\n",
        "    for t in tokens:\n",
        "        t_norm = norm_text(t)\n",
        "        if t_norm in HEADER_SET:\n",
        "            continue\n",
        "\n",
        "        t2 = strip_header_prefix(t)\n",
        "        t2_norm = norm_text(t2)\n",
        "        if not t2 or t2_norm in HEADER_SET:\n",
        "            continue\n",
        "\n",
        "        processed.append(t2.strip())\n",
        "\n",
        "    # Not enough data to form a row\n",
        "    if len(processed) < EXPECTED_COLS:\n",
        "        return None\n",
        "\n",
        "    # Exactly the correct number of fields\n",
        "    if len(processed) == EXPECTED_COLS:\n",
        "        # Drop if it still looks like a header row\n",
        "        if any(norm_text(x) in HEADER_SET for x in processed):\n",
        "            return None\n",
        "        return processed\n",
        "\n",
        "    # More than 6 tokens => choose the best \"slice\" of 6 tokens\n",
        "    def badness(x: str) -> int:\n",
        "        xn = norm_text(x)\n",
        "        if xn in HEADER_SET:\n",
        "            return 100\n",
        "        if xn.isdigit():  # sometimes SharePoint injects numeric IDs\n",
        "            return 10\n",
        "        if any(xn.startswith(h + \" \") for h in HEADER_LABELS):  # e.g. \"city zurich\"\n",
        "            return 10\n",
        "        return 0\n",
        "\n",
        "    best_window = None\n",
        "    best_score = None\n",
        "\n",
        "    for start in range(0, len(processed) - EXPECTED_COLS + 1):\n",
        "        window = processed[start:start + EXPECTED_COLS]\n",
        "        if any(norm_text(w) in HEADER_SET for w in window):\n",
        "            continue\n",
        "\n",
        "        score = sum(badness(w) for w in window)\n",
        "        if best_score is None or score < best_score:\n",
        "            best_score = score\n",
        "            best_window = window\n",
        "\n",
        "    return best_window\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Page navigation helpers\n",
        "# -----------------------------\n",
        "async def wait_for_table_refresh(page, prev_first_row_text, timeout_ms=20000):\n",
        "    \"\"\"\n",
        "    After clicking Next, wait until the first row content changes.\n",
        "    This is more reliable than a fixed sleep on SharePoint pages.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        await page.wait_for_function(\n",
        "            \"\"\"(prev) => {\n",
        "                const r = document.querySelector('table.ms-listviewtable tbody tr');\n",
        "                return r && r.innerText && r.innerText !== prev;\n",
        "            }\"\"\",\n",
        "            arg=prev_first_row_text,\n",
        "            timeout=timeout_ms\n",
        "        )\n",
        "    except:\n",
        "        # Fallback: small sleep if SharePoint is slow\n",
        "        await page.wait_for_timeout(1200)\n",
        "\n",
        "\n",
        "async def robust_click(el):\n",
        "    \"\"\"\n",
        "    SharePoint \"Next\" can be visible but outside viewport.\n",
        "    Try:\n",
        "    1) scroll into view\n",
        "    2) force-click\n",
        "    3) JS click as fallback\n",
        "    \"\"\"\n",
        "    try:\n",
        "        await el.scroll_into_view_if_needed()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        await el.click(force=True, timeout=20000)\n",
        "        return True\n",
        "    except:\n",
        "        try:\n",
        "            await el.evaluate(\"node => node.click()\")\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main scraper\n",
        "# -----------------------------\n",
        "async def scrape_all_pages():\n",
        "    all_rows = []\n",
        "    seen_rows = set()\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        # Launch headless browser\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "\n",
        "        # Create a browser context and block heavy resources for speed\n",
        "        context = await browser.new_context()\n",
        "        await context.route(\n",
        "            \"**/*\",\n",
        "            lambda route: route.abort()\n",
        "            if route.request.resource_type in (\"image\", \"font\", \"media\", \"stylesheet\")\n",
        "            else route.continue_()\n",
        "        )\n",
        "\n",
        "        page = await context.new_page()\n",
        "        page.set_default_timeout(20000)\n",
        "\n",
        "        # Open the first page (hash-based view)\n",
        "        url = BASE_URL + HASH_PREFIX\n",
        "        print(f\"Loading first page: {url}\")\n",
        "        await page.goto(url, timeout=60000, wait_until=\"domcontentloaded\")\n",
        "\n",
        "        page_index = 0\n",
        "\n",
        "        while True:\n",
        "            page_index += 1\n",
        "            print(f\"\\n--- Page {page_index} ---\")\n",
        "\n",
        "            # Ensure list view rows exist\n",
        "            try:\n",
        "                await page.wait_for_selector(TABLE_ROW_SELECTOR, timeout=20000)\n",
        "            except:\n",
        "                print(\"No list view table rows found → stopping\")\n",
        "                break\n",
        "\n",
        "            # Extract all rows from the list view in ONE browser call (fast)\n",
        "            raw_rows = await page.eval_on_selector_all(\n",
        "                TABLE_ROW_SELECTOR,\n",
        "                \"\"\"trs => trs.map(tr =>\n",
        "                    Array.from(tr.querySelectorAll('td')).map(td => td.innerText)\n",
        "                )\"\"\"\n",
        "            )\n",
        "\n",
        "            print(f\"Rows extracted from DOM this page: {len(raw_rows)}\")\n",
        "\n",
        "            new_rows_this_page = 0\n",
        "            kept_after_parsing = 0\n",
        "\n",
        "            # Parse each extracted row\n",
        "            for raw_cells in raw_rows:\n",
        "                cells = clean_and_align_tokens(raw_cells)\n",
        "                if cells is None or len(cells) != EXPECTED_COLS:\n",
        "                    continue\n",
        "\n",
        "                # Safety: never allow header labels as data\n",
        "                if any(norm_text(x) in HEADER_SET for x in cells):\n",
        "                    continue\n",
        "\n",
        "                kept_after_parsing += 1\n",
        "\n",
        "                # Deduplicate across pages\n",
        "                fp = row_fingerprint(cells)\n",
        "                if fp in seen_rows:\n",
        "                    continue\n",
        "\n",
        "                seen_rows.add(fp)\n",
        "                new_rows_this_page += 1\n",
        "\n",
        "                all_rows.append({\n",
        "                    \"Country\": cells[0],\n",
        "                    \"City\": cells[1],\n",
        "                    \"Facility\": cells[2],\n",
        "                    \"Manufacturer\": cells[3],\n",
        "                    \"Model\": cells[4],\n",
        "                    \"Proton energy (MeV)\": cells[5],\n",
        "                })\n",
        "\n",
        "            print(f\"Rows kept after parsing this page: {kept_after_parsing}\")\n",
        "            print(f\"New unique rows added this page: {new_rows_this_page}\")\n",
        "            print(f\"Total unique rows so far: {len(all_rows)}\")\n",
        "\n",
        "            # Capture first row text to detect refresh after clicking Next\n",
        "            try:\n",
        "                prev_first = await page.locator(TABLE_ROW_SELECTOR).first.inner_text()\n",
        "            except:\n",
        "                prev_first = \"\"\n",
        "\n",
        "            # Find \"Next\" and click it\n",
        "            next_el = page.locator(\n",
        "                'a[title=\"Next\"], a[aria-label=\"Next\"], a[title=\"Next page\"], a[aria-label=\"Next page\"], a:has-text(\"Next\")'\n",
        "            ).first\n",
        "\n",
        "            # Stop if no next page\n",
        "            if await next_el.count() == 0 or not await next_el.is_visible() or not await next_el.is_enabled():\n",
        "                print(\"No Next button → stopping\")\n",
        "                break\n",
        "\n",
        "            ok = await robust_click(next_el)\n",
        "            if not ok:\n",
        "                print(\"Could not click Next → stopping\")\n",
        "                break\n",
        "\n",
        "            # Wait for the table to refresh\n",
        "            await wait_for_table_refresh(page, prev_first)\n",
        "\n",
        "        await context.close()\n",
        "        await browser.close()\n",
        "\n",
        "    # Build final DataFrame, sort, and save\n",
        "    df = pd.DataFrame(all_rows)\n",
        "    df = df.sort_values([\"Country\", \"City\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "    df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "    print(\"\\nDONE\")\n",
        "    print(f\"Saved {len(df)} unique cyclotron rows to:\")\n",
        "    print(OUTPUT_CSV)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Run in Colab (top-level await is supported in Colab notebooks)\n",
        "# -----------------------------\n",
        "await scrape_all_pages()"
      ],
      "metadata": {
        "id": "Ypt70A4qdvOL",
        "outputId": "c8040167-617e-4f69-c395-315300deff46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading first page: https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx#InplviewHashd5afe566-18ad-4ac0-8aeb-ccf833dbc282=\n",
            "\n",
            "--- Page 1 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 30\n",
            "\n",
            "--- Page 2 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 29\n",
            "Total unique rows so far: 59\n",
            "\n",
            "--- Page 3 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 89\n",
            "\n",
            "--- Page 4 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 24\n",
            "Total unique rows so far: 113\n",
            "\n",
            "--- Page 5 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 140\n",
            "\n",
            "--- Page 6 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 168\n",
            "\n",
            "--- Page 7 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 193\n",
            "\n",
            "--- Page 8 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 221\n",
            "\n",
            "--- Page 9 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 249\n",
            "\n",
            "--- Page 10 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 29\n",
            "Total unique rows so far: 278\n",
            "\n",
            "--- Page 11 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 306\n",
            "\n",
            "--- Page 12 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 26\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 332\n",
            "\n",
            "--- Page 13 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 360\n",
            "\n",
            "--- Page 14 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 390\n",
            "\n",
            "--- Page 15 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 420\n",
            "\n",
            "--- Page 16 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 24\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 442\n",
            "\n",
            "--- Page 17 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 26\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 464\n",
            "\n",
            "--- Page 18 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 491\n",
            "\n",
            "--- Page 19 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 518\n",
            "\n",
            "--- Page 20 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 540\n",
            "\n",
            "--- Page 21 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 562\n",
            "\n",
            "--- Page 22 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 587\n",
            "\n",
            "--- Page 23 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 615\n",
            "\n",
            "--- Page 24 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 643\n",
            "\n",
            "--- Page 25 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 19\n",
            "New unique rows added this page: 16\n",
            "Total unique rows so far: 659\n",
            "\n",
            "--- Page 26 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 18\n",
            "New unique rows added this page: 15\n",
            "Total unique rows so far: 674\n",
            "\n",
            "--- Page 27 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 27\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 701\n",
            "\n",
            "--- Page 28 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 23\n",
            "New unique rows added this page: 21\n",
            "Total unique rows so far: 722\n",
            "\n",
            "--- Page 29 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 748\n",
            "\n",
            "--- Page 30 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 774\n",
            "\n",
            "--- Page 31 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 27\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 800\n",
            "\n",
            "--- Page 32 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 826\n",
            "\n",
            "--- Page 33 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 19\n",
            "Total unique rows so far: 845\n",
            "\n",
            "--- Page 34 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 870\n",
            "\n",
            "--- Page 35 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 892\n",
            "\n",
            "--- Page 36 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 23\n",
            "Total unique rows so far: 915\n",
            "\n",
            "--- Page 37 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 940\n",
            "\n",
            "--- Page 38 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 965\n",
            "\n",
            "--- Page 39 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 990\n",
            "\n",
            "--- Page 40 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 27\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 1017\n",
            "\n",
            "--- Page 41 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 1045\n",
            "\n",
            "--- Page 42 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 1073\n",
            "\n",
            "--- Page 43 ---\n",
            "Rows extracted from DOM this page: 20\n",
            "Rows kept after parsing this page: 20\n",
            "New unique rows added this page: 20\n",
            "Total unique rows so far: 1093\n",
            "No Next button → stopping\n",
            "\n",
            "DONE\n",
            "Saved 1093 unique cyclotron rows to:\n",
            "/content/iaea_cyclotrons.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the first part of the multimodal\n",
        "\n",
        "Creation of the function: print_country_report"
      ],
      "metadata": {
        "id": "16Js_U3pdyC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# 1) Load your CSV\n",
        "# =========================\n",
        "CSV_PATH = \"/content/iaea_cyclotrons.csv\"   # <- adjust if different\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Normalize whitespace (helpful for grouping)\n",
        "for col in [\"Country\",\"City\",\"Facility\",\"Manufacturer\",\"Model\",\"Proton energy (MeV)\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "# =========================\n",
        "# 2) Helper: parse energy to numeric (best-effort)\n",
        "#    Handles: \"11\", \"16.5\", \"16-18\", \"16 / 18\", etc.\n",
        "# =========================\n",
        "def parse_energy_to_float(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in (\"nan\", \"none\"):\n",
        "        return None\n",
        "    # find numbers in the string\n",
        "    nums = re.findall(r\"\\d+(?:\\.\\d+)?\", s)\n",
        "    if not nums:\n",
        "        return None\n",
        "    # if range-like, take the max (or change to avg if you prefer)\n",
        "    vals = [float(n) for n in nums]\n",
        "    return max(vals)\n",
        "\n",
        "df[\"Energy_num\"] = df[\"Proton energy (MeV)\"].apply(parse_energy_to_float)\n",
        "\n",
        "# =========================\n",
        "# 3) Country summary function\n",
        "# =========================\n",
        "def country_summary(country, top_n=15):\n",
        "    \"\"\"\n",
        "    Return a structured summary for a country:\n",
        "    - total cyclotrons\n",
        "    - cities list + counts\n",
        "    - facilities list + counts\n",
        "    - manufacturer / model breakdown\n",
        "    - energy stats\n",
        "    \"\"\"\n",
        "    # case-insensitive match\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    if sub.empty:\n",
        "        # fuzzy suggestion: show close matches by containment\n",
        "        candidates = df[df[\"Country\"].str.lower().str.contains(str(country).strip().lower(), na=False)][\"Country\"].unique()\n",
        "        return {\n",
        "            \"country\": country,\n",
        "            \"found\": False,\n",
        "            \"message\": f\"No exact match for '{country}'.\",\n",
        "            \"did_you_mean\": sorted(candidates)[:20]\n",
        "        }\n",
        "\n",
        "    total = len(sub)\n",
        "\n",
        "    cities = (sub.groupby(\"City\")\n",
        "                .size()\n",
        "                .sort_values(ascending=False))\n",
        "\n",
        "    facilities = (sub.groupby(\"Facility\")\n",
        "                    .size()\n",
        "                    .sort_values(ascending=False))\n",
        "\n",
        "    manufacturers = (sub.groupby(\"Manufacturer\")\n",
        "                       .size()\n",
        "                       .sort_values(ascending=False))\n",
        "\n",
        "    models = (sub.groupby(\"Model\")\n",
        "                .size()\n",
        "                .sort_values(ascending=False))\n",
        "\n",
        "    # manufacturer-model combo\n",
        "    manu_model = (sub.groupby([\"Manufacturer\",\"Model\"])\n",
        "                    .size()\n",
        "                    .sort_values(ascending=False))\n",
        "\n",
        "    energy_stats = {\n",
        "        \"count_numeric\": int(sub[\"Energy_num\"].notna().sum()),\n",
        "        \"min\": float(sub[\"Energy_num\"].min()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "        \"median\": float(sub[\"Energy_num\"].median()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "        \"max\": float(sub[\"Energy_num\"].max()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"country\": country,\n",
        "        \"found\": True,\n",
        "        \"total_cyclotrons\": total,\n",
        "        \"cities_top\": cities.head(top_n),\n",
        "        \"facilities_top\": facilities.head(top_n),\n",
        "        \"manufacturers\": manufacturers,\n",
        "        \"models_top\": models.head(top_n),\n",
        "        \"manufacturer_model_top\": manu_model.head(top_n),\n",
        "        \"energy_stats\": energy_stats,\n",
        "        \"all_cities_count\": int(cities.shape[0]),\n",
        "        \"all_facilities_count\": int(facilities.shape[0]),\n",
        "    }\n",
        "\n",
        "# =========================\n",
        "# 4) Pretty-print function (human readable)\n",
        "# =========================\n",
        "def print_country_report(country, top_n=10):\n",
        "    out = country_summary(country, top_n=top_n)\n",
        "    if not out[\"found\"]:\n",
        "        print(out[\"message\"])\n",
        "        if out.get(\"did_you_mean\"):\n",
        "            print(\"Did you mean one of these?\")\n",
        "            for c in out[\"did_you_mean\"]:\n",
        "                print(\" -\", c)\n",
        "        return\n",
        "\n",
        "    print(f\"=== {out['country']} ===\")\n",
        "    print(f\"Total cyclotrons: {out['total_cyclotrons']}\")\n",
        "    print(f\"Cities covered: {out['all_cities_count']}\")\n",
        "    print(f\"Facilities covered: {out['all_facilities_count']}\")\n",
        "    print()\n",
        "\n",
        "    es = out[\"energy_stats\"]\n",
        "    print(\"Energy (MeV) stats (numeric rows only):\")\n",
        "    print(f\"  numeric entries: {es['count_numeric']}\")\n",
        "    print(f\"  min / median / max: {es['min']} / {es['median']} / {es['max']}\")\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} cities by count:\")\n",
        "    print(out[\"cities_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} facilities by count:\")\n",
        "    print(out[\"facilities_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(\"Manufacturer counts:\")\n",
        "    print(out[\"manufacturers\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} models:\")\n",
        "    print(out[\"models_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} (Manufacturer, Model) pairs:\")\n",
        "    print(out[\"manufacturer_model_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "# =========================\n",
        "# 5) Example usage\n",
        "# =========================\n",
        "print_country_report(\"Switzerland\", top_n=10)"
      ],
      "metadata": {
        "id": "CYd5RFlyGaoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d7bcd5-7ded-48c1-d37f-d7935f0d5cbf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Switzerland ===\n",
            "Total cyclotrons: 4\n",
            "Cities covered: 4\n",
            "Facilities covered: 4\n",
            "\n",
            "Energy (MeV) stats (numeric rows only):\n",
            "  numeric entries: 4\n",
            "  min / median / max: 16.0 / 17.0 / 18.0\n",
            "\n",
            "Top 10 cities by count:\n",
            "City\n",
            "Bern         1\n",
            "Genève       1\n",
            "Schlieren    1\n",
            "Zürich       1\n",
            "\n",
            "Top 10 facilities by count:\n",
            "Facility\n",
            "SWAN / Uni. Bern                                                             1\n",
            "Universitätsspital Zueurich, Labor Schlieren, Klinik für Onkologie (Wagi)    1\n",
            "Universitätsspital Zürich (USZ)                                              1\n",
            "unspecified                                                                  1\n",
            "\n",
            "Manufacturer counts:\n",
            "Manufacturer\n",
            "GE     2\n",
            "IBA    2\n",
            "\n",
            "Top 10 models:\n",
            "Model\n",
            "PETtrace            2\n",
            "CYCLONE 18          1\n",
            "CYCLONE 18/18 HC    1\n",
            "\n",
            "Top 10 (Manufacturer, Model) pairs:\n",
            "Manufacturer  Model           \n",
            "GE            PETtrace            2\n",
            "IBA           CYCLONE 18          1\n",
            "              CYCLONE 18/18 HC    1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora altro"
      ],
      "metadata": {
        "id": "q28_7bbDrUKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up OpenAI LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(temperature=0.9, model=\"gpt-4.1-mini\")\n"
      ],
      "metadata": {
        "id": "d50cJdyarBTD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the structured report into a prompt for the LLM"
      ],
      "metadata": {
        "id": "iX0CDcARsldc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def country_report_for_llm(country, top_n=10):\n",
        "    out = country_summary(country, top_n=top_n)\n",
        "\n",
        "    if not out[\"found\"]:\n",
        "        return f\"No data found for country: {country}\"\n",
        "\n",
        "    # Convert tables into readable text blocks\n",
        "    cities_text = out[\"cities_top\"].to_string()\n",
        "    facilities_text = out[\"facilities_top\"].to_string()\n",
        "    manufacturers_text = out[\"manufacturers\"].to_string()\n",
        "    models_text = out[\"models_top\"].to_string()\n",
        "\n",
        "    es = out[\"energy_stats\"]\n",
        "    energy_text = (\n",
        "        f\"Numeric entries: {es['count_numeric']}\\n\"\n",
        "        f\"Min energy: {es['min']}\\n\"\n",
        "        f\"Median energy: {es['median']}\\n\"\n",
        "        f\"Max energy: {es['max']}\"\n",
        "    )\n",
        "\n",
        "    # Final report string fed to the LLM\n",
        "    report = f\"\"\"\n",
        "IAEA Cyclotron Dataset Report for {out['country']}\n",
        "\n",
        "Total cyclotrons: {out['total_cyclotrons']}\n",
        "Cities covered: {out['all_cities_count']}\n",
        "Facilities covered: {out['all_facilities_count']}\n",
        "\n",
        "Top cities:\n",
        "{cities_text}\n",
        "\n",
        "Top facilities:\n",
        "{facilities_text}\n",
        "\n",
        "Manufacturers:\n",
        "{manufacturers_text}\n",
        "\n",
        "Top models:\n",
        "{models_text}\n",
        "\n",
        "Energy statistics:\n",
        "{energy_text}\n",
        "\"\"\"\n",
        "\n",
        "    return report.strip()"
      ],
      "metadata": {
        "id": "Sua-G7FSrkZz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use it with LLM syntax"
      ],
      "metadata": {
        "id": "HgrQPVXKsq3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "country = \"Italy\"\n",
        "\n",
        "report_text = country_report_for_llm(country)\n",
        "\n",
        "response = llm.invoke(\n",
        "    f\"\"\"\n",
        "You are a market analyst in nuclear medicine.\n",
        "Write a concise executive summary (max 150 words) based on this dataset report.\n",
        "\n",
        "Focus on:\n",
        "- Overall infrastructure scale\n",
        "- Geographic concentration\n",
        "- Major suppliers\n",
        "- Any notable patterns\n",
        "\n",
        "Report:\n",
        "{report_text}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "id": "WVJruj2Sr6x3",
        "outputId": "e85fdd2a-8eb4-4923-f99f-3aacbc2e3759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Italy’s nuclear medicine infrastructure comprises 40 cyclotrons distributed across 30 cities and 39 facilities, indicating a broad yet moderately concentrated network. Milan and Rome lead with four cyclotrons each, followed by Naples with three, highlighting key urban hubs. Major suppliers dominate the market, with GE providing half (20) of the cyclotrons, followed by IBA (13) and Siemens (4), reflecting strong supplier concentration. The most common models are GE’s PETtrace (11 units) and IBA’s CYCLONE 18 and MiniTrace (18 combined), underscoring reliance on established, high-energy cyclotrons (median energy 16 MeV). Notably, dual-unit facilities like Castelfranco Veneto Radiopharmacy enhance regional production capacity. The energy range (10-19 MeV) supports diverse radionuclide production, vital for diagnostic and therapeutic applications. Overall, Italy’s cyclotron network exhibits strategic urban concentration, supplier dominance by GE and IBA, and a preference for mid-to-high energy models, positioning the country as a robust player in nuclear medicine infrastructure."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QB_LeP0YswpW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}