{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommasocarzaniga/CNM_CycNucMed/blob/main/EMBA_Exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7iWPXagdQhK"
      },
      "source": [
        "# Cyclotrons for Nuclear Medicine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asugYFtKJUsQ"
      },
      "source": [
        "###Setup\n",
        "First, make the necessary imports.\n",
        "Note that further imports may have to be made in addition to the ones below, if your application uses additional fetures such as loaders and tools. You can find the code for these imports in the respective sections of the tutorial notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6iiGJtSmt1H5"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-community langchain-core langchain-openai langchain-huggingface\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "import pprint\n",
        "import getpass\n",
        "from IPython.display import Markdown\n",
        "import IPython.display as ipd\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qviEHKsjTZ1a"
      },
      "source": [
        "Then, assign the API keys to be able to use OpenAI, Google Serper, Huggingface, etc.\n",
        "\n",
        "When working with sensitive information like API keys or passwords in Google Colab, it's crucial to handle data securely. As you learnt in the tutorial session, two common approaches for this are using **Colab's Secrets Manager**, which stores and retrieves secrets without exposing them in the notebook, and `getpass`, a Python function that securely prompts users to input secrets during runtime without showing them. Both methods help ensure your sensitive data remains protected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9h1QKVPyTYTL"
      },
      "outputs": [],
      "source": [
        "#You can remove the keys you will not use\n",
        "\n",
        "#When using Colab Secret Manager\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "#When using getpass\n",
        "#os.environ['OPENAI_API_KEY'] = getpass.getpass()\n",
        "\n",
        "#When using Colab Secret Manager\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n",
        "#When using getpass\n",
        "#os.environ['SERPER_API_KEY'] = getpass.getpass()\n",
        "\n",
        "#When using Colab Secret Manager\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "#When using getpass\n",
        "#os.environ['HF_TOKEN'] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do I have to install this:"
      ],
      "metadata": {
        "id": "gPChC-Eh4bLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y \\\n",
        "  libatk1.0-0 \\\n",
        "  libatk-bridge2.0-0 \\\n",
        "  libcups2 \\\n",
        "  libdrm2 \\\n",
        "  libxkbcommon0 \\\n",
        "  libxcomposite1 \\\n",
        "  libxdamage1 \\\n",
        "  libxfixes3 \\\n",
        "  libxrandr2 \\\n",
        "  libgbm1 \\\n",
        "  libpango-1.0-0 \\\n",
        "  libcairo2 \\\n",
        "  libasound2"
      ],
      "metadata": {
        "id": "oz0G5ijXdJwh",
        "outputId": "ce0f1934-b207-495b-8db1-85ae175ba7cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.16).\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do I have to install this:"
      ],
      "metadata": {
        "id": "ECg-VASV4fWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright pandas\n",
        "!playwright install chromium"
      ],
      "metadata": {
        "id": "4s5mABJXbxwQ",
        "outputId": "e712c74a-b964-45fa-f020-5ae55151c730",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.12/dist-packages (1.57.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyee<14,>=13 in /usr/local/lib/python3.12/dist-packages (from playwright) (13.0.0)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What am I doing here:"
      ],
      "metadata": {
        "id": "Bd2vY8px4iCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def test_playwright():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        await page.goto(\"https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx\")\n",
        "        print(await page.title())\n",
        "        await browser.close()\n",
        "\n",
        "await test_playwright()\n"
      ],
      "metadata": {
        "id": "jy6Mjd2ebs3H",
        "outputId": "e7b29d1b-5afb-4c6a-be4a-872bcb3422b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pages - Cyclotrons used for Radionuclide Production\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web scraping of the IAEA cyclotron database\n",
        "\n",
        "This script automatically extracts structured data from the IAEA public web page\n",
        "listing cyclotron facilities worldwide:\n",
        "https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx\n",
        "\n",
        "The website is implemented using SharePoint and loads data dynamically across\n",
        "multiple pages. Because of this, a simple HTTP request is not sufficient and a\n",
        "browser automation tool (Playwright) is used to simulate real user navigation.\n",
        "\n",
        "Main features of the script:\n",
        "- Uses Playwright (async) to control a headless Chromium browser.\n",
        "- Navigates through all pages of the table by clicking the \"Next\" button.\n",
        "- Extracts tabular data from each page.\n",
        "- Cleans and normalizes rows to handle SharePoint quirks (e.g. header rows\n",
        "  injected into the table body, extra cells in the first row of each page).\n",
        "- Deduplicates rows using a hash-based fingerprint.\n",
        "- Stores the final structured dataset into a CSV file.\n",
        "\n",
        "Extracted fields:\n",
        "- Country\n",
        "- City\n",
        "- Facility\n",
        "- Manufacturer\n",
        "- Model\n",
        "- Proton energy (MeV)\n",
        "\n",
        "The final output is saved as:\n",
        "iaea_cyclotrons_normalized.csv\n",
        "\n",
        "This approach demonstrates:\n",
        "- Practical web scraping of JavaScript-heavy websites\n",
        "- Asynchronous programming in Python\n",
        "- Robust data cleaning\n",
        "- Reproducible data extraction for research purposes"
      ],
      "metadata": {
        "id": "fOJasaYn4uNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# IAEA Cyclotron List Scraper (SharePoint) — Colab-ready\n",
        "# ------------------------------------------------------------\n",
        "# What this does:\n",
        "# - Opens the IAEA SharePoint list view of cyclotrons\n",
        "# - Iterates through pages by clicking the \"Next\" button\n",
        "# - Extracts the table rows efficiently (one JS call per page)\n",
        "# - Cleans SharePoint quirks (header labels inside tbody, multi-line / tabbed cells)\n",
        "# - Deduplicates rows (hash fingerprint)\n",
        "# - Saves a CSV sorted by Country and City\n",
        "#\n",
        "# Output:\n",
        "#   /content/iaea_cyclotrons.csv\n",
        "# ============================================================\n",
        "\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "from playwright.async_api import async_playwright\n",
        "import os\n",
        "import hashlib\n",
        "import re\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "BASE_URL = \"https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx\"\n",
        "HASH_PREFIX = \"#InplviewHashd5afe566-18ad-4ac0-8aeb-ccf833dbc282=\"\n",
        "OUTPUT_CSV = os.path.join(os.getcwd(), \"iaea_cyclotrons.csv\")\n",
        "\n",
        "# Expected columns in the IAEA list view\n",
        "EXPECTED_COLS = 6  # Country, City, Facility, Manufacturer, Model, Proton energy (MeV)\n",
        "\n",
        "# SharePoint sometimes injects these header labels inside table rows\n",
        "HEADER_LABELS = [\n",
        "    \"country\", \"city\", \"facility\", \"manufacturer\", \"model\",\n",
        "    \"proton energy (mev)\", \"proton energy\"\n",
        "]\n",
        "HEADER_SET = set(HEADER_LABELS)\n",
        "\n",
        "# IMPORTANT: target the actual SharePoint \"list view\" table (reduces missing rows)\n",
        "TABLE_ROW_SELECTOR = \"table.ms-listviewtable tbody tr\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers for cleaning/parsing\n",
        "# -----------------------------\n",
        "def norm_text(s: str) -> str:\n",
        "    \"\"\"Normalize text for comparisons.\"\"\"\n",
        "    return \" \".join((s or \"\").split()).strip().lower()\n",
        "\n",
        "\n",
        "def row_fingerprint(cells):\n",
        "    \"\"\"\n",
        "    Make a stable hash of the cleaned row values.\n",
        "    Used to deduplicate rows across pages (SharePoint sometimes repeats).\n",
        "    \"\"\"\n",
        "    norm = [\" \".join((c or \"\").split()) for c in cells]\n",
        "    return hashlib.md5(\" | \".join(norm).encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "\n",
        "def strip_header_prefix(cell: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert 'City: Vienna' -> 'Vienna' (for known header labels).\n",
        "    Sometimes SharePoint prepends labels inside a cell.\n",
        "    \"\"\"\n",
        "    c = (cell or \"\").strip()\n",
        "    c_norm = norm_text(c)\n",
        "    for h in HEADER_LABELS:\n",
        "        if re.match(rf\"^{re.escape(h)}(\\s*[:\\-]?\\s+)\", c_norm):\n",
        "            return re.sub(rf\"(?i)^{re.escape(h)}\\s*[:\\-]?\\s+\", \"\", c).strip()\n",
        "    return c\n",
        "\n",
        "\n",
        "def flatten_multiline_cells(raw_cells):\n",
        "    \"\"\"\n",
        "    SharePoint quirks:\n",
        "    - multiple values in one cell separated by tabs\n",
        "    - multiple values separated by newlines\n",
        "    This function splits on tabs/newlines and flattens into a token list.\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    for c in raw_cells:\n",
        "        if not c:\n",
        "            continue\n",
        "        parts = re.split(r\"[\\t\\r\\n]+\", str(c))\n",
        "        for part in parts:\n",
        "            part = part.strip()\n",
        "            if part:\n",
        "                tokens.append(part)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def clean_and_align_tokens(raw_cells):\n",
        "    \"\"\"\n",
        "    Convert raw table cells -> exactly 6 cleaned fields.\n",
        "    Strategy:\n",
        "    - flatten tabs/newlines\n",
        "    - remove header-label tokens\n",
        "    - strip 'Label: value' prefixes\n",
        "    - if too many tokens remain, pick the best contiguous window of length 6\n",
        "    \"\"\"\n",
        "    tokens = flatten_multiline_cells(raw_cells)\n",
        "\n",
        "    processed = []\n",
        "    for t in tokens:\n",
        "        t_norm = norm_text(t)\n",
        "        if t_norm in HEADER_SET:\n",
        "            continue\n",
        "\n",
        "        t2 = strip_header_prefix(t)\n",
        "        t2_norm = norm_text(t2)\n",
        "        if not t2 or t2_norm in HEADER_SET:\n",
        "            continue\n",
        "\n",
        "        processed.append(t2.strip())\n",
        "\n",
        "    # Not enough data to form a row\n",
        "    if len(processed) < EXPECTED_COLS:\n",
        "        return None\n",
        "\n",
        "    # Exactly the correct number of fields\n",
        "    if len(processed) == EXPECTED_COLS:\n",
        "        # Drop if it still looks like a header row\n",
        "        if any(norm_text(x) in HEADER_SET for x in processed):\n",
        "            return None\n",
        "        return processed\n",
        "\n",
        "    # More than 6 tokens => choose the best \"slice\" of 6 tokens\n",
        "    def badness(x: str) -> int:\n",
        "        xn = norm_text(x)\n",
        "        if xn in HEADER_SET:\n",
        "            return 100\n",
        "        if xn.isdigit():  # sometimes SharePoint injects numeric IDs\n",
        "            return 10\n",
        "        if any(xn.startswith(h + \" \") for h in HEADER_LABELS):  # e.g. \"city zurich\"\n",
        "            return 10\n",
        "        return 0\n",
        "\n",
        "    best_window = None\n",
        "    best_score = None\n",
        "\n",
        "    for start in range(0, len(processed) - EXPECTED_COLS + 1):\n",
        "        window = processed[start:start + EXPECTED_COLS]\n",
        "        if any(norm_text(w) in HEADER_SET for w in window):\n",
        "            continue\n",
        "\n",
        "        score = sum(badness(w) for w in window)\n",
        "        if best_score is None or score < best_score:\n",
        "            best_score = score\n",
        "            best_window = window\n",
        "\n",
        "    return best_window\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Page navigation helpers\n",
        "# -----------------------------\n",
        "async def wait_for_table_refresh(page, prev_first_row_text, timeout_ms=20000):\n",
        "    \"\"\"\n",
        "    After clicking Next, wait until the first row content changes.\n",
        "    This is more reliable than a fixed sleep on SharePoint pages.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        await page.wait_for_function(\n",
        "            \"\"\"(prev) => {\n",
        "                const r = document.querySelector('table.ms-listviewtable tbody tr');\n",
        "                return r && r.innerText && r.innerText !== prev;\n",
        "            }\"\"\",\n",
        "            arg=prev_first_row_text,\n",
        "            timeout=timeout_ms\n",
        "        )\n",
        "    except:\n",
        "        # Fallback: small sleep if SharePoint is slow\n",
        "        await page.wait_for_timeout(1200)\n",
        "\n",
        "\n",
        "async def robust_click(el):\n",
        "    \"\"\"\n",
        "    SharePoint \"Next\" can be visible but outside viewport.\n",
        "    Try:\n",
        "    1) scroll into view\n",
        "    2) force-click\n",
        "    3) JS click as fallback\n",
        "    \"\"\"\n",
        "    try:\n",
        "        await el.scroll_into_view_if_needed()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        await el.click(force=True, timeout=20000)\n",
        "        return True\n",
        "    except:\n",
        "        try:\n",
        "            await el.evaluate(\"node => node.click()\")\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main scraper\n",
        "# -----------------------------\n",
        "async def scrape_all_pages():\n",
        "    all_rows = []\n",
        "    seen_rows = set()\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        # Launch headless browser\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "\n",
        "        # Create a browser context and block heavy resources for speed\n",
        "        context = await browser.new_context()\n",
        "        await context.route(\n",
        "            \"**/*\",\n",
        "            lambda route: route.abort()\n",
        "            if route.request.resource_type in (\"image\", \"font\", \"media\", \"stylesheet\")\n",
        "            else route.continue_()\n",
        "        )\n",
        "\n",
        "        page = await context.new_page()\n",
        "        page.set_default_timeout(20000)\n",
        "\n",
        "        # Open the first page (hash-based view)\n",
        "        url = BASE_URL + HASH_PREFIX\n",
        "        print(f\"Loading first page: {url}\")\n",
        "        await page.goto(url, timeout=60000, wait_until=\"domcontentloaded\")\n",
        "\n",
        "        page_index = 0\n",
        "\n",
        "        while True:\n",
        "            page_index += 1\n",
        "            print(f\"\\n--- Page {page_index} ---\")\n",
        "\n",
        "            # Ensure list view rows exist\n",
        "            try:\n",
        "                await page.wait_for_selector(TABLE_ROW_SELECTOR, timeout=20000)\n",
        "            except:\n",
        "                print(\"No list view table rows found → stopping\")\n",
        "                break\n",
        "\n",
        "            # Extract all rows from the list view in ONE browser call (fast)\n",
        "            raw_rows = await page.eval_on_selector_all(\n",
        "                TABLE_ROW_SELECTOR,\n",
        "                \"\"\"trs => trs.map(tr =>\n",
        "                    Array.from(tr.querySelectorAll('td')).map(td => td.innerText)\n",
        "                )\"\"\"\n",
        "            )\n",
        "\n",
        "            print(f\"Rows extracted from DOM this page: {len(raw_rows)}\")\n",
        "\n",
        "            new_rows_this_page = 0\n",
        "            kept_after_parsing = 0\n",
        "\n",
        "            # Parse each extracted row\n",
        "            for raw_cells in raw_rows:\n",
        "                cells = clean_and_align_tokens(raw_cells)\n",
        "                if cells is None or len(cells) != EXPECTED_COLS:\n",
        "                    continue\n",
        "\n",
        "                # Safety: never allow header labels as data\n",
        "                if any(norm_text(x) in HEADER_SET for x in cells):\n",
        "                    continue\n",
        "\n",
        "                kept_after_parsing += 1\n",
        "\n",
        "                # Deduplicate across pages\n",
        "                fp = row_fingerprint(cells)\n",
        "                if fp in seen_rows:\n",
        "                    continue\n",
        "\n",
        "                seen_rows.add(fp)\n",
        "                new_rows_this_page += 1\n",
        "\n",
        "                all_rows.append({\n",
        "                    \"Country\": cells[0],\n",
        "                    \"City\": cells[1],\n",
        "                    \"Facility\": cells[2],\n",
        "                    \"Manufacturer\": cells[3],\n",
        "                    \"Model\": cells[4],\n",
        "                    \"Proton energy (MeV)\": cells[5],\n",
        "                })\n",
        "\n",
        "            print(f\"Rows kept after parsing this page: {kept_after_parsing}\")\n",
        "            print(f\"New unique rows added this page: {new_rows_this_page}\")\n",
        "            print(f\"Total unique rows so far: {len(all_rows)}\")\n",
        "\n",
        "            # Capture first row text to detect refresh after clicking Next\n",
        "            try:\n",
        "                prev_first = await page.locator(TABLE_ROW_SELECTOR).first.inner_text()\n",
        "            except:\n",
        "                prev_first = \"\"\n",
        "\n",
        "            # Find \"Next\" and click it\n",
        "            next_el = page.locator(\n",
        "                'a[title=\"Next\"], a[aria-label=\"Next\"], a[title=\"Next page\"], a[aria-label=\"Next page\"], a:has-text(\"Next\")'\n",
        "            ).first\n",
        "\n",
        "            # Stop if no next page\n",
        "            if await next_el.count() == 0 or not await next_el.is_visible() or not await next_el.is_enabled():\n",
        "                print(\"No Next button → stopping\")\n",
        "                break\n",
        "\n",
        "            ok = await robust_click(next_el)\n",
        "            if not ok:\n",
        "                print(\"Could not click Next → stopping\")\n",
        "                break\n",
        "\n",
        "            # Wait for the table to refresh\n",
        "            await wait_for_table_refresh(page, prev_first)\n",
        "\n",
        "        await context.close()\n",
        "        await browser.close()\n",
        "\n",
        "    # Build final DataFrame, sort, and save\n",
        "    df = pd.DataFrame(all_rows)\n",
        "    df = df.sort_values([\"Country\", \"City\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "    df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "    print(\"\\nDONE\")\n",
        "    print(f\"Saved {len(df)} unique cyclotron rows to:\")\n",
        "    print(OUTPUT_CSV)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Run in Colab (top-level await is supported in Colab notebooks)\n",
        "# -----------------------------\n",
        "await scrape_all_pages()"
      ],
      "metadata": {
        "id": "Ypt70A4qdvOL",
        "outputId": "aa952093-99a5-46df-ffa1-06554dbe759d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading first page: https://nucleus.iaea.org/sites/accelerators/Pages/Cyclotron.aspx#InplviewHashd5afe566-18ad-4ac0-8aeb-ccf833dbc282=\n",
            "\n",
            "--- Page 1 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 30\n",
            "\n",
            "--- Page 2 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 29\n",
            "Total unique rows so far: 59\n",
            "\n",
            "--- Page 3 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 89\n",
            "\n",
            "--- Page 4 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 24\n",
            "Total unique rows so far: 113\n",
            "\n",
            "--- Page 5 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 140\n",
            "\n",
            "--- Page 6 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 168\n",
            "\n",
            "--- Page 7 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 193\n",
            "\n",
            "--- Page 8 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 221\n",
            "\n",
            "--- Page 9 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 249\n",
            "\n",
            "--- Page 10 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 29\n",
            "Total unique rows so far: 278\n",
            "\n",
            "--- Page 11 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 306\n",
            "\n",
            "--- Page 12 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 26\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 332\n",
            "\n",
            "--- Page 13 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 360\n",
            "\n",
            "--- Page 14 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 390\n",
            "\n",
            "--- Page 15 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 30\n",
            "Total unique rows so far: 420\n",
            "\n",
            "--- Page 16 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 24\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 442\n",
            "\n",
            "--- Page 17 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 26\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 464\n",
            "\n",
            "--- Page 18 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 491\n",
            "\n",
            "--- Page 19 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 518\n",
            "\n",
            "--- Page 20 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 540\n",
            "\n",
            "--- Page 21 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 562\n",
            "\n",
            "--- Page 22 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 587\n",
            "\n",
            "--- Page 23 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 615\n",
            "\n",
            "--- Page 24 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 643\n",
            "\n",
            "--- Page 25 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 19\n",
            "New unique rows added this page: 16\n",
            "Total unique rows so far: 659\n",
            "\n",
            "--- Page 26 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 18\n",
            "New unique rows added this page: 15\n",
            "Total unique rows so far: 674\n",
            "\n",
            "--- Page 27 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 27\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 701\n",
            "\n",
            "--- Page 28 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 23\n",
            "New unique rows added this page: 21\n",
            "Total unique rows so far: 722\n",
            "\n",
            "--- Page 29 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 748\n",
            "\n",
            "--- Page 30 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 774\n",
            "\n",
            "--- Page 31 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 27\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 800\n",
            "\n",
            "--- Page 32 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 26\n",
            "Total unique rows so far: 826\n",
            "\n",
            "--- Page 33 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 19\n",
            "Total unique rows so far: 845\n",
            "\n",
            "--- Page 34 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 870\n",
            "\n",
            "--- Page 35 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 22\n",
            "Total unique rows so far: 892\n",
            "\n",
            "--- Page 36 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 23\n",
            "Total unique rows so far: 915\n",
            "\n",
            "--- Page 37 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 940\n",
            "\n",
            "--- Page 38 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 29\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 965\n",
            "\n",
            "--- Page 39 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 30\n",
            "New unique rows added this page: 25\n",
            "Total unique rows so far: 990\n",
            "\n",
            "--- Page 40 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 27\n",
            "New unique rows added this page: 27\n",
            "Total unique rows so far: 1017\n",
            "\n",
            "--- Page 41 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 1045\n",
            "\n",
            "--- Page 42 ---\n",
            "Rows extracted from DOM this page: 30\n",
            "Rows kept after parsing this page: 28\n",
            "New unique rows added this page: 28\n",
            "Total unique rows so far: 1073\n",
            "\n",
            "--- Page 43 ---\n",
            "Rows extracted from DOM this page: 20\n",
            "Rows kept after parsing this page: 20\n",
            "New unique rows added this page: 20\n",
            "Total unique rows so far: 1093\n",
            "No Next button → stopping\n",
            "\n",
            "DONE\n",
            "Saved 1093 unique cyclotron rows to:\n",
            "/content/iaea_cyclotrons.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the first part of the multimodal\n",
        "\n",
        "Creation of the function: print_country_report"
      ],
      "metadata": {
        "id": "16Js_U3pdyC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# 1) Load your CSV\n",
        "# =========================\n",
        "CSV_PATH = \"/content/iaea_cyclotrons.csv\"   # <- adjust if different\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Normalize whitespace (helpful for grouping)\n",
        "for col in [\"Country\",\"City\",\"Facility\",\"Manufacturer\",\"Model\",\"Proton energy (MeV)\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "# =========================\n",
        "# 2) Helper: parse energy to numeric (best-effort)\n",
        "#    Handles: \"11\", \"16.5\", \"16-18\", \"16 / 18\", etc.\n",
        "# =========================\n",
        "def parse_energy_to_float(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in (\"nan\", \"none\"):\n",
        "        return None\n",
        "    # find numbers in the string\n",
        "    nums = re.findall(r\"\\d+(?:\\.\\d+)?\", s)\n",
        "    if not nums:\n",
        "        return None\n",
        "    # if range-like, take the max (or change to avg if you prefer)\n",
        "    vals = [float(n) for n in nums]\n",
        "    return max(vals)\n",
        "\n",
        "df[\"Energy_num\"] = df[\"Proton energy (MeV)\"].apply(parse_energy_to_float)\n",
        "\n",
        "# =========================\n",
        "# 3) Country summary function\n",
        "# =========================\n",
        "def country_summary(country, top_n=15):\n",
        "    \"\"\"\n",
        "    Return a structured summary for a country:\n",
        "    - total cyclotrons\n",
        "    - cities list + counts\n",
        "    - facilities list + counts\n",
        "    - manufacturer / model breakdown\n",
        "    - energy stats\n",
        "    \"\"\"\n",
        "    # case-insensitive match\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    if sub.empty:\n",
        "        # fuzzy suggestion: show close matches by containment\n",
        "        candidates = df[df[\"Country\"].str.lower().str.contains(str(country).strip().lower(), na=False)][\"Country\"].unique()\n",
        "        return {\n",
        "            \"country\": country,\n",
        "            \"found\": False,\n",
        "            \"message\": f\"No exact match for '{country}'.\",\n",
        "            \"did_you_mean\": sorted(candidates)[:20]\n",
        "        }\n",
        "\n",
        "    total = len(sub)\n",
        "\n",
        "    cities = (sub.groupby(\"City\")\n",
        "                .size()\n",
        "                .sort_values(ascending=False))\n",
        "\n",
        "    facilities = (sub.groupby(\"Facility\")\n",
        "                    .size()\n",
        "                    .sort_values(ascending=False))\n",
        "\n",
        "    manufacturers = (sub.groupby(\"Manufacturer\")\n",
        "                       .size()\n",
        "                       .sort_values(ascending=False))\n",
        "\n",
        "    models = (sub.groupby(\"Model\")\n",
        "                .size()\n",
        "                .sort_values(ascending=False))\n",
        "\n",
        "    # manufacturer-model combo\n",
        "    manu_model = (sub.groupby([\"Manufacturer\",\"Model\"])\n",
        "                    .size()\n",
        "                    .sort_values(ascending=False))\n",
        "\n",
        "    energy_stats = {\n",
        "        \"count_numeric\": int(sub[\"Energy_num\"].notna().sum()),\n",
        "        \"min\": float(sub[\"Energy_num\"].min()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "        \"median\": float(sub[\"Energy_num\"].median()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "        \"max\": float(sub[\"Energy_num\"].max()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"country\": country,\n",
        "        \"found\": True,\n",
        "        \"total_cyclotrons\": total,\n",
        "        \"cities_top\": cities.head(top_n),\n",
        "        \"facilities_top\": facilities.head(top_n),\n",
        "        \"manufacturers\": manufacturers,\n",
        "        \"models_top\": models.head(top_n),\n",
        "        \"manufacturer_model_top\": manu_model.head(top_n),\n",
        "        \"energy_stats\": energy_stats,\n",
        "        \"all_cities_count\": int(cities.shape[0]),\n",
        "        \"all_facilities_count\": int(facilities.shape[0]),\n",
        "    }\n",
        "\n",
        "# =========================\n",
        "# 4) Pretty-print function (human readable)\n",
        "# =========================\n",
        "def print_country_report(country, top_n=10):\n",
        "    out = country_summary(country, top_n=top_n)\n",
        "    if not out[\"found\"]:\n",
        "        print(out[\"message\"])\n",
        "        if out.get(\"did_you_mean\"):\n",
        "            print(\"Did you mean one of these?\")\n",
        "            for c in out[\"did_you_mean\"]:\n",
        "                print(\" -\", c)\n",
        "        return\n",
        "\n",
        "    print(f\"=== {out['country']} ===\")\n",
        "    print(f\"Total cyclotrons: {out['total_cyclotrons']}\")\n",
        "    print(f\"Cities covered: {out['all_cities_count']}\")\n",
        "    print(f\"Facilities covered: {out['all_facilities_count']}\")\n",
        "    print()\n",
        "\n",
        "    es = out[\"energy_stats\"]\n",
        "    print(\"Energy (MeV) stats (numeric rows only):\")\n",
        "    print(f\"  numeric entries: {es['count_numeric']}\")\n",
        "    print(f\"  min / median / max: {es['min']} / {es['median']} / {es['max']}\")\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} cities by count:\")\n",
        "    print(out[\"cities_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} facilities by count:\")\n",
        "    print(out[\"facilities_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(\"Manufacturer counts:\")\n",
        "    print(out[\"manufacturers\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} models:\")\n",
        "    print(out[\"models_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "    print(f\"Top {top_n} (Manufacturer, Model) pairs:\")\n",
        "    print(out[\"manufacturer_model_top\"].to_string())\n",
        "    print()\n",
        "\n",
        "# =========================\n",
        "# 5) Example usage\n",
        "# =========================\n",
        "print_country_report(\"Switzerland\", top_n=10)"
      ],
      "metadata": {
        "id": "CYd5RFlyGaoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf202ee-7487-4ee6-ab9c-354a385ca372"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Switzerland ===\n",
            "Total cyclotrons: 4\n",
            "Cities covered: 4\n",
            "Facilities covered: 4\n",
            "\n",
            "Energy (MeV) stats (numeric rows only):\n",
            "  numeric entries: 4\n",
            "  min / median / max: 16.0 / 17.0 / 18.0\n",
            "\n",
            "Top 10 cities by count:\n",
            "City\n",
            "Bern         1\n",
            "Genève       1\n",
            "Schlieren    1\n",
            "Zürich       1\n",
            "\n",
            "Top 10 facilities by count:\n",
            "Facility\n",
            "SWAN / Uni. Bern                                                             1\n",
            "Universitätsspital Zueurich, Labor Schlieren, Klinik für Onkologie (Wagi)    1\n",
            "Universitätsspital Zürich (USZ)                                              1\n",
            "unspecified                                                                  1\n",
            "\n",
            "Manufacturer counts:\n",
            "Manufacturer\n",
            "GE     2\n",
            "IBA    2\n",
            "\n",
            "Top 10 models:\n",
            "Model\n",
            "PETtrace            2\n",
            "CYCLONE 18          1\n",
            "CYCLONE 18/18 HC    1\n",
            "\n",
            "Top 10 (Manufacturer, Model) pairs:\n",
            "Manufacturer  Model           \n",
            "GE            PETtrace            2\n",
            "IBA           CYCLONE 18          1\n",
            "              CYCLONE 18/18 HC    1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora altro"
      ],
      "metadata": {
        "id": "q28_7bbDrUKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up OpenAI LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(temperature=0.9, model=\"gpt-4.1-mini\")\n"
      ],
      "metadata": {
        "id": "d50cJdyarBTD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the structured report into a prompt for the LLM"
      ],
      "metadata": {
        "id": "iX0CDcARsldc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def country_report_for_llm(country, top_n=10):\n",
        "    out = country_summary(country, top_n=top_n)\n",
        "\n",
        "    if not out[\"found\"]:\n",
        "        return f\"No data found for country: {country}\"\n",
        "\n",
        "    # Convert tables into readable text blocks\n",
        "    cities_text = out[\"cities_top\"].to_string()\n",
        "    facilities_text = out[\"facilities_top\"].to_string()\n",
        "    manufacturers_text = out[\"manufacturers\"].to_string()\n",
        "    models_text = out[\"models_top\"].to_string()\n",
        "\n",
        "    es = out[\"energy_stats\"]\n",
        "    energy_text = (\n",
        "        f\"Numeric entries: {es['count_numeric']}\\n\"\n",
        "        f\"Min energy: {es['min']}\\n\"\n",
        "        f\"Median energy: {es['median']}\\n\"\n",
        "        f\"Max energy: {es['max']}\"\n",
        "    )\n",
        "\n",
        "    # Final report string fed to the LLM\n",
        "    report = f\"\"\"\n",
        "IAEA Cyclotron Dataset Report for {out['country']}\n",
        "\n",
        "Total cyclotrons: {out['total_cyclotrons']}\n",
        "Cities covered: {out['all_cities_count']}\n",
        "Facilities covered: {out['all_facilities_count']}\n",
        "\n",
        "Top cities:\n",
        "{cities_text}\n",
        "\n",
        "Top facilities:\n",
        "{facilities_text}\n",
        "\n",
        "Manufacturers:\n",
        "{manufacturers_text}\n",
        "\n",
        "Top models:\n",
        "{models_text}\n",
        "\n",
        "Energy statistics:\n",
        "{energy_text}\n",
        "\"\"\"\n",
        "\n",
        "    return report.strip()"
      ],
      "metadata": {
        "id": "Sua-G7FSrkZz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell sends a structured country-level report to an LLM and asks it to generate a concise executive summary.\n",
        "The model is prompted to focus on market-relevant aspects such as:\n",
        "- Infrastructure scale\n",
        "- Geographic concentration\n",
        "- Major suppliers\n",
        "- Notable patterns in the data\n",
        "The output is rendered nicely using Markdown.\n"
      ],
      "metadata": {
        "id": "HgrQPVXKsq3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "country = \"Italy\"\n",
        "\n",
        "report_text = country_report_for_llm(country)\n",
        "\n",
        "response = llm.invoke(\n",
        "    f\"\"\"\n",
        "You are a market analyst in nuclear medicine.\n",
        "Write a concise executive summary (max 150 words) based on this dataset report.\n",
        "\n",
        "Focus on:\n",
        "- Overall infrastructure scale\n",
        "- Geographic concentration\n",
        "- Major suppliers\n",
        "- Any notable patterns\n",
        "\n",
        "Report:\n",
        "{report_text}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "WVJruj2Sr6x3",
        "outputId": "d6cd45c3-700c-4bab-db23-980414931a90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Italy’s cyclotron infrastructure comprises 40 units across 39 facilities in 30 cities, indicating broad but moderately concentrated nuclear medicine capacity. The highest cyclotron density is in Rome and Milan, each with 4 units, followed by Naples with 3, reflecting key urban hubs for radiopharmaceutical production. Major suppliers dominate the market, with GE leading (20 units), followed by IBA (13) and Siemens (4), suggesting a preference for well-established manufacturers. The PETtrace, CYCLONE 18, and MiniTrace models are the most prevalent, accounting for nearly three-quarters of installations, signaling a focus on proven, versatile systems. Energy output ranges from 10 to 19 MeV, with a median of 16 MeV, suitable for diverse isotope production. Notably, a small number of facilities operate multiple cyclotrons, such as Castelfranco Veneto Radiopharmacy with 2 units, underscoring targeted investment in select centers. This distribution supports a robust but regionally focused cyclotron network aligned with clinical and research demands."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we should make an analysis of all countries in the IAEA database and create a document with all executive summaries."
      ],
      "metadata": {
        "id": "YDJ5UilKt9se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "Oa_N9nWg4xoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install reportlab geopy geopandas shapely pyproj fiona\n"
      ],
      "metadata": {
        "id": "QB_LeP0YswpW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the country name for consistency"
      ],
      "metadata": {
        "id": "pXVjaXVyoqnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install country_converter\n",
        "\n",
        "import os\n",
        "import json\n",
        "import country_converter as coco\n",
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "IN_CSV  = \"/content/iaea_cyclotrons.csv\"\n",
        "OUT_CSV = \"/content/iaea_cyclotrons_clean.csv\"\n",
        "CACHE   = \"/content/country_fix_cache.json\"\n",
        "\n",
        "df = pd.read_csv(IN_CSV)\n",
        "\n",
        "# Load or create cache for LLM fixes\n",
        "if os.path.exists(CACHE):\n",
        "    with open(CACHE, \"r\", encoding=\"utf-8\") as f:\n",
        "        fix_cache = json.load(f)\n",
        "else:\n",
        "    fix_cache = {}\n",
        "\n",
        "# ✅ Allowed SHORT names\n",
        "allowed = set(coco.CountryConverter().data[\"name_short\"].dropna().astype(str))\n",
        "\n",
        "def llm_fix_country(raw):\n",
        "    raw = str(raw).strip()\n",
        "    if raw in fix_cache:\n",
        "        return fix_cache[raw]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Map this value to the correct country name (common short form).\n",
        "Return only the country name, nothing else.\n",
        "\n",
        "Examples of desired style: Italy, Belarus, North Macedonia, Czechia, Russia.\n",
        "\n",
        "Input: {raw}\n",
        "\"\"\".strip()\n",
        "\n",
        "    resp = llm.invoke(prompt)\n",
        "    candidate = (resp.content or \"\").strip().strip('\"').strip(\"'\")\n",
        "\n",
        "    if candidate in allowed:\n",
        "        fixed = candidate\n",
        "    else:\n",
        "        fixed = coco.convert(names=candidate, to=\"name_short\", not_found=raw)\n",
        "\n",
        "    fix_cache[raw] = fixed\n",
        "    with open(CACHE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fix_cache, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return fixed\n",
        "\n",
        "# -----------------------\n",
        "# Step A: deterministic conversion first (SHORT names)\n",
        "# -----------------------\n",
        "unique = df[\"Country\"].dropna().unique()\n",
        "short_names = coco.convert(names=list(unique), to=\"name_short\", not_found=None)\n",
        "mapping = dict(zip(unique, short_names))\n",
        "df[\"Country_clean\"] = df[\"Country\"].map(mapping)\n",
        "\n",
        "# -----------------------\n",
        "# Step B: LLM fallback only for missing\n",
        "# -----------------------\n",
        "missing = df.loc[df[\"Country_clean\"].isna(), \"Country\"].unique()\n",
        "print(\"LLM resolving:\", missing)\n",
        "\n",
        "for val in missing:\n",
        "    fixed = llm_fix_country(val)\n",
        "    df.loc[df[\"Country\"] == val, \"Country_clean\"] = fixed\n",
        "\n",
        "# -----------------------\n",
        "# Step C: overwrite original column\n",
        "# -----------------------\n",
        "df[\"Country\"] = df[\"Country_clean\"]\n",
        "df = df.drop(columns=[\"Country_clean\"])\n",
        "\n",
        "# -----------------------\n",
        "# Step C2: ADD ISO3 (use this for maps!)\n",
        "# -----------------------\n",
        "df[\"Country_iso3\"] = coco.convert(\n",
        "    names=df[\"Country\"].fillna(\"\"),\n",
        "    to=\"ISO3\",\n",
        "    not_found=None\n",
        ")\n",
        "\n",
        "# Quick debug for your two cases\n",
        "print(\"\\nCheck Bahrain / Belarus:\")\n",
        "print(df[df[\"Country\"].isin([\"Bahrain\", \"Belarus\"])][[\"Country\", \"Country_iso3\"]].drop_duplicates())\n",
        "\n",
        "# Show any countries still failing ISO3 conversion (these will fail mapping too)\n",
        "bad = df.loc[df[\"Country\"].notna() & df[\"Country_iso3\"].isna(), \"Country\"].unique()\n",
        "print(\"\\nCountries with missing ISO3:\", bad)\n",
        "\n",
        "# -----------------------\n",
        "# Step D: save cleaned file\n",
        "# -----------------------\n",
        "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "print(f\"\\n✅ Cleaned file saved to: {OUT_CSV}\")"
      ],
      "metadata": {
        "id": "1il6h6IVouev",
        "outputId": "1058c063-e9ab-4f15-fbd2-01664e5aeb20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:country_converter.country_converter:Dubai not found in regex\n",
            "WARNING:country_converter.country_converter:Northern Ireland not found in regex\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM resolving: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:country_converter.country_converter:Dubai not found in regex\n",
            "WARNING:country_converter.country_converter:Northern Ireland not found in regex\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Check Bahrain / Belarus:\n",
            "    Country Country_iso3\n",
            "36  Bahrain          BHR\n",
            "39  Belarus          BLR\n",
            "\n",
            "Countries with missing ISO3: []\n",
            "\n",
            "✅ Cleaned file saved to: /content/iaea_cyclotrons_clean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted(df[\"Country\"].unique()))"
      ],
      "metadata": {
        "id": "x1MnIoPBsCer",
        "outputId": "c1cbb398-bcc1-4838-df26-41ad59c0ddb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Algeria', 'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahrain', 'Bangladesh', 'Belarus', 'Belgium', 'Bolivia', 'Brazil', 'Brunei Darussalam', 'Bulgaria', 'Canada', 'Chile', 'China', 'Colombia', 'Costa Rica', 'Croatia', 'Cuba', 'Cyprus', 'Czechia', 'Denmark', 'Dominican Republic', 'Dubai', 'Ecuador', 'Egypt', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Kuwait', 'Latvia', 'Lebanon', 'Libya', 'Lithuania', 'Malaysia', 'Mexico', 'Morocco', 'Myanmar', 'Netherlands', 'New Zealand', 'Nigeria', 'North Macedonia', 'Northern Ireland', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Romania', 'Russia', 'Saudi Arabia', 'Singapore', 'Slovakia', 'South Africa', 'South Korea', 'Spain', 'Sweden', 'Switzerland', 'Syria', 'Taiwan', 'Thailand', 'Tunisia', 'Türkiye', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Venezuela', 'Vietnam']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manufacturer cleaner"
      ],
      "metadata": {
        "id": "IQIjWp1PAcEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Manufacturer cleaning (LLM-heavy) with:\n",
        "# - Canon set (persisted)\n",
        "# - LLM cache (persisted)\n",
        "# - Manual truth rules (Siemens/CTI, PMB->Avelion, ABT->BCS, ACSI/ASCI->Advanced Cyclotron Systems)\n",
        "# - Guardrails to prevent \"everything maps to one canonical\"\n",
        "# - Single entry point: canonicalize_manufacturers(..., overwrite=True)\n",
        "# ============================================================\n",
        "\n",
        "import os, json, re\n",
        "import pandas as pd\n",
        "\n",
        "# Optional but recommended for guardrails similarity checks\n",
        "!pip -q install rapidfuzz\n",
        "from rapidfuzz import fuzz\n",
        "\n",
        "# -----------------------\n",
        "# Paths\n",
        "# -----------------------\n",
        "CANON_PATH = \"/content/manufacturer_canon_set.json\"\n",
        "CACHE_PATH = \"/content/manufacturer_llm_cache.json\"\n",
        "\n",
        "# -----------------------\n",
        "# Seed canon (your known truths)\n",
        "# -----------------------\n",
        "SEED_CANON = {\n",
        "    \"Siemens Healthineers\",        # acquired CTI\n",
        "    \"Avelion\",                     # PMB -> Avelion (Alcen)\n",
        "    \"Best Cyclotron Systems\",      # ABT -> BCS\n",
        "    \"Advanced Cyclotron Systems\",  # ACSI/ASCI\n",
        "    \"Rosatom\",                     # Rosatom\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# Load/init canon + cache\n",
        "# -----------------------\n",
        "if os.path.exists(CANON_PATH):\n",
        "    with open(CANON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        canon_set = set(json.load(f))\n",
        "else:\n",
        "    canon_set = set(SEED_CANON)\n",
        "\n",
        "if os.path.exists(CACHE_PATH):\n",
        "    with open(CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        llm_cache = json.load(f)\n",
        "else:\n",
        "    llm_cache = {}\n",
        "\n",
        "def save_canon():\n",
        "    with open(CANON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(sorted(canon_set), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def save_cache():\n",
        "    with open(CACHE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(llm_cache, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# -----------------------\n",
        "# Basic normalization\n",
        "# -----------------------\n",
        "def basic_cleanup(s: str) -> str:\n",
        "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
        "        return \"\"\n",
        "    s = str(s).strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    s = re.sub(r\"[,\\.;:\\-]+$\", \"\", s).strip()\n",
        "    return s\n",
        "\n",
        "def norm_key(s: str) -> str:\n",
        "    s = basic_cleanup(s).lower()\n",
        "    s = re.sub(r\"\\(([^)]{1,20})\\)\", \" \", s)   # remove short bracket chunks\n",
        "    s = re.sub(r\"[^\\w\\s]\", \" \", s)            # remove punctuation\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# -----------------------\n",
        "# Manual truth rules (highest priority)\n",
        "# -----------------------\n",
        "def manual_map(raw: str) -> str | None:\n",
        "    k = norm_key(raw)\n",
        "    if not k:\n",
        "        return None\n",
        "\n",
        "    # Rossatom -> Rosatom (typo)\n",
        "    if k in {\"rossatom\", \"ross atom\", \"rosatom\"} or \"rossatom\" in k:\n",
        "        return \"Rosatom\"\n",
        "\n",
        "    # ACSI/ASCI (typo) -> Advanced Cyclotron Systems\n",
        "    if (\n",
        "        k in {\"acsi\", \"asci\"}\n",
        "        or k.startswith(\"acsi \")\n",
        "        or k.startswith(\"asci \")\n",
        "        or \"advanced cyclotron systems\" in k\n",
        "    ):\n",
        "        return \"Advanced Cyclotron Systems\"\n",
        "\n",
        "    # Siemens / CTI\n",
        "    if \"siemens\" in k or k == \"cti\" or k.startswith(\"cti \"):\n",
        "        return \"Siemens Healthineers\"\n",
        "\n",
        "    # PMB -> Avelion\n",
        "    if \"pmb\" in k:\n",
        "        return \"Avelion\"\n",
        "\n",
        "    # ABT -> Best Cyclotron Systems\n",
        "    if (\n",
        "        k == \"abt\"\n",
        "        or k.startswith(\"abt \")\n",
        "        or \"advanced beam technologies\" in k\n",
        "        or \"bcs\" in k\n",
        "        or \"best cyclotron systems\" in k\n",
        "    ):\n",
        "        return \"Best Cyclotron Systems\"\n",
        "\n",
        "    return None\n",
        "\n",
        "def looks_like_acsi(raw: str) -> bool:\n",
        "    k = norm_key(raw)\n",
        "    return (\n",
        "        k in {\"acsi\", \"asci\"}\n",
        "        or k.startswith(\"acsi \")\n",
        "        or k.startswith(\"asci \")\n",
        "        or \"advanced cyclotron systems\" in k\n",
        "    )\n",
        "\n",
        "# -----------------------\n",
        "# LLM chooser with guardrails\n",
        "# - LLM must return exact canon OR NEW:<name>\n",
        "# - If LLM chooses a canon, we validate similarity; otherwise force NEW\n",
        "# - Special-case: only allow mapping to \"Advanced Cyclotron Systems\" if it truly looks like ACSI/ASCI\n",
        "# -----------------------\n",
        "def llm_choose_canonical_or_new(raw: str, canon_list: list[str]) -> str:\n",
        "    raw0 = basic_cleanup(raw)\n",
        "    if not raw0:\n",
        "        return raw0\n",
        "\n",
        "    if raw0 in llm_cache:\n",
        "        return llm_cache[raw0]\n",
        "\n",
        "    canon_preview = canon_list if len(canon_list) <= 400 else canon_list[:400]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You standardize manufacturer/company names.\n",
        "\n",
        "Return EXACTLY one of:\n",
        "1) An EXACT string from the canonical list below (character-for-character), OR\n",
        "2) NEW:<canonical name> if it is NOT clearly the same company.\n",
        "\n",
        "CRITICAL RULE:\n",
        "- Do NOT guess. If you are not highly confident it's the same company, return NEW:<...>.\n",
        "- Only choose an existing canonical when the input is the same company name, an obvious acronym, or a trivial typo.\n",
        "\n",
        "Rules:\n",
        "- Remove legal suffixes (Inc., GmbH, AG, SA, Co., Ltd., Ltd, LLC, BV, SRL, SpA, etc.)\n",
        "- Resolve acronyms when appropriate\n",
        "- Prefer the best-known short public brand name\n",
        "- Output ONLY the chosen canonical OR NEW:<...>\n",
        "\n",
        "Canonical list:\n",
        "{chr(10).join(\"- \" + c for c in canon_preview)}\n",
        "\n",
        "Input: {raw0}\n",
        "\"\"\".strip()\n",
        "\n",
        "    resp = llm.invoke(prompt)\n",
        "    ans = (resp.content or \"\").strip().strip('\"').strip(\"'\")\n",
        "\n",
        "    # Parse NEW:\n",
        "    if ans.startswith(\"NEW:\"):\n",
        "        canon = basic_cleanup(ans[4:].strip())\n",
        "        llm_cache[raw0] = canon\n",
        "        save_cache()\n",
        "        return canon\n",
        "\n",
        "    chosen = basic_cleanup(ans)\n",
        "\n",
        "    # If LLM chose a canonical, validate.\n",
        "    if chosen in canon_set:\n",
        "        # Special case: don't let everything collapse to ACS\n",
        "        if chosen == \"Advanced Cyclotron Systems\":\n",
        "            if looks_like_acsi(raw0):\n",
        "                llm_cache[raw0] = chosen\n",
        "                save_cache()\n",
        "                return chosen\n",
        "            else:\n",
        "                # reject this mapping; treat as NEW using cleaned raw\n",
        "                new_name = basic_cleanup(raw0)\n",
        "                llm_cache[raw0] = new_name\n",
        "                save_cache()\n",
        "                return new_name\n",
        "\n",
        "        # Generic validation for other canonicals\n",
        "        score = fuzz.token_sort_ratio(norm_key(raw0), norm_key(chosen))\n",
        "        if score >= 90:\n",
        "            llm_cache[raw0] = chosen\n",
        "            save_cache()\n",
        "            return chosen\n",
        "        else:\n",
        "            # too dissimilar -> force NEW\n",
        "            new_name = basic_cleanup(raw0)\n",
        "            llm_cache[raw0] = new_name\n",
        "            save_cache()\n",
        "            return new_name\n",
        "\n",
        "    # If it returned something not in canon without NEW:, accept it as a new canonical candidate\n",
        "    llm_cache[raw0] = chosen\n",
        "    save_cache()\n",
        "    return chosen\n",
        "\n",
        "# -----------------------\n",
        "# Main entry point\n",
        "# -----------------------\n",
        "def canonicalize_manufacturers(\n",
        "    df: pd.DataFrame,\n",
        "    col=\"Manufacturer\",\n",
        "    out_col=\"Manufacturer_clean\",\n",
        "    overwrite=False,\n",
        "    grow_canon=True,\n",
        "    keep_backup=True\n",
        "):\n",
        "    \"\"\"\n",
        "    overwrite=False -> creates out_col\n",
        "    overwrite=True  -> overwrites col and drops out_col\n",
        "    keep_backup=True -> saves original to f\"{col}_raw\" before overwrite\n",
        "    \"\"\"\n",
        "    if keep_backup and overwrite:\n",
        "        raw_col = f\"{col}_raw\"\n",
        "        if raw_col not in df.columns:\n",
        "            df[raw_col] = df[col]\n",
        "\n",
        "    uniq = sorted(set(basic_cleanup(x) for x in df[col].dropna().astype(str).unique()))\n",
        "    uniq = [u for u in uniq if u]\n",
        "\n",
        "    mapping = {}\n",
        "\n",
        "    # Ensure seeds are in canon_set\n",
        "    canon_set.update(SEED_CANON)\n",
        "    save_canon()\n",
        "\n",
        "    print(f\"Unique manufacturers: {len(uniq)}\")\n",
        "    print(f\"Canon set size (start): {len(canon_set)}\")\n",
        "\n",
        "    for i, raw in enumerate(uniq, start=1):\n",
        "        # Manual truth layer first\n",
        "        m = manual_map(raw)\n",
        "        if m:\n",
        "            mapping[raw] = m\n",
        "            canon_set.add(m)\n",
        "            continue\n",
        "\n",
        "        canon_list = sorted(canon_set)\n",
        "        chosen = llm_choose_canonical_or_new(raw, canon_list)\n",
        "\n",
        "        mapping[raw] = chosen\n",
        "        if grow_canon and chosen:\n",
        "            canon_set.add(chosen)\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"  resolved {i}/{len(uniq)} (canon now {len(canon_set)})\")\n",
        "\n",
        "    save_canon()\n",
        "\n",
        "    df[out_col] = df[col].map(\n",
        "        lambda x: mapping.get(basic_cleanup(x), basic_cleanup(x)) if pd.notna(x) else None\n",
        "    )\n",
        "\n",
        "    if overwrite:\n",
        "        df[col] = df[out_col]\n",
        "        df.drop(columns=[out_col], inplace=True)\n",
        "\n",
        "    print(f\"Canon set size (end): {len(canon_set)}\")\n",
        "    return df, mapping\n",
        "\n",
        "# -----------------------\n",
        "# Optional: clear specific bad cached entries (run if needed)\n",
        "# -----------------------\n",
        "def clear_manufacturer_cache_keys(keys):\n",
        "    removed = 0\n",
        "    for k in keys:\n",
        "        if k in llm_cache:\n",
        "            del llm_cache[k]\n",
        "            removed += 1\n",
        "    save_cache()\n",
        "    print(f\"Cleared {removed} cached entries.\")\n",
        "\n",
        "# Example use if ACSI/ASCI were cached wrongly earlier:\n",
        "# clear_manufacturer_cache_keys([\"ACSI\",\"ASCI\",\"A.C.S.I.\",\"acsi\",\"asci\"])\n",
        "\n",
        "# -----------------------\n",
        "# USAGE:\n",
        "# df, mapping = canonicalize_manufacturers(df, overwrite=True)\n",
        "# Then use df[\"Manufacturer\"] (cleaned). Backup in df[\"Manufacturer_raw\"] if enabled.\n",
        "# -----------------------"
      ],
      "metadata": {
        "id": "P2uxzs6NAfkp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage and test"
      ],
      "metadata": {
        "id": "S3qyeSTSFkVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df, mapping = canonicalize_manufacturers(df, overwrite=True, keep_backup=True)\n",
        "print(df[\"Manufacturer\"].value_counts().head(30))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41n8qk_xFmiS",
        "outputId": "ec94c3f9-c5b3-4419-f6c4-d4866668dd6f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique manufacturers: 29\n",
            "Canon set size (start): 20\n",
            "Canon set size (end): 20\n",
            "Manufacturer\n",
            "GE                                       396\n",
            "IBA                                      237\n",
            "Siemens Healthineers                     190\n",
            "Sumitomo                                 161\n",
            "Advanced Cyclotron Systems                46\n",
            "Best Cyclotron Systems                    21\n",
            "Longevous Beamtech                        14\n",
            "TCC                                        9\n",
            "Niiefa                                     6\n",
            "Avelion                                    4\n",
            "Scanditronix                               4\n",
            "TRIUMF                                     1\n",
            "Rosatom                                    1\n",
            "Lawrence Berkeley National Laboratory      1\n",
            "W M Brobeck                                1\n",
            "Ionetix                                    1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data + basic cleaning"
      ],
      "metadata": {
        "id": "YnqKziIt41bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, hashlib, math\n",
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"/content/iaea_cyclotrons_clean.csv\"  # <-- adjust if needed\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Clean whitespace\n",
        "for col in [\"Country\",\"City\",\"Facility\",\"Manufacturer\",\"Model\",\"Proton energy (MeV)\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "# Standardize \"unspecified\" to NaN for analysis\n",
        "def to_nan_if_unspecified(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in (\"nan\",\"none\",\"unspecified\",\"n/a\",\"na\"):\n",
        "        return None\n",
        "    return s\n",
        "\n",
        "for col in [\"Facility\",\"Manufacturer\",\"Model\",\"City\",\"Country\",\"Proton energy (MeV)\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].apply(to_nan_if_unspecified)\n",
        "\n",
        "# Parse energy numeric (best-effort)\n",
        "def parse_energy_to_float(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    s = str(x).strip()\n",
        "    if s == \"\":\n",
        "        return None\n",
        "    nums = re.findall(r\"\\d+(?:\\.\\d+)?\", s)\n",
        "    if not nums:\n",
        "        return None\n",
        "    vals = [float(n) for n in nums]\n",
        "    return max(vals)  # choose max if ranges exist\n",
        "\n",
        "df[\"Energy_num\"] = df[\"Proton energy (MeV)\"].apply(parse_energy_to_float)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SYVR40I54zY6",
        "outputId": "dcd764b1-5eab-49d7-fdba-08953a153625"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Country          City                                           Facility  \\\n",
              "0    Algeria         Alger                          Clinique Fatema Al Azhare   \n",
              "1    Algeria   Constantine                                               None   \n",
              "2    Algeria    Tizi Ouzou                           Hôpital Chahids Mahmoudi   \n",
              "3  Argentina     Bariloche  Centro Nacional de Medicina Nuclear y Radioter...   \n",
              "4  Argentina  Buenos Aires                                              Fleni   \n",
              "\n",
              "  Manufacturer          Model Proton energy (MeV) Country_iso3  Energy_num  \n",
              "0          IBA  CYCLONE KIUBE                  18          DZA        18.0  \n",
              "1          IBA  CYCLONE KIUBE                  18          DZA        18.0  \n",
              "2           GE       PETtrace                16.5          DZA        16.5  \n",
              "3          IBA     CYCLONE 18                  18          ARG        18.0  \n",
              "4           GE       PETtrace                  16          ARG        16.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9aabf80-c194-4ec7-abf0-e7429d287565\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>Facility</th>\n",
              "      <th>Manufacturer</th>\n",
              "      <th>Model</th>\n",
              "      <th>Proton energy (MeV)</th>\n",
              "      <th>Country_iso3</th>\n",
              "      <th>Energy_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Algeria</td>\n",
              "      <td>Alger</td>\n",
              "      <td>Clinique Fatema Al Azhare</td>\n",
              "      <td>IBA</td>\n",
              "      <td>CYCLONE KIUBE</td>\n",
              "      <td>18</td>\n",
              "      <td>DZA</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Algeria</td>\n",
              "      <td>Constantine</td>\n",
              "      <td>None</td>\n",
              "      <td>IBA</td>\n",
              "      <td>CYCLONE KIUBE</td>\n",
              "      <td>18</td>\n",
              "      <td>DZA</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Algeria</td>\n",
              "      <td>Tizi Ouzou</td>\n",
              "      <td>Hôpital Chahids Mahmoudi</td>\n",
              "      <td>GE</td>\n",
              "      <td>PETtrace</td>\n",
              "      <td>16.5</td>\n",
              "      <td>DZA</td>\n",
              "      <td>16.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Argentina</td>\n",
              "      <td>Bariloche</td>\n",
              "      <td>Centro Nacional de Medicina Nuclear y Radioter...</td>\n",
              "      <td>IBA</td>\n",
              "      <td>CYCLONE 18</td>\n",
              "      <td>18</td>\n",
              "      <td>ARG</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Argentina</td>\n",
              "      <td>Buenos Aires</td>\n",
              "      <td>Fleni</td>\n",
              "      <td>GE</td>\n",
              "      <td>PETtrace</td>\n",
              "      <td>16</td>\n",
              "      <td>ARG</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9aabf80-c194-4ec7-abf0-e7429d287565')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f9aabf80-c194-4ec7-abf0-e7429d287565 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f9aabf80-c194-4ec7-abf0-e7429d287565');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1093,\n  \"fields\": [\n    {\n      \"column\": \"Country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 93,\n        \"samples\": [\n          \"Italy\",\n          \"Czechia\",\n          \"Myanmar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"City\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 699,\n        \"samples\": [\n          \"Zhengzhou\",\n          \"Je-ju\",\n          \"Tokyo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Facility\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 623,\n        \"samples\": [\n          \"Landspitali\",\n          \"University of Southern California Molecular bldg\",\n          \"CICLOTRON COLOMBIA S.A.S.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Manufacturer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"Ionetix\",\n          \"TCC The Cyclotron Corporation\",\n          \"Sichuan Longevous Beamtech(LBT).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 122,\n        \"samples\": [\n          \"Cyclone 18/18 Twin\",\n          \"LB-11MTS\",\n          \"LBT-11\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Proton energy (MeV)\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"3-20\",\n          \"19\",\n          \"30\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Country_iso3\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 93,\n        \"samples\": [\n          \"ITA\",\n          \"CZE\",\n          \"MMR\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Energy_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.082378358480085,\n        \"min\": 3.0,\n        \"max\": 700.0,\n        \"num_unique_values\": 33,\n        \"samples\": [\n          12.5,\n          20.0,\n          80.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Country report text generator for LLM"
      ],
      "metadata": {
        "id": "gSwW9HGt4-TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def country_summary(country, top_n=15):\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    if sub.empty:\n",
        "        candidates = df[df[\"Country\"].str.lower().str.contains(str(country).strip().lower(), na=False)][\"Country\"].dropna().unique()\n",
        "        return {\"country\": country, \"found\": False, \"did_you_mean\": sorted(candidates)[:20]}\n",
        "\n",
        "    total = len(sub)\n",
        "    cities = sub.groupby(\"City\").size().sort_values(ascending=False)\n",
        "    facilities = sub.groupby(\"Facility\").size().sort_values(ascending=False)\n",
        "    manufacturers = sub.groupby(\"Manufacturer\").size().sort_values(ascending=False)\n",
        "    models = sub.groupby(\"Model\").size().sort_values(ascending=False)\n",
        "    manu_model = sub.groupby([\"Manufacturer\",\"Model\"]).size().sort_values(ascending=False)\n",
        "\n",
        "    energy_stats = {\n",
        "        \"count_numeric\": int(sub[\"Energy_num\"].notna().sum()),\n",
        "        \"min\": float(sub[\"Energy_num\"].min()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "        \"median\": float(sub[\"Energy_num\"].median()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "        \"max\": float(sub[\"Energy_num\"].max()) if sub[\"Energy_num\"].notna().any() else None,\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"country\": country,\n",
        "        \"found\": True,\n",
        "        \"total_cyclotrons\": total,\n",
        "        \"cities_top\": cities.head(top_n),\n",
        "        \"facilities_top\": facilities.head(top_n),\n",
        "        \"manufacturers\": manufacturers,\n",
        "        \"models_top\": models.head(top_n),\n",
        "        \"manufacturer_model_top\": manu_model.head(top_n),\n",
        "        \"energy_stats\": energy_stats,\n",
        "        \"all_cities_count\": int(cities.shape[0]),\n",
        "        \"all_facilities_count\": int(facilities.shape[0]),\n",
        "    }\n",
        "\n",
        "def country_report_for_llm(country, top_n=10):\n",
        "    out = country_summary(country, top_n=top_n)\n",
        "    if not out[\"found\"]:\n",
        "        return f\"No exact match for '{country}'. Suggestions: {out.get('did_you_mean', [])}\"\n",
        "\n",
        "    cities_text = out[\"cities_top\"].to_string()\n",
        "    facilities_text = out[\"facilities_top\"].to_string()\n",
        "    manufacturers_text = out[\"manufacturers\"].to_string()\n",
        "    models_text = out[\"models_top\"].to_string()\n",
        "\n",
        "    es = out[\"energy_stats\"]\n",
        "    energy_text = (\n",
        "        f\"Numeric entries: {es['count_numeric']}\\n\"\n",
        "        f\"Min energy: {es['min']}\\n\"\n",
        "        f\"Median energy: {es['median']}\\n\"\n",
        "        f\"Max energy: {es['max']}\"\n",
        "    )\n",
        "\n",
        "    report = f\"\"\"\n",
        "IAEA Cyclotron Dataset Report for {out['country']}\n",
        "\n",
        "Total cyclotrons: {out['total_cyclotrons']}\n",
        "Cities covered: {out['all_cities_count']}\n",
        "Facilities covered: {out['all_facilities_count']}\n",
        "\n",
        "Top cities:\n",
        "{cities_text}\n",
        "\n",
        "Top facilities:\n",
        "{facilities_text}\n",
        "\n",
        "Manufacturers:\n",
        "{manufacturers_text}\n",
        "\n",
        "Top models:\n",
        "{models_text}\n",
        "\n",
        "Energy statistics:\n",
        "{energy_text}\n",
        "\"\"\"\n",
        "    return report.strip()\n",
        "\n",
        "def llm_exec_summary(country, top_n=10):\n",
        "    report_text = country_report_for_llm(country, top_n=top_n)\n",
        "    prompt = f\"\"\"\n",
        "You are a market analyst in nuclear medicine.\n",
        "Write a concise executive summary (max 150 words) based on this dataset report.\n",
        "\n",
        "Focus on:\n",
        "- Overall infrastructure scale\n",
        "- Geographic concentration\n",
        "- Major suppliers\n",
        "- Any notable patterns\n",
        "\n",
        "Report:\n",
        "{report_text}\n",
        "\"\"\".strip()\n",
        "    return llm.invoke(prompt).content.strip()"
      ],
      "metadata": {
        "id": "XwA0zk8744-B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geocoding + caching (for map dots)"
      ],
      "metadata": {
        "id": "0TJqwDJq5FRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "from geopy.extra.rate_limiter import RateLimiter\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "GEO_CACHE = \"/content/geocode_cache.csv\"\n",
        "\n",
        "# Load existing cache if available\n",
        "if os.path.exists(GEO_CACHE):\n",
        "    geo_cache = pd.read_csv(GEO_CACHE)\n",
        "else:\n",
        "    geo_cache = pd.DataFrame(columns=[\"Country_iso3\", \"Country\", \"City\", \"lat\", \"lon\", \"display_name\"])\n",
        "\n",
        "# Ensure string columns exist and are strings\n",
        "for col in [\"Country_iso3\", \"Country\", \"City\"]:\n",
        "    if col not in geo_cache.columns:\n",
        "        geo_cache[col] = \"\"\n",
        "    geo_cache[col] = geo_cache[col].astype(str)\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"iaea_cyclotron_colab_geocoder\")\n",
        "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1.0, swallow_exceptions=True)\n",
        "\n",
        "def _norm(x):\n",
        "    return str(x).strip().lower()\n",
        "\n",
        "def get_latlon(country, city, country_iso3=None):\n",
        "    global geo_cache  # must come first\n",
        "\n",
        "    country = str(country).strip()\n",
        "    city = str(city).strip()\n",
        "    country_iso3 = None if country_iso3 is None else str(country_iso3).strip()\n",
        "\n",
        "    # -----------------------\n",
        "    # Cache lookup (prefer ISO3 match if available)\n",
        "    # -----------------------\n",
        "    if country_iso3:\n",
        "        hit = geo_cache[\n",
        "            (geo_cache[\"Country_iso3\"].apply(_norm) == _norm(country_iso3)) &\n",
        "            (geo_cache[\"City\"].apply(_norm) == _norm(city))\n",
        "        ]\n",
        "        if not hit.empty and pd.notna(hit.iloc[0][\"lat\"]) and pd.notna(hit.iloc[0][\"lon\"]):\n",
        "            return float(hit.iloc[0][\"lat\"]), float(hit.iloc[0][\"lon\"])\n",
        "\n",
        "    # fallback cache lookup by country name\n",
        "    hit = geo_cache[\n",
        "        (geo_cache[\"Country\"].apply(_norm) == _norm(country)) &\n",
        "        (geo_cache[\"City\"].apply(_norm) == _norm(city))\n",
        "    ]\n",
        "    if not hit.empty and pd.notna(hit.iloc[0][\"lat\"]) and pd.notna(hit.iloc[0][\"lon\"]):\n",
        "        return float(hit.iloc[0][\"lat\"]), float(hit.iloc[0][\"lon\"])\n",
        "\n",
        "    # -----------------------\n",
        "    # Best-effort geocode with fallback query strategies\n",
        "    # -----------------------\n",
        "    queries = []\n",
        "    queries.append(f\"{city}, {country}\")                 # normal\n",
        "    if country_iso3:\n",
        "        queries.append(f\"{city}, {country_iso3}\")        # sometimes helps\n",
        "    queries.append(city)                                  # last resort\n",
        "\n",
        "    loc = None\n",
        "    for q in queries:\n",
        "        loc = geocode(q)\n",
        "        if loc is not None:\n",
        "            break\n",
        "\n",
        "    if loc is None:\n",
        "        # store miss to avoid retry storms\n",
        "        new_row = {\n",
        "            \"Country_iso3\": country_iso3 or \"\",\n",
        "            \"Country\": country,\n",
        "            \"City\": city,\n",
        "            \"lat\": None,\n",
        "            \"lon\": None,\n",
        "            \"display_name\": None\n",
        "        }\n",
        "        geo_cache = pd.concat([geo_cache, pd.DataFrame([new_row])], ignore_index=True)\n",
        "        geo_cache.to_csv(GEO_CACHE, index=False)\n",
        "        return None, None\n",
        "\n",
        "    lat, lon = loc.latitude, loc.longitude\n",
        "    display_name = getattr(loc, \"address\", None)\n",
        "\n",
        "    new_row = {\n",
        "        \"Country_iso3\": country_iso3 or \"\",\n",
        "        \"Country\": country,\n",
        "        \"City\": city,\n",
        "        \"lat\": lat,\n",
        "        \"lon\": lon,\n",
        "        \"display_name\": display_name\n",
        "    }\n",
        "\n",
        "    geo_cache = pd.concat([geo_cache, pd.DataFrame([new_row])], ignore_index=True)\n",
        "    geo_cache.to_csv(GEO_CACHE, index=False)\n",
        "\n",
        "    return lat, lon\n",
        "\n",
        "def geocode_country_cities(country):\n",
        "    # Use the cleaned Country string, but also pass ISO3 where possible\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    cities = sorted([c for c in sub[\"City\"].dropna().unique() if str(c).strip()])\n",
        "\n",
        "    # try to get ISO3 for this country from your df\n",
        "    iso3 = None\n",
        "    if \"Country_iso3\" in sub.columns:\n",
        "        iso3_vals = sub[\"Country_iso3\"].dropna().unique()\n",
        "        iso3 = iso3_vals[0] if len(iso3_vals) else None\n",
        "\n",
        "    pts = []\n",
        "    for city in cities:\n",
        "        lat, lon = get_latlon(country, city, country_iso3=iso3)\n",
        "        if lat is not None and lon is not None:\n",
        "            pts.append((city, lat, lon))\n",
        "    return pts"
      ],
      "metadata": {
        "id": "RnItl1Uc5CoZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates charts + maps"
      ],
      "metadata": {
        "id": "waf1MbKa5P8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install rasterio geopandas shapely\n",
        "\n",
        "def save_country_map(country, year=2020):\n",
        "    import geopandas as gpd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import rasterio\n",
        "    from rasterio.mask import mask\n",
        "    from matplotlib.colors import LogNorm\n",
        "    from shapely.geometry import mapping, Point\n",
        "\n",
        "    # --- City points (your existing function) ---\n",
        "    pts = geocode_country_cities(country)\n",
        "    if not pts:\n",
        "        return None\n",
        "\n",
        "    # --- ISO3 for robust polygon match (assumes df has Country_iso3) ---\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    iso3 = None\n",
        "    if \"Country_iso3\" in sub.columns:\n",
        "        v = sub[\"Country_iso3\"].dropna().unique()\n",
        "        iso3 = v[0] if len(v) else None\n",
        "\n",
        "    # --- Country polygon (Natural Earth) ---\n",
        "    world = gpd.read_file(\n",
        "        \"https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\"\n",
        "    )\n",
        "    if iso3 and \"ISO_A3\" in world.columns:\n",
        "        country_poly = world[world[\"ISO_A3\"] == iso3].copy()\n",
        "    else:\n",
        "        country_poly = world[world[\"NAME\"].str.lower() == str(country).strip().lower()].copy()\n",
        "\n",
        "    if country_poly.empty:\n",
        "        # fallback: just plot points if polygon missing\n",
        "        country_poly = None\n",
        "\n",
        "    # --- GPW v4 density raster (2020) ---\n",
        "    # NOTE: This is a public mirror that links to a .tiff. It can be large.\n",
        "    # We read it as a remote dataset via /vsicurl/ (no full download required if server supports range requests).\n",
        "    GPW_URL_2020 = \"https://pacific-data.sprep.org/system/files/Global_2020_PopulationDensity30sec_GPWv4.tiff\"\n",
        "    gpw_url = GPW_URL_2020\n",
        "\n",
        "    # Output\n",
        "    path = os.path.join(IMG_DIR, f\"{country}_map.png\")\n",
        "\n",
        "    # Points GeoDF\n",
        "    gdf_pts = gpd.GeoDataFrame(\n",
        "        [{\"City\": c, \"geometry\": Point(lon, lat)} for c, lat, lon in pts],\n",
        "        crs=\"EPSG:4326\"\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(7, 4.5))\n",
        "    ax = plt.gca()\n",
        "\n",
        "    # ---- Open raster remotely ----\n",
        "    # /vsicurl/ enables HTTP range reads (fast) if the host supports it.\n",
        "    # If it fails, we fall back to downloading once (see below).\n",
        "    raster_path = f\"/vsicurl/{gpw_url}\"\n",
        "\n",
        "    try:\n",
        "        with rasterio.Env(GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\"):\n",
        "            with rasterio.open(raster_path) as src:\n",
        "                # Reproject polygon to raster CRS (usually EPSG:4326)\n",
        "                if country_poly is not None:\n",
        "                    poly_in_crs = country_poly.to_crs(src.crs)\n",
        "                    geoms = [mapping(geom) for geom in poly_in_crs.geometry if geom is not None]\n",
        "                else:\n",
        "                    geoms = None\n",
        "\n",
        "                if geoms:\n",
        "                    out_img, out_transform = mask(src, geoms, crop=True, filled=True)\n",
        "                    data = out_img[0]\n",
        "                    extent = (\n",
        "                        out_transform[2],\n",
        "                        out_transform[2] + out_transform[0] * data.shape[1],\n",
        "                        out_transform[5] + out_transform[4] * data.shape[0],\n",
        "                        out_transform[5],\n",
        "                    )\n",
        "                else:\n",
        "                    # No polygon: plot a small window around points\n",
        "                    # compute bounds in raster CRS\n",
        "                    pts_in = gdf_pts.to_crs(src.crs)\n",
        "                    minx, miny, maxx, maxy = pts_in.total_bounds\n",
        "                    pad_x = (maxx - minx) * 0.25 if maxx > minx else 2e5\n",
        "                    pad_y = (maxy - miny) * 0.25 if maxy > miny else 2e5\n",
        "                    window = rasterio.windows.from_bounds(minx - pad_x, miny - pad_y, maxx + pad_x, maxy + pad_y, src.transform)\n",
        "                    out_img = src.read(1, window=window, masked=True)\n",
        "                    data = out_img\n",
        "                    win_transform = src.window_transform(window)\n",
        "                    extent = (\n",
        "                        win_transform[2],\n",
        "                        win_transform[2] + win_transform[0] * data.shape[1],\n",
        "                        win_transform[5] + win_transform[4] * data.shape[0],\n",
        "                        win_transform[5],\n",
        "                    )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Remote raster read failed, consider local download. Error:\", e)\n",
        "        return None\n",
        "\n",
        "    # --- Plot raster with log scale (density is very skewed) ---\n",
        "    arr = np.array(data)\n",
        "    arr = np.ma.masked_invalid(arr)\n",
        "    arr = np.ma.masked_where(arr <= 0, arr)\n",
        "\n",
        "    if arr.count() == 0:\n",
        "        print(\"⚠️ No valid density values found in clipped raster.\")\n",
        "        return None\n",
        "\n",
        "    vmin = float(np.percentile(arr.compressed(), 5))\n",
        "    vmax = float(np.percentile(arr.compressed(), 99))\n",
        "    vmin = max(vmin, 1e-3)\n",
        "\n",
        "    im = ax.imshow(arr, extent=extent, origin=\"upper\", norm=LogNorm(vmin=vmin, vmax=vmax))\n",
        "    plt.colorbar(im, ax=ax, shrink=0.75, label=\"Population density (persons/km², log scale)\")\n",
        "\n",
        "    # --- Plot country outline on top (optional) ---\n",
        "    if country_poly is not None:\n",
        "        # outline only so raster is visible\n",
        "        country_poly.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\", linewidth=0.8, zorder=4)\n",
        "\n",
        "    # --- Red dots on top ---\n",
        "    gdf_pts.plot(\n",
        "        ax=ax,\n",
        "        color=\"red\",\n",
        "        markersize=60,\n",
        "        edgecolor=\"black\",\n",
        "        linewidth=0.4,\n",
        "        zorder=6\n",
        "    )\n",
        "\n",
        "    plt.title(f\"{country}: Cyclotron locations + GPWv4 population density (2020)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=220)\n",
        "    plt.close()\n",
        "    return path"
      ],
      "metadata": {
        "id": "1T6NA7lm5JhG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison + data quality sections (tables)"
      ],
      "metadata": {
        "id": "MF1RwQm05Uyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def global_comparison_tables():\n",
        "    # Top countries by cyclotron count\n",
        "    top_countries = df[\"Country\"].dropna().value_counts().head(25)\n",
        "\n",
        "    # Manufacturer global counts\n",
        "    top_manu = df[\"Manufacturer\"].dropna().value_counts().head(15)\n",
        "\n",
        "    # Energy by country (numeric only)\n",
        "    energy_country = (\n",
        "        df.dropna(subset=[\"Country\"])\n",
        "          .groupby(\"Country\")[\"Energy_num\"]\n",
        "          .agg([\"count\",\"min\",\"median\",\"max\"])\n",
        "          .sort_values(\"count\", ascending=False)\n",
        "          .head(25)\n",
        "    )\n",
        "\n",
        "    return top_countries, top_manu, energy_country\n",
        "\n",
        "def data_quality_summary():\n",
        "    total = len(df)\n",
        "    missing = {}\n",
        "    for col in [\"City\",\"Facility\",\"Manufacturer\",\"Model\",\"Proton energy (MeV)\",\"Energy_num\"]:\n",
        "        missing[col] = int(df[col].isna().sum())\n",
        "\n",
        "    return {\n",
        "        \"total_rows\": total,\n",
        "        \"missing_counts\": missing,\n",
        "        \"missing_pct\": {k: (v/total*100 if total else 0) for k,v in missing.items()}\n",
        "    }"
      ],
      "metadata": {
        "id": "ztifd5fN5SPG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multimodal nested call to comment on each map"
      ],
      "metadata": {
        "id": "H7NwaFbC5eGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL: only if you have a multimodal model client that can accept images\n",
        "# You can keep it disabled by default.\n",
        "\n",
        "USE_MAP_LLM = False  # set True only if you have an image-capable model client\n",
        "\n",
        "def llm_map_insight(country, map_path):\n",
        "    if not USE_MAP_LLM:\n",
        "        return None\n",
        "\n",
        "    # Example placeholder: adapt to your multimodal client API\n",
        "    # Many multimodal APIs require: [{\"type\":\"text\",...}, {\"type\":\"image_url\",...}]\n",
        "    # Here we leave it as a stub so you can plug your provider.\n",
        "    raise NotImplementedError(\"Plug in your multimodal LLM client here.\")"
      ],
      "metadata": {
        "id": "S7vO8MvK5XzW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the PDF (ReportLab)"
      ],
      "metadata": {
        "id": "U9oVZ8pe5h0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# IAEA Cyclotron PDF Report — FULL CODE\n",
        "# - Precomputes LLM summaries + images + facility landscape (LLM JSON -> validated -> markdown)\n",
        "# - Builds PDF from precomputed artifacts\n",
        "# - Provides run_test(country=...) and run_full()\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import hashlib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from reportlab.platypus import (\n",
        "    SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak, Table, TableStyle\n",
        ")\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib import colors\n",
        "from xml.sax.saxutils import escape as xml_escape\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "# -----------------------\n",
        "# CONFIG\n",
        "# -----------------------\n",
        "PDF_PATH_FULL = \"/content/IAEA_Cyclotron_Country_Executive_Summaries.pdf\"\n",
        "PDF_PATH_TEST = \"/content/IAEA_Cyclotron_TEST_ONE_COUNTRY.pdf\"\n",
        "IMG_DIR = \"/content/iaea_report_imgs\"\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "styles = getSampleStyleSheet()\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Helpers\n",
        "# -----------------------\n",
        "def escape_paragraph_text(s: str) -> str:\n",
        "    \"\"\"Escape &, <, > for ReportLab Paragraph mini-HTML + preserve newlines.\"\"\"\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = xml_escape(str(s))\n",
        "    return s.replace(\"\\n\", \"<br/>\")\n",
        "\n",
        "\n",
        "def df_to_table(dataframe, max_rows=30):\n",
        "    df2 = dataframe.copy()\n",
        "    if len(df2) > max_rows:\n",
        "        df2 = df2.head(max_rows)\n",
        "\n",
        "    data = [list(df2.reset_index().columns)] + df2.reset_index().values.tolist()\n",
        "    t = Table(data, repeatRows=1)\n",
        "    t.setStyle(TableStyle([\n",
        "        (\"BACKGROUND\", (0,0), (-1,0), colors.lightgrey),\n",
        "        (\"GRID\", (0,0), (-1,-1), 0.25, colors.grey),\n",
        "        (\"FONTNAME\", (0,0), (-1,0), \"Helvetica-Bold\"),\n",
        "        (\"FONTSIZE\", (0,0), (-1,-1), 8),\n",
        "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
        "        (\"LEFTPADDING\", (0,0), (-1,-1), 4),\n",
        "        (\"RIGHTPADDING\", (0,0), (-1,-1), 4),\n",
        "        (\"TOPPADDING\", (0,0), (-1,-1), 3),\n",
        "        (\"BOTTOMPADDING\", (0,0), (-1,-1), 3),\n",
        "    ]))\n",
        "    return t\n",
        "\n",
        "def two_column_block(left_title, left_flowable, right_title, right_flowable, gap=12):\n",
        "    \"\"\"\n",
        "    Create a 2-column block (ReportLab Table) with a small title + content in each column.\n",
        "    left_flowable / right_flowable are usually Tables returned by df_to_table(...)\n",
        "    \"\"\"\n",
        "    left = [Paragraph(left_title, styles[\"Heading2\"]), Spacer(1, 6), left_flowable]\n",
        "    right = [Paragraph(right_title, styles[\"Heading2\"]), Spacer(1, 6), right_flowable]\n",
        "\n",
        "    t = Table(\n",
        "        [[left, right]],\n",
        "        colWidths=[(A4[0] - 72 - gap) / 2, (A4[0] - 72 - gap) / 2],  # A4 width minus margins (36+36)\n",
        "    )\n",
        "    t.setStyle(TableStyle([\n",
        "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
        "        (\"LEFTPADDING\", (0,0), (-1,-1), 0),\n",
        "        (\"RIGHTPADDING\", (0,0), (-1,-1), 0),\n",
        "        (\"TOPPADDING\", (0,0), (-1,-1), 0),\n",
        "        (\"BOTTOMPADDING\", (0,0), (-1,-1), 0),\n",
        "    ]))\n",
        "    return t\n",
        "\n",
        "def rl_image_fit(img_path, max_w=480, max_h=300):\n",
        "    \"\"\"Fit image into max_w x max_h (points) preserving aspect ratio.\"\"\"\n",
        "    with PILImage.open(img_path) as im:\n",
        "        w, h = im.size\n",
        "    scale = min(max_w / w, max_h / h)\n",
        "    return Image(img_path, width=w * scale, height=h * scale)\n",
        "\n",
        "\n",
        "def plots_2x2_block(imgs, cell_w=255, cell_h=185, pad=6):\n",
        "    \"\"\"\n",
        "    imgs: dict keys: 'city','manufacturer','energy','map'\n",
        "    Returns a ReportLab Table laid out 2x2.\n",
        "    \"\"\"\n",
        "    def cell(title, path):\n",
        "        if path and os.path.exists(path):\n",
        "            return [\n",
        "                Paragraph(title, styles[\"Heading4\"]),\n",
        "                rl_image_fit(path, max_w=cell_w, max_h=cell_h)\n",
        "            ]\n",
        "        else:\n",
        "            return [Paragraph(title + \" (not available)\", styles[\"Normal\"])]\n",
        "\n",
        "    data = [\n",
        "        [cell(\"Top cities\", imgs.get(\"city\")), cell(\"Manufacturers\", imgs.get(\"manufacturer\"))],\n",
        "        [cell(\"Energy distribution\", imgs.get(\"energy\")), cell(\"Map (red dots)\", imgs.get(\"map\"))],\n",
        "    ]\n",
        "\n",
        "    t = Table(\n",
        "        data,\n",
        "        colWidths=[(cell_w + pad), (cell_w + pad)],\n",
        "        rowHeights=[cell_h + 30, cell_h + 30],\n",
        "    )\n",
        "    t.setStyle(TableStyle([\n",
        "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
        "        (\"LEFTPADDING\", (0,0), (-1,-1), pad),\n",
        "        (\"RIGHTPADDING\", (0,0), (-1,-1), pad),\n",
        "        (\"TOPPADDING\", (0,0), (-1,-1), pad),\n",
        "        (\"BOTTOMPADDING\", (0,0), (-1,-1), pad),\n",
        "    ]))\n",
        "    return t\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Facility Landscape (LLM) — Cache + Helpers\n",
        "# ============================================================\n",
        "\n",
        "FAC_LANDSCAPE_CACHE = \"/content/facility_landscape_cache.json\"\n",
        "\n",
        "# Load cache once\n",
        "if os.path.exists(FAC_LANDSCAPE_CACHE):\n",
        "    with open(FAC_LANDSCAPE_CACHE, \"r\", encoding=\"utf-8\") as f:\n",
        "        facility_landscape_cache = json.load(f)\n",
        "else:\n",
        "    facility_landscape_cache = {}\n",
        "\n",
        "def _save_facility_landscape_cache():\n",
        "    with open(FAC_LANDSCAPE_CACHE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(facility_landscape_cache, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def _cache_key(country: str, top_n: int, version: str = \"v1\") -> str:\n",
        "    raw = f\"{version}|{country.strip().lower()}|top_n={top_n}\"\n",
        "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def top_facilities_table_df(country: str, top_n: int = 12) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Deterministic table: top facilities by row count for that country.\n",
        "    If Facility is mostly missing, falls back to top cities.\n",
        "    \"\"\"\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    if sub.empty:\n",
        "        return pd.DataFrame(columns=[\"Facility_or_City\", \"Count\", \"Note\"])\n",
        "\n",
        "    fac = (\n",
        "        sub[\"Facility\"].fillna(\"\").astype(str).str.strip()\n",
        "        .replace(\"\", pd.NA)\n",
        "        .dropna()\n",
        "        .value_counts()\n",
        "        .head(top_n)\n",
        "    )\n",
        "\n",
        "    if not fac.empty:\n",
        "        out = fac.reset_index()\n",
        "        out.columns = [\"Facility_or_City\", \"Count\"]\n",
        "        out[\"Note\"] = \"Facility\"\n",
        "        return out\n",
        "\n",
        "    city = sub[\"City\"].dropna().astype(str).str.strip().replace(\"\", pd.NA).dropna().value_counts().head(top_n)\n",
        "    out = city.reset_index()\n",
        "    out.columns = [\"Facility_or_City\", \"Count\"]\n",
        "    out[\"Note\"] = \"City proxy (Facility missing)\"\n",
        "    return out\n",
        "\n",
        "\n",
        "def llm_facility_landscape_json(country: str, top_n: int = 12) -> dict:\n",
        "    \"\"\"\n",
        "    Verbose facility landscape.\n",
        "    Strict schema:\n",
        "    {\n",
        "      \"overview\": \"...(multi-paragraph)...\",\n",
        "      \"notes\": \"...\",\n",
        "      \"facilities\": [\n",
        "        {\n",
        "          \"facility\": \"<exact name from input list>\",\n",
        "          \"count\": <int>,\n",
        "          \"interpretation\": \"<2–5 sentences>\",\n",
        "          \"signals\": [\"...\",\"...\"],\n",
        "          \"typology\": \"<Hospital/University | Private radiopharmacy | Network/group | State/public institute | Unknown>\",\n",
        "          \"confidence\": \"<High|Medium|Low>\",\n",
        "          \"network_hint\": \"<brand if explicitly in name else —>\",\n",
        "          \"caveat\": \"<1 sentence>\"\n",
        "        }, ...\n",
        "      ]\n",
        "    }\n",
        "    \"\"\"\n",
        "    ck = _cache_key(country, top_n, version=\"v3_verbose\")\n",
        "    if ck in facility_landscape_cache:\n",
        "        return facility_landscape_cache[ck]\n",
        "\n",
        "    tdf = top_facilities_table_df(country, top_n=top_n)\n",
        "\n",
        "    items = []\n",
        "    for _, r in tdf.iterrows():\n",
        "        items.append({\"name\": str(r[\"Facility_or_City\"]), \"count\": int(r[\"Count\"]), \"note\": str(r[\"Note\"])})\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are writing a verbose country report section about \"key cyclotron facilities\".\n",
        "\n",
        "Country: {country}\n",
        "\n",
        "Input list (top entries from database; do not add others):\n",
        "{json.dumps(items, ensure_ascii=False, indent=2)}\n",
        "\n",
        "CRITICAL RULES (non-hallucinating):\n",
        "- You MUST NOT invent ownership facts.\n",
        "- You MUST NOT claim a network/brand unless the facility name explicitly contains it (e.g., contains “Curium”).\n",
        "- Use only NAME CUES (words in the facility string) and the input “note” field.\n",
        "- If the entry is a city proxy (note contains \"City proxy\"), treat it as missing facility info.\n",
        "\n",
        "TASK:\n",
        "Return STRICT JSON ONLY (no markdown, no extra text) with this schema:\n",
        "\n",
        "{{\n",
        "  \"overview\": \"Write 2–4 paragraphs (verbose) describing the facility landscape in this country. Mention data-quality caveats (missing facility names) if relevant. Explain what the naming patterns suggest about institution types (hospital/university vs private vs public), WITHOUT stating ownership as fact.\",\n",
        "  \"notes\": \"Optional short notes (or empty string).\",\n",
        "  \"facilities\": [\n",
        "    {{\n",
        "      \"facility\": \"<exact name from input list>\",\n",
        "      \"count\": <integer from input list>,\n",
        "      \"interpretation\": \"2–5 sentences describing what the name suggests (departmental hospital, university clinic, private company, institute etc.)\",\n",
        "      \"signals\": [\"List 2–6 short name cues you used, e.g. 'University', 'Hospital', 'Ltd', 'Institute', 'Nuclear Medicine', etc. If none, use []\"],\n",
        "      \"typology\": \"<ONE of: Hospital/University | Private radiopharmacy | Network/group | State/public institute | Unknown>\",\n",
        "      \"confidence\": \"<High|Medium|Low>\",\n",
        "      \"network_hint\": \"<If name explicitly contains a network brand (Curium/Siemens/AAA etc.) write it else '—'>\",\n",
        "      \"caveat\": \"1 sentence caveat, e.g., 'Based on name cues only; ownership not verified.'\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Output constraints:\n",
        "- Every facility must be one of the input list names.\n",
        "- For City proxy entries: typology=Unknown, confidence=Low, network_hint='—', signals=[], interpretation should say facility is missing.\n",
        "\"\"\".strip()\n",
        "\n",
        "    resp = llm.invoke(prompt)\n",
        "    raw = (resp.content or \"\").strip()\n",
        "    raw = raw.strip().strip(\"`\")\n",
        "    if raw.lower().startswith(\"json\"):\n",
        "        raw = raw[4:].strip()\n",
        "\n",
        "    try:\n",
        "        obj = json.loads(raw)\n",
        "    except Exception as e:\n",
        "        obj = {\"overview\": f\"(Verbose facility landscape JSON parse failed: {e})\", \"notes\": \"\", \"facilities\": []}\n",
        "\n",
        "    allowed_typology = {\n",
        "        \"Hospital/University\",\n",
        "        \"Private radiopharmacy\",\n",
        "        \"Network/group\",\n",
        "        \"State/public institute\",\n",
        "        \"Unknown\",\n",
        "    }\n",
        "    allowed_conf = {\"High\", \"Medium\", \"Low\"}\n",
        "\n",
        "    allowed_names = {x[\"name\"]: x for x in items}\n",
        "\n",
        "    cleaned = []\n",
        "    for r in obj.get(\"facilities\", []):\n",
        "        fac = str(r.get(\"facility\", \"\")).strip()\n",
        "        if fac not in allowed_names:\n",
        "            continue\n",
        "\n",
        "        count = int(allowed_names[fac][\"count\"])\n",
        "        note = str(allowed_names[fac][\"note\"])\n",
        "\n",
        "        typ = str(r.get(\"typology\", \"Unknown\")).strip()\n",
        "        conf = str(r.get(\"confidence\", \"Low\")).strip()\n",
        "        net = str(r.get(\"network_hint\", \"—\")).strip() or \"—\"\n",
        "        interp = str(r.get(\"interpretation\", \"\")).strip()\n",
        "        caveat = str(r.get(\"caveat\", \"\")).strip()\n",
        "        signals = r.get(\"signals\", [])\n",
        "        if not isinstance(signals, list):\n",
        "            signals = []\n",
        "\n",
        "        # enforce city-proxy rules\n",
        "        if \"City proxy\" in note:\n",
        "            typ, conf, net = \"Unknown\", \"Low\", \"—\"\n",
        "            signals = []\n",
        "            if not interp:\n",
        "                interp = \"Facility name is missing in the dataset; this entry is a city-level proxy.\"\n",
        "            if not caveat:\n",
        "                caveat = \"Facility not available; classification not possible.\"\n",
        "\n",
        "        if typ not in allowed_typology:\n",
        "            typ = \"Unknown\"\n",
        "        if conf not in allowed_conf:\n",
        "            conf = \"Low\"\n",
        "        if not interp:\n",
        "            interp = \"No strong name cues available; leaving classification as Unknown.\"\n",
        "        if not caveat:\n",
        "            caveat = \"Based on name cues only; ownership not verified.\"\n",
        "\n",
        "        cleaned.append({\n",
        "            \"facility\": fac,\n",
        "            \"count\": count,\n",
        "            \"interpretation\": interp,\n",
        "            \"signals\": [str(s).strip() for s in signals if str(s).strip()][:8],\n",
        "            \"typology\": typ,\n",
        "            \"confidence\": conf,\n",
        "            \"network_hint\": net,\n",
        "            \"caveat\": caveat\n",
        "        })\n",
        "\n",
        "    out = {\n",
        "        \"overview\": str(obj.get(\"overview\", \"\")).strip() or \"—\",\n",
        "        \"notes\": str(obj.get(\"notes\", \"\")).strip(),\n",
        "        \"facilities\": cleaned\n",
        "    }\n",
        "\n",
        "    facility_landscape_cache[ck] = out\n",
        "    _save_facility_landscape_cache()\n",
        "    return out\n",
        "\n",
        "\n",
        "def facility_landscape_to_markdown(obj: dict) -> str:\n",
        "    \"\"\"\n",
        "    Render verbose facility landscape into readable markdown-like text\n",
        "    (ReportLab Paragraph will display it after escape_paragraph_text).\n",
        "    \"\"\"\n",
        "    overview = obj.get(\"overview\", \"—\").strip()\n",
        "    notes = obj.get(\"notes\", \"\").strip()\n",
        "    facilities = obj.get(\"facilities\", [])\n",
        "\n",
        "    lines = []\n",
        "    lines.append(overview)\n",
        "\n",
        "    if notes:\n",
        "        lines.append(\"\")\n",
        "        lines.append(f\"Notes: {notes}\")\n",
        "\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"Key facilities (name-cue based interpretation):\")\n",
        "\n",
        "    for f in facilities:\n",
        "        sig = \", \".join(f.get(\"signals\", [])) if f.get(\"signals\") else \"—\"\n",
        "        lines.append(\"\")\n",
        "        lines.append(f\"• {f['facility']} — {int(f['count'])} entries\")\n",
        "        lines.append(f\"  Typology: {f['typology']} | Confidence: {f['confidence']} | Network hint: {f['network_hint']}\")\n",
        "        lines.append(f\"  Signals: {sig}\")\n",
        "        lines.append(f\"  Interpretation: {f['interpretation']}\")\n",
        "        lines.append(f\"  Caveat: {f['caveat']}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def precompute_facility_landscapes(countries, top_n=12):\n",
        "    \"\"\"\n",
        "    Precompute facility landscape markdown per country.\n",
        "    Uses JSON cache internally -> cheap on reruns.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for country in countries:\n",
        "        try:\n",
        "            obj = llm_facility_landscape_json(country, top_n=top_n)\n",
        "            out[country] = facility_landscape_to_markdown(obj)\n",
        "        except Exception as e:\n",
        "            out[country] = f\"(Facility landscape failed: {e})\"\n",
        "    print(f\"Precomputed facility landscapes: {len(out)} countries\")\n",
        "    return out\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Core pipeline functions\n",
        "# (These assume you already have:\n",
        "#   - df (loaded)\n",
        "#   - llm_exec_summary(country, top_n=10)\n",
        "#   - country_summary(country, top_n=10)\n",
        "#   - global_comparison_tables()\n",
        "#   - data_quality_summary()\n",
        "#   - save_country_map(country)\n",
        "# ============================================================\n",
        "\n",
        "def save_city_bar_chart(country, top_n=10):\n",
        "    out = country_summary(country, top_n=top_n)\n",
        "    if not out[\"found\"] or out[\"cities_top\"].empty:\n",
        "        return None\n",
        "    s = out[\"cities_top\"]\n",
        "    path = os.path.join(IMG_DIR, f\"{country}_cities.png\")\n",
        "    plt.figure()\n",
        "    s[::-1].plot(kind=\"barh\")\n",
        "    plt.title(f\"{country}: Top {min(top_n, len(s))} cities by cyclotron count\")\n",
        "    plt.xlabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=180)\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "\n",
        "def save_manufacturer_bar_chart(country, top_n=10):\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    s = sub[\"Manufacturer\"].dropna().value_counts().head(top_n)\n",
        "    if s.empty:\n",
        "        return None\n",
        "    path = os.path.join(IMG_DIR, f\"{country}_manufacturers.png\")\n",
        "    plt.figure()\n",
        "    s[::-1].plot(kind=\"barh\")\n",
        "    plt.title(f\"{country}: Top manufacturers\")\n",
        "    plt.xlabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=180)\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "\n",
        "def save_energy_hist(country):\n",
        "    sub = df[df[\"Country\"].str.lower() == str(country).strip().lower()].copy()\n",
        "    vals = sub.get(\"Energy_num\", pd.Series(dtype=float)).dropna()\n",
        "    if vals.empty:\n",
        "        return None\n",
        "    path = os.path.join(IMG_DIR, f\"{country}_energy.png\")\n",
        "    plt.figure()\n",
        "    plt.hist(vals, bins=12)\n",
        "    plt.title(f\"{country}: Proton energy distribution (MeV)\")\n",
        "    plt.xlabel(\"MeV\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=180)\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "\n",
        "def precompute_summaries(countries, top_n=10):\n",
        "    summaries_by_country = {}\n",
        "    for country in countries:\n",
        "        try:\n",
        "            summaries_by_country[country] = llm_exec_summary(country, top_n=top_n)\n",
        "        except Exception as e:\n",
        "            summaries_by_country[country] = f\"(LLM summary failed: {e})\"\n",
        "    print(f\"Precomputed LLM summaries: {len(summaries_by_country)} countries\")\n",
        "    return summaries_by_country\n",
        "\n",
        "\n",
        "def precompute_images(countries, top_n=10):\n",
        "    images_by_country = {}\n",
        "    for country in countries:\n",
        "        imgs = {}\n",
        "        try: imgs[\"city\"] = save_city_bar_chart(country, top_n=top_n)\n",
        "        except Exception: imgs[\"city\"] = None\n",
        "        try: imgs[\"manufacturer\"] = save_manufacturer_bar_chart(country, top_n=top_n)\n",
        "        except Exception: imgs[\"manufacturer\"] = None\n",
        "        try: imgs[\"energy\"] = save_energy_hist(country)\n",
        "        except Exception: imgs[\"energy\"] = None\n",
        "        try: imgs[\"map\"] = save_country_map(country)\n",
        "        except Exception: imgs[\"map\"] = None\n",
        "        images_by_country[country] = imgs\n",
        "    print(f\"Precomputed images for: {len(images_by_country)} countries\")\n",
        "    return images_by_country\n",
        "\n",
        "\n",
        "def build_pdf(countries, summaries_by_country, images_by_country, facility_landscape_by_country, pdf_path, title_suffix=\"\"):\n",
        "    doc = SimpleDocTemplate(\n",
        "        pdf_path,\n",
        "        pagesize=A4,\n",
        "        rightMargin=36,\n",
        "        leftMargin=36,\n",
        "        topMargin=36,\n",
        "        bottomMargin=36\n",
        "    )\n",
        "    story = []\n",
        "\n",
        "    title = \"IAEA Cyclotron Database — Executive Summaries by Country\"\n",
        "    if title_suffix:\n",
        "        title += f\" ({title_suffix})\"\n",
        "    story.append(Paragraph(title, styles[\"Title\"]))\n",
        "    story.append(Spacer(1, 10))\n",
        "    story.append(Paragraph(\"Automatically generated from the scraped IAEA SharePoint cyclotron list.\", styles[\"Normal\"]))\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    story.append(Paragraph(\"Global Comparison\", styles[\"Heading1\"]))\n",
        "    top_countries, top_manu, energy_country = global_comparison_tables()\n",
        "\n",
        "    # --- Global Comparison (2 columns: countries left, manufacturers right)\n",
        "    left_tbl  = df_to_table(top_countries.to_frame(\"count\"))\n",
        "    right_tbl = df_to_table(top_manu.to_frame(\"count\"))\n",
        "\n",
        "    story.append(two_column_block(\n",
        "        left_title=\"Top countries by cyclotron count\",\n",
        "        left_flowable=left_tbl,\n",
        "        right_title=\"Top manufacturers (global)\",\n",
        "        right_flowable=right_tbl,\n",
        "        gap=18\n",
        "    ))\n",
        "    story.append(Spacer(1, 14))\n",
        "\n",
        "    # Keep energy table below (same page if it fits; otherwise it will spill naturally)\n",
        "    story.append(Paragraph(\"Energy statistics by country (numeric rows only)\", styles[\"Heading2\"]))\n",
        "    story.append(df_to_table(energy_country))\n",
        "    story.append(PageBreak())\n",
        "\n",
        "\n",
        "    dq = data_quality_summary()\n",
        "    story.append(Paragraph(\"Data Quality Summary\", styles[\"Heading1\"]))\n",
        "    story.append(Paragraph(f\"Total rows: <b>{dq['total_rows']}</b>\", styles[\"Normal\"]))\n",
        "    story.append(Spacer(1, 8))\n",
        "\n",
        "    dq_table_df = pd.DataFrame({\n",
        "        \"missing_count\": dq[\"missing_counts\"],\n",
        "        \"missing_pct\": {k: f\"{v:.1f}%\" for k, v in dq[\"missing_pct\"].items()}\n",
        "    })\n",
        "    story.append(df_to_table(dq_table_df))\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    story.append(Paragraph(\"Country Executive Summaries\", styles[\"Heading1\"]))\n",
        "    story.append(Paragraph(\"Each country section includes an LLM-generated summary plus charts and a map.\", styles[\"Normal\"]))\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    for idx, country in enumerate(countries, start=1):\n",
        "        story.append(Paragraph(escape_paragraph_text(country), styles[\"Heading1\"]))\n",
        "        story.append(Spacer(1, 6))\n",
        "\n",
        "        summary = summaries_by_country.get(country, \"\")\n",
        "        story.append(Paragraph(escape_paragraph_text(summary), styles[\"Normal\"]))\n",
        "        story.append(Spacer(1, 10))\n",
        "\n",
        "        imgs = images_by_country.get(country, {})\n",
        "\n",
        "        # Page 1: plots\n",
        "        story.append(plots_2x2_block(imgs, cell_w=255, cell_h=185, pad=6))\n",
        "        story.append(Spacer(1, 10))\n",
        "\n",
        "        # Page 2: Facility landscape\n",
        "        story.append(PageBreak())\n",
        "        story.append(Paragraph(\"Facility Landscape and Ownership Typology\", styles[\"Heading2\"]))\n",
        "        story.append(Spacer(1, 6))\n",
        "\n",
        "        tdf = top_facilities_table_df(country, top_n=12)\n",
        "        story.append(Paragraph(\"Top facilities (from database)\", styles[\"Heading3\"]))\n",
        "        story.append(df_to_table(tdf.set_index(\"Facility_or_City\")))\n",
        "        story.append(Spacer(1, 10))\n",
        "\n",
        "        story.append(Paragraph(\"LLM-assisted classification (name-cue based, non-hallucinating)\", styles[\"Heading3\"]))\n",
        "        fac_text = facility_landscape_by_country.get(country, \"—\")\n",
        "        story.append(Paragraph(escape_paragraph_text(fac_text), styles[\"Normal\"]))\n",
        "        story.append(Spacer(1, 10))\n",
        "\n",
        "        if idx != len(countries):\n",
        "            story.append(PageBreak())\n",
        "\n",
        "    doc.build(story)\n",
        "    print(f\"✅ PDF written: {pdf_path}\")\n",
        "    return pdf_path\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# RUNNERS — choose what to run each time\n",
        "# ============================================================\n",
        "\n",
        "def run_test(country=\"Belarus\", top_n=10):\n",
        "    all_countries = sorted(df[\"Country\"].dropna().unique())\n",
        "    if country not in all_countries:\n",
        "        print(f\"⚠️ '{country}' not found in df['Country']. Using first available country instead.\")\n",
        "        country = all_countries[0] if all_countries else country\n",
        "\n",
        "    countries = [country]\n",
        "    print(\"TEST MODE countries:\", countries)\n",
        "\n",
        "    facility_landscape_by_country = precompute_facility_landscapes(countries, top_n=12)\n",
        "    summaries_by_country = precompute_summaries(countries, top_n=top_n)\n",
        "    images_by_country = precompute_images(countries, top_n=top_n)\n",
        "\n",
        "    return build_pdf(\n",
        "        countries=countries,\n",
        "        summaries_by_country=summaries_by_country,\n",
        "        images_by_country=images_by_country,\n",
        "        facility_landscape_by_country=facility_landscape_by_country,\n",
        "        pdf_path=PDF_PATH_TEST,\n",
        "        title_suffix=f\"TEST: {country}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def run_full(top_n=10):\n",
        "    countries = sorted(df[\"Country\"].dropna().unique())\n",
        "    print(f\"FULL MODE countries: {len(countries)}\")\n",
        "\n",
        "    facility_landscape_by_country = precompute_facility_landscapes(countries, top_n=12)\n",
        "    summaries_by_country = precompute_summaries(countries, top_n=top_n)\n",
        "    images_by_country = precompute_images(countries, top_n=top_n)\n",
        "\n",
        "    return build_pdf(\n",
        "        countries=countries,\n",
        "        summaries_by_country=summaries_by_country,\n",
        "        images_by_country=images_by_country,\n",
        "        facility_landscape_by_country=facility_landscape_by_country,\n",
        "        pdf_path=PDF_PATH_FULL,\n",
        "        title_suffix=\"FULL\"\n",
        "    )"
      ],
      "metadata": {
        "id": "M2MdRIDj5hAw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run one"
      ],
      "metadata": {
        "id": "xOrfFkkF5Wym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run_test(\"Bahrain\") or Belarus\n",
        "run_test(\"Italy\")\n",
        "# run_full()"
      ],
      "metadata": {
        "id": "DflYFSX95mlx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3d8bce7b-6c33-467f-c37d-fc6aa9f8c57d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST MODE countries: ['Italy']\n",
            "Precomputed facility landscapes: 1 countries\n",
            "Precomputed LLM summaries: 1 countries\n",
            "Precomputed images for: 1 countries\n",
            "✅ PDF written: /content/IAEA_Cyclotron_TEST_ONE_COUNTRY.pdf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/IAEA_Cyclotron_TEST_ONE_COUNTRY.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WWeN-rxA8k4O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}